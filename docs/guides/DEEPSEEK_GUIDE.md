# DeepSeek ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

DeepSeek æ˜¯ä¸€å®¶ä¸“æ³¨äºå¤§è¯­è¨€æ¨¡å‹çš„ AI å…¬å¸ï¼Œæä¾› OpenAI å…¼å®¹çš„ API æ¥å£ï¼Œç‰¹åˆ«æ”¯æŒæ¨ç†æ¨¡å‹ï¼ˆReasoning Modelï¼‰ã€‚

**å®˜ç½‘**: https://www.deepseek.com/  
**API æ–‡æ¡£**: https://api-docs.deepseek.com/zh-cn/

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```toml
[dependencies]
llm-connector = "0.4.20"
tokio = { version = "1", features = ["full"] }

# æµå¼æ”¯æŒï¼ˆå¯é€‰ï¼‰
llm-connector = { version = "0.4.20", features = ["streaming"] }
```

### è·å– API Key

1. è®¿é—® https://platform.deepseek.com/
2. æ³¨å†Œ/ç™»å½•è´¦å·
3. åœ¨æ§åˆ¶å°åˆ›å»º API Key
4. API Key æ ¼å¼: `sk-...`

---

## ğŸ’¡ åŸºç¡€ç”¨æ³•

### æ ‡å‡†å¯¹è¯æ¨¡å‹ï¼ˆdeepseek-chatï¼‰

```rust
use llm_connector::{LlmClient, types::{ChatRequest, Message, Role}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆ›å»ºå®¢æˆ·ç«¯
    let client = LlmClient::deepseek("sk-...")?;
    
    // åˆ›å»ºè¯·æ±‚
    let request = ChatRequest {
        model: "deepseek-chat".to_string(),
        messages: vec![Message {
            role: Role::User,
            content: "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±".to_string(),
            ..Default::default()
        }],
        max_tokens: Some(100),
        ..Default::default()
    };
    
    // å‘é€è¯·æ±‚
    let response = client.chat(&request).await?;
    println!("Response: {}", response.content);
    
    Ok(())
}
```

### æ¨ç†æ¨¡å‹ï¼ˆdeepseek-reasonerï¼‰

DeepSeek çš„æ¨ç†æ¨¡å‹ä¼šå±•ç¤ºæ€è€ƒè¿‡ç¨‹ï¼ˆreasoning contentï¼‰ï¼š

```rust
use llm_connector::{LlmClient, types::{ChatRequest, Message, Role}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let client = LlmClient::deepseek("sk-...")?;
    
    let request = ChatRequest {
        model: "deepseek-reasoner".to_string(),
        messages: vec![Message {
            role: Role::User,
            content: "9.11 å’Œ 9.9 å“ªä¸ªæ›´å¤§ï¼Ÿ".to_string(),
            ..Default::default()
        }],
        max_tokens: Some(500),
        ..Default::default()
    };
    
    let response = client.chat(&request).await?;
    
    // æ¨ç†è¿‡ç¨‹ï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
    if let Some(reasoning) = response.reasoning_content {
        println!("ğŸ§  æ€è€ƒè¿‡ç¨‹:\n{}", reasoning);
    }
    
    // æœ€ç»ˆç­”æ¡ˆ
    println!("\nğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:\n{}", response.content);
    
    Ok(())
}
```

### æµå¼å“åº”

```rust
use llm_connector::{LlmClient, types::{ChatRequest, Message, Role}};
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let client = LlmClient::deepseek("sk-...")?;
    
    let request = ChatRequest {
        model: "deepseek-chat".to_string(),
        messages: vec![Message {
            role: Role::User,
            content: "ä»‹ç»ä¸€ä¸‹åŒ—äº¬".to_string(),
            ..Default::default()
        }],
        stream: Some(true),
        max_tokens: Some(200),
        ..Default::default()
    };
    
    let mut stream = client.chat_stream(&request).await?;
    
    while let Some(chunk) = stream.next().await {
        let chunk = chunk?;
        if let Some(content) = chunk.get_content() {
            print!("{}", content);
        }
    }
    
    Ok(())
}
```

---

## ğŸ¯ æ”¯æŒçš„æ¨¡å‹

| æ¨¡å‹ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| **deepseek-chat** | æ ‡å‡†å¯¹è¯ | é€šç”¨å¯¹è¯æ¨¡å‹ |
| **deepseek-reasoner** | æ¨ç†æ¨¡å‹ | å±•ç¤ºæ€è€ƒè¿‡ç¨‹çš„æ¨ç†æ¨¡å‹ |

### æ¨¡å‹é€‰æ‹©

```rust
// æ ‡å‡†å¯¹è¯æ¨¡å‹
let request = ChatRequest {
    model: "deepseek-chat".to_string(),
    // ...
};

// æ¨ç†æ¨¡å‹ï¼ˆä¼šè¿”å›æ€è€ƒè¿‡ç¨‹ï¼‰
let request = ChatRequest {
    model: "deepseek-reasoner".to_string(),
    // ...
};
```

---

## ğŸ§  æ¨ç†æ¨¡å‹è¯¦è§£

### ä»€ä¹ˆæ˜¯æ¨ç†æ¨¡å‹ï¼Ÿ

DeepSeek çš„æ¨ç†æ¨¡å‹ï¼ˆdeepseek-reasonerï¼‰ä¼šåœ¨ç”Ÿæˆç­”æ¡ˆå‰å±•ç¤ºå…¶æ€è€ƒè¿‡ç¨‹ï¼Œç±»ä¼¼äº OpenAI çš„ o1 æ¨¡å‹ã€‚

### æ¨ç†å†…å®¹æå–

**éæµå¼å“åº”**:
```rust
let response = client.chat(&request).await?;

// è‡ªåŠ¨æå–æ¨ç†å†…å®¹
if let Some(reasoning) = response.reasoning_content {
    println!("æ€è€ƒè¿‡ç¨‹: {}", reasoning);
}

// æœ€ç»ˆç­”æ¡ˆ
println!("ç­”æ¡ˆ: {}", response.content);
```

**æµå¼å“åº”**:
```rust
let mut stream = client.chat_stream(&request).await?;

while let Some(chunk) = stream.next().await {
    let chunk = chunk?;
    
    // æ¨ç†å†…å®¹ï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
    if let Some(reasoning) = chunk.choices.first()
        .and_then(|c| c.delta.reasoning_content.as_ref()) {
        print!("ğŸ§  {}", reasoning);
    }
    
    // æœ€ç»ˆç­”æ¡ˆ
    if let Some(content) = chunk.get_content() {
        print!("ğŸ’¡ {}", content);
    }
}
```

### æ¨ç†æ¨¡å‹ç¤ºä¾‹

```rust
let request = ChatRequest {
    model: "deepseek-reasoner".to_string(),
    messages: vec![Message {
        role: Role::User,
        content: "è®¡ç®— 15 * 23".to_string(),
        ..Default::default()
    }],
    max_tokens: Some(500),
    ..Default::default()
};

let response = client.chat(&request).await?;

// è¾“å‡ºç¤ºä¾‹ï¼š
// æ€è€ƒè¿‡ç¨‹: "æˆ‘éœ€è¦è®¡ç®— 15 ä¹˜ä»¥ 23ã€‚è®©æˆ‘åˆ†æ­¥éª¤æ¥ï¼š
//            15 * 20 = 300
//            15 * 3 = 45
//            300 + 45 = 345"
// ç­”æ¡ˆ: "15 * 23 = 345"
```

---

## âš™ï¸ é«˜çº§é…ç½®

### è‡ªå®šä¹‰é…ç½®

```rust
use llm_connector::LlmClient;

let client = LlmClient::deepseek_with_config(
    "sk-...",           // API key
    None,               // base_url (ä½¿ç”¨é»˜è®¤)
    Some(60),           // timeout (60ç§’)
    None                // proxy
)?;
```

### ä½¿ç”¨ä»£ç†

```rust
let client = LlmClient::deepseek_with_config(
    "sk-...",
    None,
    Some(60),
    Some("http://proxy.example.com:8080")  // ä»£ç†åœ°å€
)?;
```

### è‡ªå®šä¹‰ç«¯ç‚¹

```rust
let client = LlmClient::deepseek_with_config(
    "sk-...",
    Some("https://custom.api.deepseek.com"),  // è‡ªå®šä¹‰ç«¯ç‚¹
    Some(60),
    None
)?;
```

---

## ğŸ“Š è¯·æ±‚å‚æ•°

### å¸¸ç”¨å‚æ•°

```rust
let request = ChatRequest {
    model: "deepseek-chat".to_string(),
    messages: vec![/* ... */],
    
    // å¯é€‰å‚æ•°
    temperature: Some(0.7),        // æ¸©åº¦ (0.0-2.0)
    top_p: Some(0.9),              // æ ¸é‡‡æ ·
    max_tokens: Some(1000),        // æœ€å¤§ç”Ÿæˆ tokens
    stream: Some(true),            // æµå¼å“åº”
    
    ..Default::default()
};
```

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `model` | String | å¿…éœ€ | æ¨¡å‹åç§° |
| `messages` | Vec<Message> | å¿…éœ€ | å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ |
| `temperature` | f32 | 1.0 | æ§åˆ¶éšæœºæ€§ (0.0-2.0) |
| `top_p` | f32 | 1.0 | æ ¸é‡‡æ ·å‚æ•° |
| `max_tokens` | u32 | - | æœ€å¤§ç”Ÿæˆ tokens |
| `stream` | bool | false | æ˜¯å¦æµå¼å“åº” |

---

## ğŸ¨ ä½¿ç”¨åœºæ™¯

### 1. æ•°å­¦æ¨ç†

åˆ©ç”¨æ¨ç†æ¨¡å‹è§£å†³æ•°å­¦é—®é¢˜ï¼š

```rust
let request = ChatRequest {
    model: "deepseek-reasoner".to_string(),
    messages: vec![Message {
        role: Role::User,
        content: "å¦‚æœä¸€ä¸ªæ•°çš„å¹³æ–¹æ˜¯ 144ï¼Œè¿™ä¸ªæ•°æ˜¯å¤šå°‘ï¼Ÿ".to_string(),
        ..Default::default()
    }],
    max_tokens: Some(500),
    ..Default::default()
};

let response = client.chat(&request).await?;

// ä¼šå±•ç¤ºå®Œæ•´çš„æ¨ç†è¿‡ç¨‹
if let Some(reasoning) = response.reasoning_content {
    println!("æ¨ç†è¿‡ç¨‹: {}", reasoning);
}
println!("ç­”æ¡ˆ: {}", response.content);
```

### 2. é€»è¾‘æ¨ç†

```rust
let request = ChatRequest {
    model: "deepseek-reasoner".to_string(),
    messages: vec![Message {
        role: Role::User,
        content: "æ‰€æœ‰çš„çŒ«éƒ½æ˜¯åŠ¨ç‰©ã€‚Fluffy æ˜¯ä¸€åªçŒ«ã€‚é‚£ä¹ˆ Fluffy æ˜¯åŠ¨ç‰©å—ï¼Ÿ".to_string(),
        ..Default::default()
    }],
    ..Default::default()
};
```

### 3. æ ‡å‡†å¯¹è¯

å¯¹äºä¸éœ€è¦æ¨ç†è¿‡ç¨‹çš„åœºæ™¯ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å‹ï¼š

```rust
let request = ChatRequest {
    model: "deepseek-chat".to_string(),
    messages: vec![Message {
        role: Role::User,
        content: "ä»‹ç»ä¸€ä¸‹ Rust ç¼–ç¨‹è¯­è¨€".to_string(),
        ..Default::default()
    }],
    ..Default::default()
};
```

### 4. å®æ—¶æµå¼æ¨ç†

```rust
use futures_util::StreamExt;

let request = ChatRequest {
    model: "deepseek-reasoner".to_string(),
    messages: vec![Message {
        role: Role::User,
        content: "è§£é‡Šä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„".to_string(),
        ..Default::default()
    }],
    stream: Some(true),
    ..Default::default()
};

let mut stream = client.chat_stream(&request).await?;

println!("ğŸ§  æ€è€ƒè¿‡ç¨‹:");
let mut in_reasoning = true;

while let Some(chunk) = stream.next().await {
    let chunk = chunk?;
    
    // æ¨ç†å†…å®¹
    if let Some(reasoning) = chunk.choices.first()
        .and_then(|c| c.delta.reasoning_content.as_ref()) {
        print!("{}", reasoning);
    }
    
    // æœ€ç»ˆç­”æ¡ˆ
    if let Some(content) = chunk.get_content() {
        if in_reasoning {
            println!("\n\nğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:");
            in_reasoning = false;
        }
        print!("{}", content);
    }
}
```

---

## ğŸ” é”™è¯¯å¤„ç†

```rust
use llm_connector::error::LlmConnectorError;

match client.chat(&request).await {
    Ok(response) => {
        println!("æˆåŠŸ: {}", response.content);
    }
    Err(LlmConnectorError::ApiError { status, message }) => {
        eprintln!("API é”™è¯¯ {}: {}", status, message);
    }
    Err(LlmConnectorError::NetworkError(e)) => {
        eprintln!("ç½‘ç»œé”™è¯¯: {}", e);
    }
    Err(e) => {
        eprintln!("å…¶ä»–é”™è¯¯: {}", e);
    }
}
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å¤ç”¨å®¢æˆ·ç«¯

```rust
// âœ… æ¨èï¼šå¤ç”¨å®¢æˆ·ç«¯
let client = LlmClient::deepseek("sk-...")?;

for _ in 0..10 {
    let response = client.chat(&request).await?;
    // å¤„ç†å“åº”
}
```

### 2. åˆç†è®¾ç½®è¶…æ—¶

```rust
// æ¨ç†æ¨¡å‹å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
let client = LlmClient::deepseek_with_config(
    "sk-...",
    None,
    Some(120),  // 120ç§’è¶…æ—¶
    None
)?;
```

### 3. ä½¿ç”¨æµå¼å“åº”

```rust
// å¯¹äºæ¨ç†æ¨¡å‹ï¼Œä½¿ç”¨æµå¼å¯ä»¥å®æ—¶çœ‹åˆ°æ€è€ƒè¿‡ç¨‹
let request = ChatRequest {
    model: "deepseek-reasoner".to_string(),
    stream: Some(true),  // å¯ç”¨æµå¼
    // ...
};
```

---

## ğŸ‰ æ€»ç»“

DeepSeek Provider çš„ç‰¹ç‚¹ï¼š

1. âœ… **OpenAI å…¼å®¹** - ä½¿ç”¨æ ‡å‡† OpenAI API æ ¼å¼
2. âœ… **æ¨ç†æ¨¡å‹** - æ”¯æŒå±•ç¤ºæ€è€ƒè¿‡ç¨‹çš„æ¨ç†æ¨¡å‹
3. âœ… **è‡ªåŠ¨æå–** - è‡ªåŠ¨æå– reasoning_content
4. âœ… **ç»Ÿä¸€è¾“å‡º** - ä¸å…¶ä»– providers è¾“å‡ºç›¸åŒçš„ `StreamingResponse`
5. âœ… **é…ç½®é©±åŠ¨** - ä½¿ç”¨ ConfigurableProtocol æ¶æ„
6. âœ… **æ˜“äºä½¿ç”¨** - ç®€æ´çš„ API: `LlmClient::deepseek("sk-...")`

**æ¨èä½¿ç”¨åœºæ™¯**ï¼š
- æ•°å­¦æ¨ç†å’Œè®¡ç®—
- é€»è¾‘æ¨ç†
- éœ€è¦å±•ç¤ºæ€è€ƒè¿‡ç¨‹çš„åœºæ™¯
- æ ‡å‡†å¯¹è¯ï¼ˆä½¿ç”¨ deepseek-chatï¼‰

**æ¨ç†æ¨¡å‹ vs æ ‡å‡†æ¨¡å‹**ï¼š
- **deepseek-reasoner**: é€‚åˆéœ€è¦æ¨ç†çš„å¤æ‚é—®é¢˜ï¼Œä¼šå±•ç¤ºæ€è€ƒè¿‡ç¨‹
- **deepseek-chat**: é€‚åˆæ ‡å‡†å¯¹è¯ï¼Œå“åº”æ›´å¿«

---

**ç›¸å…³é“¾æ¥**:
- DeepSeek å®˜ç½‘: https://www.deepseek.com/
- API æ–‡æ¡£: https://api-docs.deepseek.com/zh-cn/
- llm-connector æ–‡æ¡£: https://docs.rs/llm-connector

