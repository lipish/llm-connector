# llm-connectorï¼ˆä¸­æ–‡æ–‡æ¡£ï¼‰

æœ€å°åŒ–çš„ Rust åº“ï¼Œç”¨äºæŠ½è±¡ LLM åè®®ã€‚

æ”¯æŒ 6 ç§åè®®ï¼šOpenAIã€Anthropicã€Aliyunã€Zhipuã€Ollamaã€Hunyuanã€‚
æ— éœ€å¤æ‚é…ç½®â€”â€”é€‰æ‹©åè®®å³å¯å¼€å§‹èŠå¤©ã€‚

## ğŸš¨ èº«ä»½éªŒè¯é—®é¢˜ï¼Ÿ

ç«‹å³æµ‹è¯•ä½ çš„ API Keyï¼š
```bash
cargo run --example test_keys_yaml
```

è¿™å°†å‘Šè¯‰ä½  API Key çš„å…·ä½“é—®é¢˜ï¼è¯¦è§ä¸‹æ–‡ã€Œè°ƒè¯•ä¸æ’éšœã€ã€‚

## âœ¨ å…³é”®ç‰¹æ€§

- 6 åè®®æ”¯æŒï¼šOpenAIã€Anthropicã€Aliyunã€Zhipuã€Ollamaã€Hunyuan
- æ— ç¡¬ç¼–ç æ¨¡å‹é™åˆ¶ï¼šå¯ä½¿ç”¨ä»»æ„æ¨¡å‹åç§°
- åœ¨çº¿æ¨¡å‹å‘ç°ï¼šä» API åŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨
- å¢å¼ºæµå¼æ”¯æŒï¼šå®æ—¶æµå¼å“åº”ï¼Œå¹¶æ­£ç¡®å¤„ç† Anthropic äº‹ä»¶
- Ollama æ¨¡å‹ç®¡ç†ï¼šæœ¬åœ°æ¨¡å‹å®Œæ•´å¢åˆ æ”¹æŸ¥
- ç»Ÿä¸€æ¥å£ï¼šè·¨åè®®ä¸€è‡´çš„è°ƒç”¨æ–¹å¼
- ç±»å‹å®‰å…¨ï¼šRust å¼‚æ­¥/ç­‰å¾…å…¨ç±»å‹ä¿éšœ

## å¿«é€Ÿå¼€å§‹

### å®‰è£…

åœ¨ `Cargo.toml` ä¸­æ·»åŠ ï¼š

```toml
[dependencies]
llm-connector = "0.3.6"
tokio = { version = "1", features = ["full"] }
```

å¯é€‰åŠŸèƒ½ï¼š
```toml
# æµå¼å“åº”æ”¯æŒ
llm-connector = { version = "0.3.6", features = ["streaming"] }

# è…¾è®¯äº‘åŸç”Ÿ API æ”¯æŒ
llm-connector = { version = "0.3.6", features = ["tencent-native"] }

# åŒæ—¶å¯ç”¨æµå¼å“åº”å’Œè…¾è®¯äº‘åŸç”Ÿ API
llm-connector = { version = "0.3.6", features = ["streaming", "tencent-native"] }
```

### åŸºæœ¬ç”¨æ³•

```rust
use llm_connector::{LlmClient, ChatRequest, Message};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // OpenAIï¼ˆé»˜è®¤ base URLï¼‰
    let client = LlmClient::openai("sk-...", None);

    // Anthropic
    let client = LlmClient::anthropic("sk-ant-...");

    // Aliyunï¼ˆDashScopeï¼‰
    let client = LlmClient::aliyun("sk-...");

    // è…¾è®¯æ··å…ƒ
    let client = LlmClient::hunyuan("sk-...");

    // Ollamaï¼ˆæœ¬åœ°ï¼Œæ— éœ€ API Keyï¼‰
    let client = LlmClient::ollama(None);

    let request = ChatRequest {
        model: "gpt-4".to_string(),
        messages: vec![Message::user("Hello!")],
        ..Default::default()
    };

    let response = client.chat(&request).await?;
    println!("Response: {}", response.choices[0].message.content);
    Ok(())
}
```

## åè®®æ”¯æŒ

### 1. OpenAI åè®®
æ ‡å‡† OpenAI API æ ¼å¼ã€‚

```rust
// OpenAIï¼ˆé»˜è®¤ base URLï¼‰
let client = LlmClient::openai("sk-...", None);

// OpenAI å…¼å®¹ç«¯ç‚¹ï¼ˆè‡ªå®šä¹‰ base URLï¼‰
let client = LlmClient::openai("sk-...", Some("https://api.example.com/v1"));
```

ç‰¹æ€§ï¼š
- æ— ç¡¬ç¼–ç æ¨¡å‹é™åˆ¶â€”â€”å¯ä½¿ç”¨ä»»æ„æ¨¡å‹åç§°
- é€šè¿‡ `fetch_models()` åœ¨çº¿å‘ç°æ¨¡å‹
- æ”¯æŒ OpenAI å…¼å®¹æä¾›å•†ï¼ˆDeepSeekã€Zhipuã€Moonshot ç­‰ï¼‰

ç¤ºä¾‹æ¨¡å‹ï¼šgpt-4ã€gpt-4-turboã€gpt-3.5-turboã€o1-previewã€o1-mini

### 2. Anthropic åè®®
Claude Messages APIï¼Œç³»ç»Ÿæ¶ˆæ¯ç‹¬ç«‹ã€‚

```rust
let client = LlmClient::anthropic("sk-ant-...");
```

æ¨¡å‹ï¼šclaude-3-5-sonnet-20241022ã€claude-3-opusã€claude-3-haiku

### 3. Aliyun åè®®ï¼ˆDashScopeï¼‰
Qwen æ¨¡å‹ä¸“ç”¨åè®®ã€‚

```rust
let client = LlmClient::aliyun("sk-...");
```

æ¨¡å‹ï¼šqwen-turboã€qwen-plusã€qwen-max

### 4. è…¾è®¯æ··å…ƒåè®®
è…¾è®¯æ··å…ƒæ¨¡å‹æä¾›ä¸¤ç§å®ç°æ–¹å¼ï¼š

#### 4.1 OpenAI å…¼å®¹æ¥å£
```rust
let client = LlmClient::hunyuan("sk-...");
```

ç‰¹æ€§ï¼š
- OpenAI å…¼å®¹ API æ ¼å¼
- æ”¯æŒæµå¼å“åº”
- é€šè¿‡ `fetch_models()` åœ¨çº¿å‘ç°æ¨¡å‹

#### 4.2 è…¾è®¯äº‘åŸç”Ÿ APIï¼ˆæ¨èï¼‰
```rust
// éœ€è¦å¯ç”¨ "tencent-native" åŠŸèƒ½
let client = LlmClient::hunyuan_native("secret-id", "secret-key", Some("ap-beijing"));
```

ç‰¹æ€§ï¼š
- è…¾è®¯äº‘åŸç”Ÿ APIï¼Œä½¿ç”¨ TC3-HMAC-SHA256 ç­¾å
- å®Œæ•´è®¿é—®è…¾è®¯äº‘åŠŸèƒ½
- æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œè°ƒè¯•
- æ”¯æŒæµå¼å“åº”
- æ”¯æŒåœ°åŸŸæŒ‡å®š

æ¨¡å‹ï¼šhunyuan-liteã€hunyuan-standardã€hunyuan-pro

### 5. Ollama åè®®ï¼ˆæœ¬åœ°ï¼‰
æœ¬åœ° LLM æœåŠ¡ï¼Œæ— éœ€ API Keyã€‚

```rust
// é»˜è®¤ï¼šlocalhost:11434
let client = LlmClient::ollama(None);

// è‡ªå®šä¹‰ URL
let client = LlmClient::ollama(Some("http://192.168.1.100:11434"));
```

æ¨¡å‹ï¼šllama3.2ã€llama3.1ã€mistralã€mixtralã€qwen2.5 ç­‰ã€‚

ç‰¹æ€§ï¼š
- é€šè¿‡ `/api/tags` åˆ—å‡ºæ¨¡å‹
- æ¨¡å‹ç®¡ç†ï¼ˆæ‹‰å–ã€æ¨é€ã€åˆ é™¤ã€è¯¦æƒ…ï¼‰
- æ”¯æŒæœ¬åœ°æœåŠ¡ä¸è‡ªå®šä¹‰ URL
- é’ˆå¯¹ Ollama æ“ä½œå¢å¼ºé”™è¯¯å¤„ç†

## Ollama æ¨¡å‹ç®¡ç†

æä¾›å®Œæ•´çš„ Ollama æ¨¡å‹ç®¡ç†èƒ½åŠ›ï¼š

```rust
use llm_connector::ollama::OllamaModelOps;
let client = LlmClient::ollama();

// åˆ—å‡ºæœ¬åœ°å®‰è£…çš„æ‰€æœ‰æ¨¡å‹
let models = client.list_models().await?;
for model in models {
    println!("Available model: {}", model);
}

// æ‹‰å–æ¨¡å‹
client.pull_model("llama3.2").await?;

// è·å–æ¨¡å‹è¯¦æƒ…
let details = client.show_model("llama3.2").await?;
println!("Model size: {} bytes", details.size.unwrap_or(0));

// åˆ é™¤æ¨¡å‹
client.delete_model("llama3.2").await?;
```

æ”¯æŒçš„ Ollama æ“ä½œï¼š
- åˆ—å‡ºæ¨¡å‹ï¼š`list_models()`
- æ‹‰å–æ¨¡å‹ï¼š`pull_model(name)`
- æ¨é€æ¨¡å‹ï¼š`push_model(name)`
- åˆ é™¤æ¨¡å‹ï¼š`delete_model(name)`
- æ¨¡å‹è¯¦æƒ…ï¼š`show_model(name)`

## å¢å¼ºçš„æµå¼æ”¯æŒ

å¯¹ Anthropic çš„æµå¼æ”¯æŒè¿›è¡Œäº†æ”¹è¿›ï¼ŒåŒ…å«å®Œæ•´äº‹ä»¶çŠ¶æ€ç®¡ç†ï¼š

```rust
use futures_util::StreamExt;

let client = LlmClient::anthropic("sk-ant-...");
let request = ChatRequest {
    model: "claude-3-5-sonnet-20241022".to_string(),
    messages: vec![Message::user("Hello!")],
    max_tokens: Some(200),
    ..Default::default()
};

let mut stream = client.chat_stream(&request).await?;

while let Some(chunk) = stream.next().await {
    let chunk = chunk?;
    if let Some(content) = chunk.get_content() {
        print!("{}", content);
    }
}
```

å¢å¼ºç‚¹ï¼š
- çŠ¶æ€ç®¡ç†ï¼šæ­£ç¡®å¤„ç† `message_start`ã€`content_block_delta`ã€`message_delta`ã€`message_stop`
- äº‹ä»¶å¤„ç†ï¼šè§£æå¤æ‚çš„ Anthropic æµå¼å“åº”
- ç”¨é‡è·Ÿè¸ªï¼šæµå¼è¿‡ç¨‹ä¸­å®æ—¶ç»Ÿè®¡ token ç”¨é‡
- é”™è¯¯éŸ§æ€§ï¼šå¯¹æµå¼ä¸­æ–­çš„é²æ£’å¤„ç†

## æ¨¡å‹å‘ç°

ä» API è·å–æœ€æ–°å¯ç”¨æ¨¡å‹ï¼š

```rust
let client = LlmClient::openai("sk-...");

// ä» API åœ¨çº¿è·å–æ¨¡å‹
let models = client.fetch_models().await?;
println!("Available models: {:?}", models);
```

æ”¯æŒæƒ…å†µï¼š
- OpenAI åè®®ï¼ˆåŒ…å« OpenAI å…¼å®¹æœåŠ¡ï¼Œå¦‚ DeepSeekã€Zhipuã€Moonshot ç­‰ï¼‰
- Anthropic åè®®ï¼ˆæœ‰é™æ”¯æŒâ€”â€”è¿”å›å›é€€ç«¯ç‚¹ï¼‰
- Ollama åè®®ï¼ˆé€šè¿‡ `/api/tags` å®Œæ•´æ”¯æŒï¼‰
- Aliyun åè®®ï¼ˆä¸æ”¯æŒï¼‰

ç¤ºä¾‹ç»“æœï¼š
- DeepSeekï¼š`["deepseek-chat", "deepseek-reasoner"]`
- Zhipuï¼š`["glm-4.5", "glm-4.5-air", "glm-4.6"]`
- Moonshotï¼š`["moonshot-v1-32k", "kimi-latest", ...]`

æ¨èï¼š
- ç¼“å­˜ `fetch_models()` ç»“æœï¼Œé¿å…é‡å¤è¯·æ±‚
- å¯¹ä¸æ”¯æŒæ¨¡å‹åˆ—è¡¨çš„åè®®ï¼Œå¯ç›´æ¥ä½¿ç”¨ä»»æ„æ¨¡å‹å

## è¯·æ±‚ç¤ºä¾‹

### OpenAI / OpenAI å…¼å®¹

```rust
let request = ChatRequest {
    model: "gpt-4".to_string(),
    messages: vec![
        Message::system("You are a helpful assistant."),
        Message::user("Hello!"),
    ],
    temperature: Some(0.7),
    max_tokens: Some(100),
    ..Default::default()
};
```

### Anthropicï¼ˆéœ€è¦ max_tokensï¼‰

```rust
let request = ChatRequest {
    model: "claude-3-5-sonnet-20241022".to_string(),
    messages: vec![Message::user("Hello!")],
    max_tokens: Some(200), // Anthropic å¿…å¡«
    ..Default::default()
};
```

### Aliyunï¼ˆDashScopeï¼‰

```rust
let request = ChatRequest {
    model: "qwen-max".to_string(),
    messages: vec![Message::user("ä½ å¥½ï¼")],
    ..Default::default()
};
```

### Ollamaï¼ˆæœ¬åœ°ï¼‰

```rust
let request = ChatRequest {
    model: "llama3.2".to_string(),
    messages: vec![Message::user("Hello!")],
    ..Default::default()
};
```

#### Ollama æµå¼ï¼ˆé€šè¿‡è¿œç¨‹ç½‘å…³è°ƒç”¨ GLM-4.6ï¼‰

å¦‚æœä½ å¯¹å¤–æä¾› Ollama å…¼å®¹ APIï¼Œä½†åç«¯è°ƒç”¨ Zhipu çš„ `glm-4.6`ï¼ˆè¿œç¨‹ç½‘å…³ï¼‰ï¼Œæ— éœ€æœ¬åœ°å®‰è£…æ¨¡å‹ã€‚å°†å®¢æˆ·ç«¯æŒ‡å‘ä½ çš„ç½‘å…³åœ°å€ï¼Œå¹¶ä½¿ç”¨æœåŠ¡ç«¯å®šä¹‰çš„æ¨¡å‹æ ‡è¯†ï¼š

```rust
use futures_util::StreamExt;
use llm_connector::{LlmClient, types::{ChatRequest, Message}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // æŒ‡å‘ä½ çš„è¿œç¨‹ Ollama å…¼å®¹ç½‘å…³ï¼ˆæ›¿æ¢ä¸ºä½ çš„å®é™…åœ°å€ï¼‰
    let client = LlmClient::ollama(Some("https://your-ollama-gateway.example.com"));

    let request = ChatRequest {
        model: "glm-4.6".to_string(),
        messages: vec![Message::user("Briefly explain the benefits of streaming.")],
        max_tokens: Some(128),
        ..Default::default()
    };

    let mut stream = client.chat_stream(&request).await?;
    while let Some(chunk) = stream.next().await {
        let chunk = chunk?;
        if let Some(content) = chunk.get_content() {
            print!("{}", content);
        }
    }
    Ok(())
}
```

è¿è¡Œç¤ºä¾‹ï¼ˆéœ€å¯ç”¨ `streaming` åŠŸèƒ½ï¼‰ï¼š

```bash
cargo run --example ollama_streaming --features streaming
```

è¯´æ˜ï¼šè¯¥æ–¹æ¡ˆé¢å‘è¿œç¨‹ Ollama å…¼å®¹ç½‘å…³ã€‚æ¨¡å‹æ ‡è¯†ç”±åç«¯å®šä¹‰ï¼ˆå¦‚ `glm-4.6`ï¼‰ï¼Œæ— éœ€æœ¬åœ°å®‰è£…ã€‚å¦‚æœä½ çš„ç½‘å…³ä½¿ç”¨ä¸åŒæ ‡è¯†ï¼Œè¯·æ›¿æ¢ä¸ºå®é™…å€¼ã€‚

## æµå¼ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰

åœ¨ `Cargo.toml` å¯ç”¨æµå¼ï¼š
```toml
llm-connector = { version = "0.3.6", features = ["streaming"] }
```

```rust
use futures_util::StreamExt;

let mut stream = client.chat_stream(&request).await?;

while let Some(chunk) = stream.next().await {
    let chunk = chunk?;
    if let Some(content) = chunk.get_content() {
        print!("{}", content);
    }
}
```

## é”™è¯¯å¤„ç†

```rust
use llm_connector::error::LlmConnectorError;

match client.chat(&request).await {
    Ok(response) => {
        println!("Response: {}", response.choices[0].message.content);
    }
    Err(e) => {
        match e {
            LlmConnectorError::AuthenticationError(msg) => {
                eprintln!("Auth error: {}", msg);
            }
            LlmConnectorError::RateLimitError(msg) => {
                eprintln!("Rate limit: {}", msg);
            }
            LlmConnectorError::UnsupportedOperation(msg) => {
                eprintln!("Not supported: {}", msg);
            }
            _ => eprintln!("Error: {}", e),
        }
    }
}
```

## é…ç½®

### ç®€å• API Keyï¼ˆæ¨èï¼‰

```rust
let client = LlmClient::openai("your-api-key");
```

### ç¯å¢ƒå˜é‡

```bash
export OPENAI_API_KEY="sk-your-key"
export ANTHROPIC_API_KEY="sk-ant-your-key"
export ALIYUN_API_KEY="sk-your-key"
```

```rust
use std::env;

let api_key = env::var("OPENAI_API_KEY")?;
let client = LlmClient::openai(&api_key, None);
```

## åè®®ä¿¡æ¯

```rust
let client = LlmClient::openai("sk-...");

// è·å–åè®®åç§°
println!("Protocol: {}", client.protocol_name());

// åœ¨çº¿è·å–æ¨¡å‹ï¼ˆéœ€è¦ API è¯·æ±‚ï¼‰
let models = client.fetch_models().await?;
println!("Available models: {:?}", models);
```

## æ¨ç†å†…å®¹åˆ«åï¼ˆReasoning Synonymsï¼‰

è®¸å¤šæä¾›å•†ä¼šè¿”å›éšè—æˆ–ç§æœ‰çš„æ¨ç†å†…å®¹é”®ã€‚ä¸ºç®€åŒ–è·¨åè®®ä½¿ç”¨ï¼Œæˆ‘ä»¬ç»Ÿä¸€å½’ä¸€åŒ–å››ä¸ªå¸¸è§é”®ï¼š

- `reasoning_content`ã€`reasoning`ã€`thought`ã€`thinking`

åå¤„ç†ä¼šè‡ªåŠ¨æ‰«æåŸå§‹ JSONï¼Œå¹¶å¡«å……æ¶ˆæ¯ï¼ˆ`Message`ï¼‰ä¸æµå¼å¢é‡ï¼ˆ`Delta`ï¼‰ä¸Šçš„è¿™äº›å¯é€‰å­—æ®µã€‚å¯é€šè¿‡ä¾¿æ·æ–¹æ³•è¯»å–é¦–ä¸ªå¯ç”¨å€¼ï¼š

```rust
// éæµå¼
let msg = &response.choices[0].message;
if let Some(reason) = msg.reasoning_any() {
    println!("Reasoning: {}", reason);
}

// æµå¼
while let Some(chunk) = stream.next().await {
    let chunk = chunk?;
    if let Some(reason) = chunk.choices[0].delta.reasoning_any() {
        println!("Reasoning (stream): {}", reason);
    }
}
```

è¯´æ˜ï¼š
- å¦‚æœæä¾›å•†ä¸è¿”å›ä»»ä½•æ¨ç†é”®ï¼Œè¿™äº›å­—æ®µä¿æŒ `None`
- å½’ä¸€åŒ–ä¸æä¾›å•†æ— å…³ï¼Œç»Ÿä¸€é€‚ç”¨äº OpenAIã€Anthropicã€Aliyunï¼ˆQwenï¼‰ã€Zhipuï¼ˆGLMï¼‰ã€DeepSeek ç­‰ï¼ˆå«æµå¼ï¼‰
- `StreamingResponse` ä¹Ÿä¼šä»é¦–ä¸ªåŒ…å«æ¨ç†çš„å¢é‡å›å¡«å…¶é¡¶å±‚ `reasoning_content`

## è°ƒè¯•ä¸æ’éšœ

### æµ‹è¯•ä½ çš„ API Key

å¿«é€ŸéªŒè¯ API Key æ˜¯å¦æœ‰æ•ˆï¼š

```bash
# ä» keys.yaml æµ‹è¯•æ‰€æœ‰å¯†é’¥
cargo run --example test_keys_yaml

# ä¸“é—¨è°ƒè¯• DeepSeek
cargo run --example debug_deepseek -- sk-your-key
```

æµ‹è¯•å·¥å…·å°†ï¼š
- éªŒè¯ API Key æ ¼å¼
- æµ‹è¯•ä¸æä¾›å•†çš„èº«ä»½éªŒè¯
- ç²¾ç¡®æ˜¾ç¤ºé—®é¢˜åŸå› 
- æä¾›å…·ä½“ä¿®å¤å»ºè®®

### æ’éšœæ–‡æ¡£

- `TROUBLESHOOTING.md` â€” ç»¼åˆæ’éšœæŒ‡å—
- `HOW_TO_TEST_YOUR_KEYS.md` â€” å¦‚ä½•æµ‹è¯•ä½ çš„ API Key
- `TEST_YOUR_DEEPSEEK_KEY.md` â€” DeepSeek å¿«é€Ÿå¼€å§‹

### å¸¸è§é—®é¢˜

**èº«ä»½éªŒè¯é”™è¯¯ï¼š**
```
âŒ Authentication failed: Incorrect API key provided
```

**è§£å†³æ–¹æ¡ˆï¼š**
1. æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®ï¼ˆæ— å¤šä½™ç©ºæ ¼ï¼‰
2. ç¡®è®¤è´¦æˆ·æ˜¯å¦æœ‰é¢åº¦/æƒé™
3. åœ¨æä¾›å•†æ§åˆ¶å°ç”Ÿæˆæ–°çš„ API Key
4. è¿è¡Œ `cargo run --example test_keys_yaml` è¿›è¡Œè¯Šæ–­

## æœ€è¿‘å˜æ›´

### v0.3.1ï¼ˆæœ€æ–°ï¼‰

é‡å¤§æ–°ç‰¹æ€§ï¼š
- å®Œæ•´çš„ Ollama æ¨¡å‹ç®¡ç†ï¼šæœ¬åœ°æ¨¡å‹å¢åˆ æ”¹æŸ¥
  - `list_models()`ã€`pull_model()`ã€`push_model()`ã€`delete_model()`ã€`show_model()`
- å¢å¼ºçš„ Anthropic æµå¼ï¼šæ­£ç¡®çš„äº‹ä»¶çŠ¶æ€ç®¡ç†
  - å¤„ç† `message_start`ã€`content_block_delta`ã€`message_delta`ã€`message_stop`
  - æµå¼è¿‡ç¨‹å®æ—¶ token ç”¨é‡è·Ÿè¸ª
  - æ”¹è¿›çš„é”™è¯¯éŸ§æ€§ä¸çŠ¶æ€ç®¡ç†

æ”¹è¿›ï¼š
- æ‰©å±•æ¨¡å‹å‘ç°æ”¯æŒï¼š
  - å¢åŠ  Ollama é€šè¿‡ `/api/tags` åˆ—å‡ºæ¨¡å‹
  - Anthropic æ¨¡å‹å‘ç°æœ‰é™æ”¯æŒ
- å¢å¼ºå®¢æˆ·ç«¯æ¥å£ï¼šæä¾› Ollama æ¨¡å‹ç®¡ç†æ–¹æ³•
- æ›´æ–°ç¤ºä¾‹ï¼šå¢åŠ æ¨¡å‹ç®¡ç†ä¸æµå¼ç¤ºä¾‹

æ–‡æ¡£ï¼š
- é‡å†™ Ollama ç« èŠ‚å¹¶åŠ å…¥æ¨¡å‹ç®¡ç†ç¤ºä¾‹
- å¢å¼ºæµå¼æ–‡æ¡£ä¸ä»£ç ç¤ºä¾‹
- æ›´æ–°ç‰¹æ€§æè¿°ä¸æ”¯æŒæ“ä½œ

### v0.2.3

ä¸å…¼å®¹å˜æ›´ï¼š
- ç§»é™¤ `supported_models()`ï¼ˆä½¿ç”¨ `fetch_models()`ï¼‰
- ç§»é™¤ `supports_model()`

æ–°ç‰¹æ€§ï¼š
- æ”¹è¿›é”™è¯¯ä¿¡æ¯ï¼ˆå»é™¤å¯¹å…¶ä»–æä¾›å•†æ··æ·†çš„ OpenAI é“¾æ¥ï¼‰
- æ–°è°ƒè¯•å·¥å…·ï¼š
  - `examples/test_keys_yaml.rs` â€” æµ‹è¯•æ‰€æœ‰ API Key
  - `examples/debug_deepseek.rs` â€” è°ƒè¯• DeepSeek èº«ä»½éªŒè¯
- ç»¼åˆæ–‡æ¡£ï¼š
  - `TROUBLESHOOTING.md`ã€`HOW_TO_TEST_YOUR_KEYS.md`ã€`TEST_YOUR_DEEPSEEK_KEY.md`

ä» v0.2.2 è¿ç§»ï¼š
```rust
// æ—§ï¼ˆå·²ä¸æ”¯æŒï¼‰
let models = client.supported_models();

// æ–°
let models = client.fetch_models().await?;
```

### v0.2.2

æ–°ç‰¹æ€§ï¼š
- å¢åŠ  `fetch_models()` åœ¨çº¿æ¨¡å‹å‘ç°
- OpenAI åè®®æ”¯æŒä» `/v1/models` åŠ¨æ€æ‹‰å–
- é€‚é… OpenAI å…¼å®¹æä¾›å•†ï¼ˆDeepSeekã€Zhipuã€Moonshot ç­‰ï¼‰

## è®¾è®¡å“²å­¦

æœ€å°åŒ–è®¾è®¡ï¼š
- ä»… 4 ç§åè®®è¦†ç›–å¤šæ•°ä¸»æµæä¾›å•†
- æ— ç¡¬ç¼–ç æ¨¡å‹é™åˆ¶â€”â€”ä»»æ„æ¨¡å‹åå¯ç”¨
- æ— å¤æ‚é…ç½®æ–‡ä»¶æˆ–æ³¨å†Œä¸­å¿ƒ
- ç›´æ¥ API è°ƒç”¨ï¼ŒæŠ½è±¡æ¸…æ™°

åè®®ä¼˜å…ˆï¼š
- æŒ‰åè®®è€Œéå…¬å¸å½’ç±»æä¾›å•†
- OpenAI å…¼å®¹æä¾›å•†å…±äº«ä¸€å¥—å®ç°
- é€šè¿‡åè®®é€‚é…å™¨å¯æ‰©å±•

## ç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•ï¼š

```bash
# ä» keys.yaml æµ‹è¯•ä½ çš„ API Key
cargo run --example test_keys_yaml

# è°ƒè¯• DeepSeek èº«ä»½éªŒè¯
cargo run --example debug_deepseek -- sk-your-key

# ç®€å• fetch_models() æ¼”ç¤º
cargo run --example fetch_models_simple

# Ollama æ¨¡å‹ç®¡ç†ï¼ˆNEW!ï¼‰
cargo run --example ollama_model_management

# Anthropic æµå¼ï¼ˆNEW!ï¼Œéœ€å¯ç”¨ streamingï¼‰
cargo run --example anthropic_streaming --features streaming

# Ollama æµå¼ï¼ˆNEW!ï¼Œéœ€å¯ç”¨ streamingï¼‰
cargo run --example ollama_streaming --features streaming

# LongCat æ¼”ç¤ºï¼ˆå…¼å®¹ OpenAI/Anthropicï¼‰
cargo run --example longcat_dual
```

### ç¤ºä¾‹è¯´æ˜

`test_keys_yaml.rs`ï¼ˆæ–°ï¼‰ï¼š
- æµ‹è¯• `keys.yaml` ä¸­çš„æ‰€æœ‰ API Key
- éªŒè¯å¯†é’¥æ ¼å¼ä¸èº«ä»½è®¤è¯
- ä¸ºæ¯ç§é”™è¯¯æä¾›å…·ä½“æ’éšœå»ºè®®
- å¦‚æœä½ é‡åˆ°è®¤è¯é—®é¢˜ï¼Œè¯·å…ˆè¿è¡Œå®ƒï¼

`debug_deepseek.rs`ï¼ˆæ–°ï¼‰ï¼š
- DeepSeek API äº¤äº’å¼è°ƒè¯•å·¥å…·
- éªŒè¯ API Key æ ¼å¼
- æµ‹è¯•æ¨¡å‹æ‹‰å–ä¸èŠå¤©è¯·æ±‚
- æä¾›è¯¦ç»†æ’éšœæŒ‡å¯¼

`fetch_models_simple.rs`ï¼š
- ç®€å•å±•ç¤º `fetch_models()`
- å±•ç¤ºå¦‚ä½•ä» OpenAI å…¼å®¹æä¾›å•†è·å–æ¨¡å‹
- é™„å¸¦ä½¿ç”¨å»ºè®®

`ollama_model_management.rs`ï¼ˆæ–°ï¼‰ï¼š
- å±•ç¤ºå®Œæ•´çš„ Ollama æ¨¡å‹ç®¡ç†åŠŸèƒ½
- åˆ—å‡ºã€æ‹‰å–ã€åˆ é™¤ã€è·å–æ¨¡å‹è¯¦æƒ…
- åŒ…å«é”™è¯¯å¤„ç†ä¸å®ç”¨ç¤ºä¾‹

`anthropic_streaming.rs`ï¼ˆæ–°ï¼‰ï¼š
- å±•ç¤ºå¢å¼ºçš„ Anthropic æµå¼äº‹ä»¶å¤„ç†
- å®æ—¶å“åº”æµä¸ç”¨é‡ç»Ÿè®¡
- åŒæ—¶åŒ…å«å¸¸è§„ä¸æµå¼èŠå¤©ç¤ºä¾‹

å·²ç§»é™¤å†—ä½™ç¤ºä¾‹ï¼š
- `test_fetch_models.rs`ã€`test_with_keys.rs` ä¸å…¶ä»–ç¤ºä¾‹é‡å ï¼Œå·²ç§»é™¤

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼æ¬¢è¿æäº¤ Pull Requestã€‚

## è®¸å¯åè®®

MIT