//! Volcengine (ç«å±±å¼•æ“) streaming example
//!
//! æµ‹è¯• Volcengine Ark API çš„æµå¼å“åº”
//!
//! ä½¿ç”¨æ–¹æ³•:
//! ```bash
//! cargo run --example volcengine_streaming --features streaming -- <api_key> <endpoint>
//! ```
//!
//! ç¤ºä¾‹:
//! ```bash
//! cargo run --example volcengine_streaming --features streaming -- \
//!   xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx \
//!   ep-20250118155555-xxxxx
//! ```

use llm_connector::providers::volcengine_with_config;
use llm_connector::types::{ChatRequest, Message, Role, MessageBlock};
use std::env;

#[cfg(feature = "streaming")]
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {

    let args: Vec<String> = env::args().collect();
    if args.len() < 3 {
        eprintln!("Usage: {} <api_key> <endpoint>", args[0]);
        eprintln!("Example: {} xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx ep-20250118155555-xxxxx", args[0]);
        std::process::exit(1);
    }

    let api_key = &args[1];
    let endpoint = &args[2];

    println!("ğŸ”§ Creating Volcengine provider...");
    println!("   API Key: {}...{}", &api_key[..8], &api_key[api_key.len()-4..]);
    println!("   Endpoint: {}", endpoint);

    let provider = volcengine_with_config(
        api_key,
        None, // ä½¿ç”¨é»˜è®¤ URL: https://ark.cn-beijing.volces.com
        Some(60),
        None,
    )?;

    let request = ChatRequest {
        model: endpoint.to_string(), // Volcengine ä½¿ç”¨ endpoint ID ä½œä¸º model
        messages: vec![Message {
            role: Role::User,
            content: vec![MessageBlock::Text {
                text: "ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±".to_string(),
            }],
            name: None,
            tool_calls: None,
            tool_call_id: None,
            reasoning: None,
            reasoning_content: None,
            thinking: None,
            thought: None,
        }],
        temperature: Some(0.7),
        max_tokens: Some(100),
        stream: Some(true),
        ..Default::default()
    };

    println!("\nğŸ“¤ Sending streaming request...");
    println!("   Model: {}", request.model);
    println!("   Message: {:?}", request.messages[0].content);

    #[cfg(feature = "streaming")]
    {
        use llm_connector::core::Provider;
        
        let mut stream = provider.chat_stream(&request).await?;
        
        println!("\nğŸ“¥ Receiving streaming response:");
        println!("---");
        
        let mut chunk_count = 0;
        let mut total_content = String::new();
        
        while let Some(chunk_result) = stream.next().await {
            match chunk_result {
                Ok(chunk) => {
                    chunk_count += 1;
                    
                    // è°ƒè¯•ï¼šæ‰“å°å®Œæ•´çš„ chunk ç»“æ„
                    if chunk_count <= 3 {
                        println!("\n[DEBUG] Chunk #{}: {:?}", chunk_count, chunk);
                    }
                    
                    // å°è¯•è·å–å†…å®¹
                    if let Some(content) = chunk.get_content() {
                        print!("{}", content);
                        total_content.push_str(content);
                    } else {
                        // å¦‚æœ get_content() ä¸ºç©ºï¼Œæ£€æŸ¥åŸå§‹æ•°æ®
                        if !chunk.choices.is_empty() {
                            let choice = &chunk.choices[0];
                            println!("\n[DEBUG] Choice delta: {:?}", choice.delta);
                        }
                    }
                    
                    // æ£€æŸ¥æ˜¯å¦æœ‰ finish_reason
                    if let Some(choice) = chunk.choices.first() {
                        if let Some(reason) = &choice.finish_reason {
                            println!("\n\n[Finish reason: {}]", reason);
                        }
                    }
                }
                Err(e) => {
                    eprintln!("\nâŒ Error in stream: {}", e);
                    break;
                }
            }
        }
        
        println!("\n---");
        println!("\nâœ… Streaming completed!");
        println!("   Total chunks: {}", chunk_count);
        println!("   Total content length: {} chars", total_content.len());
        
        if total_content.is_empty() {
            println!("\nâš ï¸  WARNING: No content received! This indicates a streaming parsing issue.");
        } else {
            println!("\nğŸ“ Complete response:");
            println!("{}", total_content);
        }
    }

    #[cfg(not(feature = "streaming"))]
    {
        println!("âŒ Streaming feature not enabled. Please run with --features streaming");
    }

    Ok(())
}

