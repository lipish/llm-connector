use llm_connector::{LlmClient, types::{ChatRequest, Message, StreamingConfig, StreamingFormat}};

#[cfg(feature = "streaming")]
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    #[cfg(not(feature = "streaming"))]
    {
        println!("âŒ æ­¤ç¤ºä¾‹éœ€è¦å¯ç”¨ streaming åŠŸèƒ½");
        println!("è¯·ä½¿ç”¨: cargo run --example streaming_ollama_format --features streaming");
        return Ok(());
    }

    #[cfg(feature = "streaming")]
    {
        // ä½¿ç”¨æ™ºè°±AIä½œä¸ºç¤ºä¾‹ï¼ˆä½ å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–æä¾›å•†ï¼‰
        let api_key = std::env::var("ZHIPU_API_KEY")
            .expect("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPU_API_KEY");

        let client = LlmClient::zhipu(&api_key);

        let request = ChatRequest {
            model: "glm-4-flash".to_string(),
            messages: vec![Message::user("è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ã€‚")],
            max_tokens: Some(200),
            ..Default::default()
        };

        println!("ğŸš€ Ollamaæ ¼å¼æµå¼è¾“å‡ºæ¼”ç¤º\n");
        println!("ğŸ“‹ å¯¹æ¯”ä¸¤ç§æ ¼å¼çš„è¾“å‡ºï¼š\n");

        // 1. æ¼”ç¤ºOpenAIæ ¼å¼ï¼ˆé»˜è®¤ï¼‰
        println!("ğŸ”¹ OpenAIæ ¼å¼æµå¼è¾“å‡ºï¼š");
        println!("{}", "=".repeat(50));
        
        let mut openai_stream = client.chat_stream(&request).await?;
        let mut openai_content = String::new();
        
        while let Some(chunk) = openai_stream.next().await {
            match chunk {
                Ok(response) => {
                    if !response.content.is_empty() {
                        print!("{}", response.content);
                        openai_content.push_str(&response.content);
                        std::io::Write::flush(&mut std::io::stdout()).unwrap();
                    }
                    
                    // æ˜¾ç¤ºåŸå§‹JSONæ ¼å¼ï¼ˆä»…ç¬¬ä¸€ä¸ªchunkï¼‰
                    if openai_content.len() < 10 {
                        println!("\n[OpenAI JSONç¤ºä¾‹]:");
                        println!("{}", serde_json::to_string_pretty(&response)?);
                        println!();
                    }
                }
                Err(e) => {
                    println!("\nâŒ OpenAIæ ¼å¼é”™è¯¯ï¼š{}", e);
                    break;
                }
            }
        }
        
        println!("\n");
        println!("{}", "=".repeat(50));
        println!();

        // 2. æ¼”ç¤ºOllamaæ ¼å¼
        println!("ğŸ”¹ Ollamaæ ¼å¼æµå¼è¾“å‡ºï¼š");
        println!("{}", "=".repeat(50));

        // ä½¿ç”¨ä¾¿åˆ©æ–¹æ³•
        let mut ollama_stream = client.chat_stream_ollama(&request).await?;
        let mut ollama_content = String::new();
        let mut chunk_count = 0;
        
        while let Some(chunk) = ollama_stream.next().await {
            match chunk {
                Ok(response) => {
                    chunk_count += 1;
                    
                    // response.content ç°åœ¨åŒ…å«Ollamaæ ¼å¼çš„JSONå­—ç¬¦ä¸²
                    if !response.content.is_empty() {
                        // è§£æOllama JSONæ¥æå–å®é™…å†…å®¹
                        if let Ok(ollama_chunk) = serde_json::from_str::<serde_json::Value>(&response.content) {
                            if let Some(content) = ollama_chunk
                                .get("message")
                                .and_then(|m| m.get("content"))
                                .and_then(|c| c.as_str()) 
                            {
                                if !content.is_empty() {
                                    print!("{}", content);
                                    ollama_content.push_str(content);
                                    std::io::Write::flush(&mut std::io::stdout()).unwrap();
                                }
                            }
                            
                            // æ˜¾ç¤ºOllama JSONæ ¼å¼ï¼ˆä»…å‰å‡ ä¸ªchunkï¼‰
                            if chunk_count <= 2 {
                                println!("\n[Ollama JSONç¤ºä¾‹ #{}]:", chunk_count);
                                println!("{}", serde_json::to_string_pretty(&ollama_chunk)?);
                                println!();
                            }
                            
                            // æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆchunk
                            if ollama_chunk.get("done").and_then(|d| d.as_bool()).unwrap_or(false) {
                                println!("\n[æœ€ç»ˆOllama chunk - done: true]:");
                                println!("{}", serde_json::to_string_pretty(&ollama_chunk)?);
                                break;
                            }
                        }
                    }
                }
                Err(e) => {
                    println!("\nâŒ Ollamaæ ¼å¼é”™è¯¯ï¼š{}", e);
                    break;
                }
            }
        }
        
        println!("\n");
        println!("{}", "=".repeat(50));
        println!();

        // 3. æ¼”ç¤ºè‡ªå®šä¹‰é…ç½®çš„æµå¼è¾“å‡º
        println!("ğŸ”¹ è‡ªå®šä¹‰é…ç½®çš„Ollamaæ ¼å¼ï¼š");
        println!("{}", "=".repeat(50));

        let custom_config = StreamingConfig {
            format: StreamingFormat::Ollama,
            include_usage: true,
            include_reasoning: false,
        };

        let mut custom_stream = client.chat_stream_with_format(&request, &custom_config).await?;
        let mut custom_content = String::new();
        
        while let Some(chunk) = custom_stream.next().await {
            match chunk {
                Ok(response) => {
                    if !response.content.is_empty() {
                        if let Ok(ollama_chunk) = serde_json::from_str::<serde_json::Value>(&response.content) {
                            if let Some(content) = ollama_chunk
                                .get("message")
                                .and_then(|m| m.get("content"))
                                .and_then(|c| c.as_str()) 
                            {
                                if !content.is_empty() {
                                    print!("{}", content);
                                    custom_content.push_str(content);
                                    std::io::Write::flush(&mut std::io::stdout()).unwrap();
                                }
                            }
                            
                            // æ£€æŸ¥æœ€ç»ˆchunkçš„usageä¿¡æ¯
                            if ollama_chunk.get("done").and_then(|d| d.as_bool()).unwrap_or(false) {
                                println!("\n[åŒ…å«usageä¿¡æ¯çš„æœ€ç»ˆchunk]:");
                                println!("{}", serde_json::to_string_pretty(&ollama_chunk)?);
                                break;
                            }
                        }
                    }
                }
                Err(e) => {
                    println!("\nâŒ è‡ªå®šä¹‰æ ¼å¼é”™è¯¯ï¼š{}", e);
                    break;
                }
            }
        }

        println!("\n");
        println!("{}", "=".repeat(50));
        println!("\nâœ… æ¼”ç¤ºå®Œæˆï¼");
        println!("\nğŸ“Š æ€»ç»“ï¼š");
        println!("â€¢ OpenAIæ ¼å¼ï¼šæ ‡å‡†çš„choices/deltaç»“æ„");
        println!("â€¢ Ollamaæ ¼å¼ï¼šmessage/contentç»“æ„ + doneæ ‡è®°");
        println!("â€¢ ä¸¤ç§æ ¼å¼éƒ½åŒ…å«ç›¸åŒçš„å†…å®¹ï¼Œåªæ˜¯JSONç»“æ„ä¸åŒ");
        println!("â€¢ Ollamaæ ¼å¼æ›´é€‚åˆä¸Zed.devç­‰å·¥å…·é›†æˆ");
    }

    Ok(())
}
