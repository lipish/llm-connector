//! æµå¼å“åº”åŸºç¡€ç¤ºä¾‹
//!
//! å±•ç¤ºå¦‚ä½•ä½¿ç”¨æµå¼å“åº”åŠŸèƒ½ï¼Œå®æ—¶æ¥æ”¶AIå›å¤
//!
//! è¿è¡Œæ–¹å¼: cargo run --example streaming_basic --features streaming

#[cfg(feature = "streaming")]
use llm_connector::{LlmClient, types::{ChatRequest, Message, Role}};
#[cfg(feature = "streaming")]
use futures_util::StreamExt;

#[cfg(feature = "streaming")]
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸŒŠ æµå¼å“åº”åŸºç¡€ç¤ºä¾‹\n");

    // ä»ç¯å¢ƒå˜é‡é€‰æ‹©provider
    let provider = std::env::var("LLM_PROVIDER").unwrap_or_else(|_| "ollama".to_string());
    
    let client = match provider.as_str() {
        "openai" => {
            let api_key = std::env::var("OPENAI_API_KEY")
                .expect("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡");
            LlmClient::openai(&api_key)?
        }
        "zhipu" => {
            let api_key = std::env::var("ZHIPU_API_KEY")
                .expect("è¯·è®¾ç½® ZHIPU_API_KEY ç¯å¢ƒå˜é‡");
            LlmClient::zhipu(&api_key)?
        }
        "ollama" => {
            LlmClient::ollama()?
        }
        _ => {
            println!("âŒ ä¸æ”¯æŒçš„provider: {}", provider);
            println!("ğŸ’¡ æ”¯æŒçš„provider: openai, zhipu, ollama");
            println!("   è®¾ç½®ç¯å¢ƒå˜é‡: export LLM_PROVIDER=ollama");
            std::process::exit(1);
        }
    };

    // é€‰æ‹©åˆé€‚çš„æ¨¡å‹
    let model = match provider.as_str() {
        "openai" => "gpt-3.5-turbo".to_string(),
        "zhipu" => "glm-4".to_string(),
        "ollama" => std::env::var("OLLAMA_MODEL").unwrap_or_else(|_| "llama2".to_string()),
        _ => unreachable!(),
    };

    // æ„å»ºèŠå¤©è¯·æ±‚
    let request = ChatRequest {
        model: model.clone(),
        messages: vec![
            Message {
                role: Role::User,
                content: "è¯·å†™ä¸€é¦–å…³äºç¼–ç¨‹çš„çŸ­è¯—ï¼Œè¦æœ‰åˆ›æ„å’Œå¹½é»˜æ„Ÿã€‚".to_string(),
                ..Default::default()
            }
        ],
        max_tokens: Some(300),
        temperature: Some(0.8),
        ..Default::default()
    };

    println!("ğŸš€ å¼€å§‹æµå¼å¯¹è¯...");
    println!("ğŸ”§ Provider: {}", provider);
    println!("ğŸ“ æ¨¡å‹: {}", model);
    println!("ğŸ’¬ æ¶ˆæ¯: {}", request.messages[0].content);
    println!();
    println!("ğŸ“¡ æµå¼å“åº”:");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    // å‘é€æµå¼è¯·æ±‚
    match client.chat_stream(&request).await {
        Ok(mut stream) => {
            let mut full_content = String::new();
            
            while let Some(chunk_result) = stream.next().await {
                match chunk_result {
                    Ok(chunk) => {
                        if let Some(content) = chunk.get_content() {
                            print!("{}", content);
                            full_content.push_str(&content);
                            std::io::Write::flush(&mut std::io::stdout()).unwrap();
                        }
                    }
                    Err(e) => {
                        println!("\nâŒ æµå¼å“åº”é”™è¯¯: {}", e);
                        break;
                    }
                }
            }
            
            println!();
            println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            println!("âœ… æµå¼å“åº”å®Œæˆ");
            println!("ğŸ“Š æ€»å­—ç¬¦æ•°: {}", full_content.len());
        }
        Err(e) => {
            println!("âŒ æµå¼è¯·æ±‚å¤±è´¥: {}", e);
            println!();
            println!("ğŸ’¡ è¯·æ£€æŸ¥:");
            println!("  1. APIå¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®");
            println!("  2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸");
            println!("  3. æ¨¡å‹æ˜¯å¦æ”¯æŒæµå¼å“åº”");
        }
    }

    Ok(())
}

#[cfg(not(feature = "streaming"))]
fn main() {
    println!("âŒ æ­¤ç¤ºä¾‹éœ€è¦å¯ç”¨ streaming åŠŸèƒ½");
    println!("è¯·ä½¿ç”¨: cargo run --example streaming_basic --features streaming");
}
