use std::time::Instant;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ” åŸå§‹ Ollama HTTP è¯·æ±‚è°ƒè¯•");

    // æ‰‹åŠ¨æ„å»º Ollama è¯·æ±‚æ ¼å¼
    let ollama_req = serde_json::json!({
        "model": "glm-4-flash",
        "messages": [
            {"role": "user", "content": "Say hello"}
        ],
        "stream": false,
        "options": {
            "num_predict": 50
        }
    });

    println!("\nğŸ”§ Ollama è¯·æ±‚:");
    println!("  URL: http://localhost:11434/api/chat");
    println!("  è¯·æ±‚ä½“: {}", serde_json::to_string_pretty(&ollama_req)?);

    // æµ‹è¯• HTTP è¯·æ±‚
    println!("\nğŸŒ å‘é€ HTTP è¯·æ±‚...");
    let start = Instant::now();

    let http_client = reqwest::Client::builder()
        .user_agent("llm-connector/0.3.7")
        .no_proxy()  // ç»•è¿‡ä»£ç†
        .build()?;
    match http_client
        .post("http://localhost:11434/api/chat")
        .json(&ollama_req)
        .send()
        .await
    {
        Ok(response) => {
            println!("âœ… HTTP å“åº”: {:?}", response.status());
            println!("  å“åº”æ—¶é—´: {:?}", start.elapsed());

            // è¯»å–å“åº”ä½“
            let body = response.text().await?;
            println!("ğŸ“„ å“åº”ä½“: {}", body);

            // å°è¯•è§£æ
            match serde_json::from_str::<serde_json::Value>(&body) {
                Ok(json) => {
                    println!("âœ… JSON è§£ææˆåŠŸ");
                    println!("  ç»“æ„: {}", serde_json::to_string_pretty(&json)?);
                }
                Err(e) => {
                    println!("âŒ JSON è§£æå¤±è´¥: {}", e);
                }
            }
        }
        Err(e) => {
            println!("âŒ HTTP è¯·æ±‚å¤±è´¥: {}", e);
        }
    }

    // æµ‹è¯• llm-connector
    println!("\nğŸ’¬ æµ‹è¯• llm-connector LlmClient...");
    let start = Instant::now();

    // æ·»åŠ å¿…è¦çš„å¯¼å…¥
    use llm_connector::{LlmClient, types::{ChatRequest, Message}};

    let client = LlmClient::ollama(Some("http://localhost:11434"));
    let request = ChatRequest {
        model: "glm-4-flash".to_string(),
        messages: vec![Message::user("Say hello")],
        max_tokens: Some(50),
        ..Default::default()
    };

    match client.chat(&request).await {
        Ok(response) => {
            println!("âœ… LlmClient å“åº”æˆåŠŸ ({:?})", start.elapsed());
            println!("  å†…å®¹: {}", response.choices[0].message.content);
        }
        Err(e) => {
            println!("âŒ LlmClient å“åº”å¤±è´¥ ({:?}): {}", start.elapsed(), e);
        }
    }

    Ok(())
}