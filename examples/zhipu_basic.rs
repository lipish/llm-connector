use llm_connector::{LlmClient, types::{ChatRequest, Message, Role}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Zhipu OpenAI ÂÖºÂÆπÁ´ØÁÇπÔºåÁº∫ÁúÅ‰∏∫ÂÆòÊñπÂú∞ÂùÄ
    let api_key = std::env::var("ZHIPU_API_KEY")
        .expect("ËØ∑ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè ZHIPU_API_KEY");

    let client = LlmClient::zhipu(&api_key)?;

    let model = std::env::var("ZHIPU_MODEL").unwrap_or_else(|_| "glm-4.5".to_string());
    let request = ChatRequest {
        model,
        messages: vec![Message::text(Role::User, "ËØ∑ÁÆÄË¶ÅËØ¥ÊòéÊµÅÂºèÂìçÂ∫îÁöÑÂ•ΩÂ§Ñ„ÄÇ")],
        max_tokens: Some(128),
        ..Default::default()
    };

    println!("üöÄ Zhipu ÈùûÊµÅÂºèËøûÊé•ÊµãËØï (model={})\n", request.model);
    match client.chat(&request).await {
        Ok(resp) => {
            println!("‚úÖ ÊàêÂäüÔºåËæìÂá∫Ôºö\n{}", resp.content);
            if let Some(usage) = resp.usage {
                println!("\nüìä Token ‰ΩøÁî®ÊÉÖÂÜµ:");
                println!("  ËæìÂÖ• tokens: {}", usage.prompt_tokens);
                println!("  ËæìÂá∫ tokens: {}", usage.completion_tokens);
                println!("  ÊÄªËÆ° tokens: {}", usage.total_tokens);
            }
        }
        Err(e) => {
            println!("‚ùå Â§±Ë¥•Ôºö{}", e);
        }
    }

    Ok(())
}