use llm_connector::{LlmClient, types::{ChatRequest, Message}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Zhipu OpenAI ÂÖºÂÆπÁ´ØÁÇπÔºåÁº∫ÁúÅ‰∏∫ÂÆòÊñπÂú∞ÂùÄ
    let api_key = std::env::var("ZHIPU_API_KEY")
        .expect("ËØ∑ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè ZHIPU_API_KEY");
    let base_url = std::env::var("ZHIPU_BASE_URL")
        .unwrap_or_else(|_| "https://open.bigmodel.cn/api/paas/v4".to_string());

    let client = LlmClient::zhipu(&api_key);

    let model = std::env::var("ZHIPU_MODEL").unwrap_or_else(|_| "glm-4.5".to_string());
    let request = ChatRequest {
        model,
        messages: vec![Message::user("ËØ∑ÁÆÄË¶ÅËØ¥ÊòéÊµÅÂºèÂìçÂ∫îÁöÑÂ•ΩÂ§Ñ„ÄÇ")],
        max_tokens: Some(128),
        ..Default::default()
    };

    println!("üöÄ Zhipu ÈùûÊµÅÂºèËøûÊé•ÊµãËØï (model={})\n", request.model);
    match client.chat(&request).await {
        Ok(resp) => {
            println!("‚úÖ ÊàêÂäüÔºåËæìÂá∫Ôºö\n{}", resp.choices[0].message.content);
        }
        Err(e) => {
            println!("‚ùå Â§±Ë¥•Ôºö{}", e);
        }
    }

    Ok(())
}