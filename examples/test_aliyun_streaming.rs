/// æµ‹è¯•é˜¿é‡Œäº‘ DashScope æµå¼å“åº”
/// 
/// éªŒè¯ä¿®å¤åçš„æµå¼å“åº”åŠŸèƒ½

#[cfg(feature = "streaming")]
use {
    futures_util::StreamExt,
    llm_connector::{LlmClient, types::{ChatRequest, Message, Role}},
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    #[cfg(not(feature = "streaming"))]
    {
        println!("âŒ éœ€è¦å¯ç”¨ 'streaming' åŠŸèƒ½");
        println!("è¿è¡Œ: cargo run --example test_aliyun_streaming --features streaming");
        return Ok(());
    }
    
    #[cfg(feature = "streaming")]
    {
        let api_key = std::env::var("ALIYUN_API_KEY")
            .expect("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ALIYUN_API_KEY");
        
        let client = LlmClient::aliyun(&api_key)?;
        
        println!("ğŸ§ª æµ‹è¯•é˜¿é‡Œäº‘ DashScope æµå¼å“åº”");
        println!("{}", "=".repeat(80));
        
        let request = ChatRequest {
            model: "qwen-turbo".to_string(),
            messages: vec![Message::text(Role::User, "ç”¨ä¸€å¥è¯ä»‹ç»åŒ—äº¬")],
            stream: Some(true),
            ..Default::default()
        };
        
        println!("\nğŸ“¤ å‘é€æµå¼è¯·æ±‚:");
        println!("   Model: qwen-turbo");
        println!("   Message: ç”¨ä¸€å¥è¯ä»‹ç»åŒ—äº¬");
        println!("   Stream: true");
        
        let mut stream = client.chat_stream(&request).await?;
        let mut chunk_count = 0;
        let mut content_chunks = 0;
        let mut full_content = String::new();
        
        println!("\nğŸ“¥ æ¥æ”¶æµå¼å“åº”:");
        println!("{}", "-".repeat(80));
        
        while let Some(chunk) = stream.next().await {
            match chunk {
                Ok(response) => {
                    chunk_count += 1;
                    
                    if let Some(choice) = response.choices.first() {
                        // æ£€æŸ¥ delta.content
                        if let Some(ref content) = choice.delta.content {
                            if !content.is_empty() {
                                content_chunks += 1;
                                full_content.push_str(content);
                                print!("{}", content);
                                std::io::Write::flush(&mut std::io::stdout())?;
                            }
                        }
                        
                        // æ£€æŸ¥ finish_reason
                        if let Some(ref reason) = choice.finish_reason {
                            println!("\n\nğŸ finish_reason: {}", reason);
                        }
                    }
                    
                    // æ£€æŸ¥ usageï¼ˆæœ€åä¸€ä¸ªå—ï¼‰
                    if let Some(ref usage) = response.usage {
                        println!("\nğŸ“Š Usage:");
                        println!("   prompt_tokens: {}", usage.prompt_tokens);
                        println!("   completion_tokens: {}", usage.completion_tokens);
                        println!("   total_tokens: {}", usage.total_tokens);
                    }
                }
                Err(e) => {
                    println!("\nâŒ é”™è¯¯: {}", e);
                    break;
                }
            }
        }
        
        println!("\n{}", "=".repeat(80));
        println!("ğŸ“Š ç»Ÿè®¡:");
        println!("   æ€»æµå¼å—æ•°: {}", chunk_count);
        println!("   åŒ…å«å†…å®¹çš„å—æ•°: {}", content_chunks);
        println!("   å®Œæ•´å†…å®¹é•¿åº¦: {} å­—ç¬¦", full_content.len());
        
        if content_chunks == 0 {
            println!("\nâŒ é—®é¢˜: æ²¡æœ‰æ”¶åˆ°ä»»ä½•å†…å®¹å—ï¼");
            println!("   è¿™è¡¨æ˜æµå¼å“åº”è§£ææœ‰é—®é¢˜ã€‚");
        } else {
            println!("\nâœ… æµå¼å“åº”æ­£å¸¸ï¼");
            println!("   å®Œæ•´å†…å®¹: {}", full_content);
        }
        
        println!("\n{}", "=".repeat(80));
    }
    
    Ok(())
}

