#[cfg(feature = "streaming")]
use {
    futures_util::StreamExt,
    llm_connector::{LlmClient, types::{ChatRequest, Message, Role, Tool, Function}},
    serde_json::json,
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    #[cfg(not(feature = "streaming"))]
    {
        println!("âŒ éœ€è¦å¯ç”¨ 'streaming' åŠŸèƒ½");
        return Ok(());
    }
    
    #[cfg(feature = "streaming")]
    {
        let api_key = std::env::var("OPENAI_API_KEY")
            .expect("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY");

        println!("ğŸ”§ ä½¿ç”¨ OpenAI");
        let client = LlmClient::openai(&api_key)?;

        let tools = vec![Tool {
            tool_type: "function".to_string(),
            function: Function {
                name: "get_weather".to_string(),
                description: Some("Get weather information for a city".to_string()),
                parameters: json!({
                    "type": "object",
                    "properties": {
                        "location": {"type": "string", "description": "City name"},
                        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
                    },
                    "required": ["location"]
                }),
            },
        }];

        let model = "gpt-4o-mini";
        
        println!("\n{}", "=".repeat(70));
        println!("ğŸ§ª æµ‹è¯• OpenAI å·¥å…·è°ƒç”¨æµå¼æ”¯æŒ");
        println!("{}\n", "=".repeat(70));
        
        println!("ğŸ“ ç¬¬ä¸€è½®è¯·æ±‚ï¼ˆè§¦å‘å·¥å…·è°ƒç”¨ï¼‰");
        
        let request = ChatRequest {
            model: model.to_string(),
            messages: vec![Message::text(Role::User, "What's the weather in Shanghai?")],
            tools: Some(tools.clone()),
            ..Default::default()
        };
        
        let mut stream = client.chat_stream(&request).await?;
        let mut tool_calls_buffer = Vec::new();
        let mut chunk_count = 0;
        
        while let Some(chunk) = stream.next().await {
            chunk_count += 1;
            match chunk {
                Ok(response) => {
                    if let Some(content) = response.get_content() {
                        print!("{}", content);
                    }
                    
                    if let Some(choice) = response.choices.first() {
                        if let Some(calls) = &choice.delta.tool_calls {
                            tool_calls_buffer.extend(calls.clone());
                        }
                        
                        if let Some(reason) = &choice.finish_reason {
                            println!("\nâœ… finish_reason: {}", reason);
                            break;
                        }
                    }
                }
                Err(e) => {
                    eprintln!("âŒ é”™è¯¯: {}", e);
                    break;
                }
            }
        }
        
        println!("\nğŸ“Š ç¬¬ä¸€è½®ç»Ÿè®¡:");
        println!("  - æ”¶åˆ° {} ä¸ªæµå¼å—", chunk_count);
        println!("  - å·¥å…·è°ƒç”¨: {}", if !tool_calls_buffer.is_empty() { "âœ… æœ‰" } else { "âŒ æ— " });
        
        if !tool_calls_buffer.is_empty() {
            println!("\nğŸ“ ç¬¬äºŒè½®è¯·æ±‚ï¼ˆåŒ…å« tool ç»“æœï¼‰");

            let first_call = tool_calls_buffer[0].clone();
            
            let request2 = ChatRequest {
                model: model.to_string(),
                messages: vec![
                    Message::text(Role::User, "What's the weather in Shanghai?"),
                    Message {
                        role: Role::Assistant,
                        content: String::new(),
                        tool_calls: Some(vec![first_call.clone()]),
                        ..Default::default()
                    },
                    Message {
                        role: Role::Tool,
                        content: r#"{"temperature": 22, "condition": "sunny"}"#.to_string(),
                        tool_call_id: Some(first_call.id.clone()),
                        name: Some(first_call.function.name.clone()),
                        ..Default::default()
                    },
                ],
                tools: Some(tools),
                ..Default::default()
            };
            
            println!("\nğŸ“¨ æµå¼å“åº”:");
            let mut stream2 = client.chat_stream(&request2).await?;
            let mut chunk_count2 = 0;
            
            while let Some(chunk) = stream2.next().await {
                chunk_count2 += 1;
                match chunk {
                    Ok(response) => {
                        if let Some(content) = response.get_content() {
                            print!("{}", content);
                        }
                        
                        if let Some(choice) = response.choices.first() {
                            if let Some(reason) = &choice.finish_reason {
                                println!("\nâœ… finish_reason: {}", reason);
                                break;
                            }
                        }
                    }
                    Err(e) => {
                        eprintln!("âŒ é”™è¯¯: {}", e);
                        break;
                    }
                }
            }
            
            println!("\nğŸ“Š ç¬¬äºŒè½®ç»Ÿè®¡:");
            println!("  - æ”¶åˆ° {} ä¸ªæµå¼å—", chunk_count2);
            
            if chunk_count2 > 1 {
                println!("\nâœ… OpenAI **æ”¯æŒ**åœ¨åŒ…å« Role::Tool æ—¶ä½¿ç”¨æµå¼ï¼");
            } else if chunk_count2 == 1 {
                println!("\nâš ï¸ åªæ”¶åˆ° 1 ä¸ªå—ï¼ˆå¯èƒ½è¢«è‡ªåŠ¨åˆ‡æ¢ä¸ºéæµå¼ï¼‰");
            } else {
                println!("\nâŒ æœªæ”¶åˆ°ä»»ä½•å“åº”");
            }
        }
    }
    
    Ok(())
}
