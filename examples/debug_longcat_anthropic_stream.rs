//! è°ƒè¯• LongCat Anthropic æµå¼å“åº”

use llm_connector::{LlmClient, types::{ChatRequest, Message, Role}};
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let api_key = std::env::var("LONGCAT_API_KEY")
        .expect("LONGCAT_API_KEY environment variable not set");

    println!("ğŸ” è°ƒè¯• LongCat Anthropic æµå¼å“åº”\n");

    // åˆ›å»ºå®¢æˆ·ç«¯
    let client = LlmClient::longcat_anthropic(&api_key)?;

    // åˆ›å»ºè¯·æ±‚
    let request = ChatRequest {
        model: "LongCat-Flash-Chat".to_string(),
        messages: vec![Message {
            role: Role::User,
            content: "ä½ å¥½".to_string(),
            ..Default::default()
        }],
        stream: Some(true),
        max_tokens: Some(50),
        ..Default::default()
    };

    println!("ğŸ“¤ å‘é€æµå¼è¯·æ±‚...\n");

    match client.chat_stream(&request).await {
        Ok(mut stream) => {
            println!("âœ… è·å–åˆ°æµ\n");
            println!("ğŸ“¥ æ¥æ”¶æµå¼å“åº”:\n");

            let mut chunk_count = 0;
            while let Some(result) = stream.next().await {
                chunk_count += 1;
                match result {
                    Ok(chunk) => {
                        println!("ğŸ“¦ Chunk #{}: {:?}", chunk_count, chunk);
                        if let Some(content) = chunk.choices.first().and_then(|c| c.delta.content.as_ref()) {
                            print!("{}", content);
                            use std::io::{self, Write};
                            io::stdout().flush().unwrap();
                        }
                    }
                    Err(e) => {
                        eprintln!("\nâŒ é”™è¯¯: {}", e);
                        break;
                    }
                }
            }

            println!("\n\nâœ… æ€»å…±æ”¶åˆ° {} ä¸ªå—", chunk_count);
        }
        Err(e) => {
            eprintln!("âŒ åˆ›å»ºæµå¤±è´¥: {}", e);
        }
    }

    Ok(())
}

