use llm_connector::{LlmClient, types::{ChatRequest, Message}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // ËÖæËÆØÊ∑∑ÂÖÉ OpenAI ÂÖºÂÆπÁ´ØÁÇπ
    let api_key = std::env::var("HUNYUAN_API_KEY")
        .expect("ËØ∑ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè HUNYUAN_API_KEY");

    let client = LlmClient::hunyuan(&api_key);

    let model = std::env::var("HUNYUAN_MODEL").unwrap_or_else(|_| "hunyuan-lite".to_string());
    let request = ChatRequest {
        model,
        messages: vec![Message::user("ËØ∑ÁÆÄË¶Å‰ªãÁªç‰∏Ä‰∏ãËÖæËÆØÊ∑∑ÂÖÉÂ§ßÊ®°ÂûãÁöÑÁâπÁÇπ„ÄÇ")],
        max_tokens: Some(256),
        ..Default::default()
    };

    println!("üöÄ ËÖæËÆØÊ∑∑ÂÖÉÈùûÊµÅÂºèËøûÊé•ÊµãËØï (model={})\n", request.model);
    match client.chat(&request).await {
        Ok(resp) => {
            println!("‚úÖ ÊàêÂäüÔºåËæìÂá∫Ôºö\n{}", resp.choices[0].message.content);
            if let Some(usage) = resp.usage {
                println!("\nüìä Token ‰ΩøÁî®ÊÉÖÂÜµ:");
                println!("  ËæìÂÖ• tokens: {}", usage.prompt_tokens);
                println!("  ËæìÂá∫ tokens: {}", usage.completion_tokens);
                println!("  ÊÄªËÆ° tokens: {}", usage.total_tokens);
            }
        }
        Err(e) => {
            println!("‚ùå Â§±Ë¥•Ôºö{}", e);
        }
    }

    Ok(())
}
