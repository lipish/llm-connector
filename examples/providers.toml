# llm-connector/providers.toml - Example configuration file for LLM providers

[providers.aliyun]
name = "aliyun"
api_key = "your-aliyun-api-key-here"
base_url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
timeout_ms = 30000
proxy = null
parser_type = "sse"
max_retries = 3
retry_backoff_ms = 1000
supported_models = ["qwen-turbo", "qwen-plus", "qwen-max"]

[providers.deepseek]
name = "deepseek"
api_key = "your-deepseek-api-key-here"
base_url = "https://api.deepseek.com/v1"
timeout_ms = 30000
proxy = null
parser_type = "sse"
max_retries = 3
retry_backoff_ms = 1000
supported_models = ["deepseek-chat", "deepseek-coder", "deepseek-reasoner"]

[providers.zhipu]
name = "zhipu"
api_key = "your-zhipu-api-key-here"
base_url = "https://open.bigmodel.cn/api/paas/v4"
timeout_ms = 30000
proxy = null
parser_type = "sse"
max_retries = 3
retry_backoff_ms = 1000
supported_models = ["glm-4", "glm-3-turbo", "glm-4v"]

# Example of a custom provider configuration
[providers.custom]
name = "custom"
api_key = "your-custom-api-key-here"
base_url = "https://api.custom-llm.com/v1"
timeout_ms = 60000
proxy = "http://proxy.example.com:8080"
parser_type = "ndjson"
max_retries = 5
retry_backoff_ms = 2000
supported_models = ["custom-model-1", "custom-model-2"]

# Environment variable substitution example (if supported)
# [providers.env_example]
# name = "env_example"
# api_key = "${ENV_API_KEY}"  # Would be replaced with actual environment variable
# base_url = "https://api.example.com"
# timeout_ms = 30000
# supported_models = ["example-model"]
