//! éªŒè¯æ‰€æœ‰ Providers çš„ reasoning_content æ”¯æŒ
//!
//! è¿™ä¸ªç¤ºä¾‹ç”¨äºéªŒè¯å„ä¸ª provider æ˜¯å¦æ­£ç¡®æå– reasoning_content

use llm_connector::{LlmClient, types::{ChatRequest, Message, Role}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ§ª éªŒè¯ Reasoning Content æ”¯æŒ");
    println!("{}", "=".repeat(80));

    // æµ‹è¯• DeepSeek
    if let Ok(api_key) = std::env::var("DEEPSEEK_API_KEY") {
        println!("\nğŸ“ æµ‹è¯• DeepSeek Reasoner");
        println!("{}", "-".repeat(80));
        
        let client = LlmClient::deepseek(&api_key)?;
        let request = ChatRequest {
            model: "deepseek-reasoner".to_string(),
            messages: vec![Message {
                role: Role::User,
                content: "9.11 å’Œ 9.9 å“ªä¸ªæ›´å¤§ï¼Ÿ".to_string(),
                ..Default::default()
            }],
            max_tokens: Some(500),
            ..Default::default()
        };

        match client.chat(&request).await {
            Ok(response) => {
                println!("âœ… DeepSeek è¯·æ±‚æˆåŠŸ");
                if let Some(reasoning) = response.reasoning_content {
                    println!("   âœ… reasoning_content å­˜åœ¨");
                    println!("   é•¿åº¦: {} å­—ç¬¦", reasoning.len());
                    println!("   é¢„è§ˆ: {}...", &reasoning[..reasoning.len().min(100)]);
                } else {
                    println!("   âŒ reasoning_content ä¸º None");
                }
                println!("   content: {}", response.content);
            }
            Err(e) => {
                println!("âŒ DeepSeek é”™è¯¯: {}", e);
            }
        }
    } else {
        println!("\nâ­ï¸  è·³è¿‡ DeepSeek (æœªè®¾ç½® DEEPSEEK_API_KEY)");
    }

    // æµ‹è¯• Moonshot
    if let Ok(api_key) = std::env::var("MOONSHOT_API_KEY") {
        println!("\nğŸ“ æµ‹è¯• Moonshot Kimi Thinking");
        println!("{}", "-".repeat(80));
        
        let client = LlmClient::moonshot(&api_key)?;
        let request = ChatRequest {
            model: "kimi-thinking-preview".to_string(),
            messages: vec![Message {
                role: Role::User,
                content: "è®¡ç®— 15 * 23".to_string(),
                ..Default::default()
            }],
            max_tokens: Some(500),
            ..Default::default()
        };

        match client.chat(&request).await {
            Ok(response) => {
                println!("âœ… Moonshot è¯·æ±‚æˆåŠŸ");
                if let Some(reasoning) = response.reasoning_content {
                    println!("   âœ… reasoning_content å­˜åœ¨");
                    println!("   é•¿åº¦: {} å­—ç¬¦", reasoning.len());
                    println!("   é¢„è§ˆ: {}...", &reasoning[..reasoning.len().min(100)]);
                } else {
                    println!("   âŒ reasoning_content ä¸º None");
                }
                println!("   content: {}", response.content);
            }
            Err(e) => {
                println!("âŒ Moonshot é”™è¯¯: {}", e);
            }
        }
    } else {
        println!("\nâ­ï¸  è·³è¿‡ Moonshot (æœªè®¾ç½® MOONSHOT_API_KEY)");
    }

    // æµ‹è¯• Zhipu GLM-Z1
    if let Ok(api_key) = std::env::var("ZHIPU_API_KEY") {
        println!("\nğŸ“ æµ‹è¯• Zhipu GLM-Z1");
        println!("{}", "-".repeat(80));
        
        let client = LlmClient::zhipu(&api_key)?;
        let request = ChatRequest {
            model: "glm-z1".to_string(),
            messages: vec![Message {
                role: Role::User,
                content: "è§£é‡Šä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„".to_string(),
                ..Default::default()
            }],
            max_tokens: Some(500),
            ..Default::default()
        };

        match client.chat(&request).await {
            Ok(response) => {
                println!("âœ… Zhipu è¯·æ±‚æˆåŠŸ");
                if let Some(reasoning) = response.reasoning_content {
                    println!("   âœ… reasoning_content å­˜åœ¨ï¼ˆä» ###Thinking æå–ï¼‰");
                    println!("   é•¿åº¦: {} å­—ç¬¦", reasoning.len());
                    println!("   é¢„è§ˆ: {}...", &reasoning[..reasoning.len().min(100)]);
                } else {
                    println!("   âš ï¸  reasoning_content ä¸º Noneï¼ˆå¯èƒ½ä¸æ˜¯æ¨ç†æ¨¡å‹ï¼‰");
                }
                println!("   content: {}", response.content);
            }
            Err(e) => {
                println!("âŒ Zhipu é”™è¯¯: {}", e);
            }
        }
    } else {
        println!("\nâ­ï¸  è·³è¿‡ Zhipu (æœªè®¾ç½® ZHIPU_API_KEY)");
    }

    // æµ‹è¯• Aliyun Qwen
    if let Ok(api_key) = std::env::var("ALIYUN_API_KEY") {
        println!("\nğŸ“ æµ‹è¯• Aliyun Qwen Plus (éœ€è¦ enable_thinking)");
        println!("{}", "-".repeat(80));
        println!("âš ï¸  æ³¨æ„: Aliyun éœ€è¦åœ¨è¯·æ±‚ä¸­è®¾ç½® enable_thinking=true");
        println!("   å½“å‰å®ç°å¯èƒ½ä¸æ”¯æŒæ­¤å‚æ•°ï¼Œéœ€è¦æ‰‹åŠ¨æµ‹è¯•");
        
        // æ³¨æ„ï¼šå½“å‰ llm-connector å¯èƒ½ä¸æ”¯æŒ enable_thinking å‚æ•°
        // è¿™éœ€è¦åœ¨ Aliyun provider ä¸­æ·»åŠ æ”¯æŒ
    } else {
        println!("\nâ­ï¸  è·³è¿‡ Aliyun (æœªè®¾ç½® ALIYUN_API_KEY)");
    }

    // æµ‹è¯• OpenAI o1
    if let Ok(api_key) = std::env::var("OPENAI_API_KEY") {
        println!("\nğŸ“ æµ‹è¯• OpenAI o1");
        println!("{}", "-".repeat(80));
        
        let client = LlmClient::openai(&api_key)?;
        let request = ChatRequest {
            model: "o1-mini".to_string(),
            messages: vec![Message {
                role: Role::User,
                content: "å¦‚æœä¸€ä¸ªæ•°çš„å¹³æ–¹æ˜¯ 144ï¼Œè¿™ä¸ªæ•°æ˜¯å¤šå°‘ï¼Ÿ".to_string(),
                ..Default::default()
            }],
            max_tokens: Some(500),
            ..Default::default()
        };

        match client.chat(&request).await {
            Ok(response) => {
                println!("âœ… OpenAI è¯·æ±‚æˆåŠŸ");
                if let Some(reasoning) = response.reasoning_content {
                    println!("   âœ… reasoning_content å­˜åœ¨");
                    println!("   é•¿åº¦: {} å­—ç¬¦", reasoning.len());
                    println!("   é¢„è§ˆ: {}...", &reasoning[..reasoning.len().min(100)]);
                } else {
                    println!("   âš ï¸  reasoning_content ä¸º None");
                    println!("   æ³¨æ„: OpenAI å¯èƒ½å·²ç§»é™¤ reasoning_content å­—æ®µ");
                }
                println!("   content: {}", response.content);
            }
            Err(e) => {
                println!("âŒ OpenAI é”™è¯¯: {}", e);
            }
        }
    } else {
        println!("\nâ­ï¸  è·³è¿‡ OpenAI (æœªè®¾ç½® OPENAI_API_KEY)");
    }

    // æµ‹è¯•éæ¨ç†æ¨¡å‹ï¼ˆåº”è¯¥è¿”å› Noneï¼‰
    println!("\nğŸ“ æµ‹è¯•éæ¨ç†æ¨¡å‹ï¼ˆåº”è¯¥è¿”å› reasoning_content = Noneï¼‰");
    println!("{}", "-".repeat(80));
    
    if let Ok(api_key) = std::env::var("DEEPSEEK_API_KEY") {
        let client = LlmClient::deepseek(&api_key)?;
        let request = ChatRequest {
            model: "deepseek-chat".to_string(),  // éæ¨ç†æ¨¡å‹
            messages: vec![Message {
                role: Role::User,
                content: "ä½ å¥½".to_string(),
                ..Default::default()
            }],
            max_tokens: Some(50),
            ..Default::default()
        };

        match client.chat(&request).await {
            Ok(response) => {
                println!("âœ… DeepSeek Chat è¯·æ±‚æˆåŠŸ");
                if response.reasoning_content.is_none() {
                    println!("   âœ… reasoning_content æ­£ç¡®ä¸º Noneï¼ˆéæ¨ç†æ¨¡å‹ï¼‰");
                } else {
                    println!("   âš ï¸  reasoning_content ä¸ä¸º Noneï¼ˆæ„å¤–ï¼‰");
                }
            }
            Err(e) => {
                println!("âŒ DeepSeek Chat é”™è¯¯: {}", e);
            }
        }
    }

    println!("\n{}", "=".repeat(80));
    println!("âœ… éªŒè¯å®Œæˆï¼");
    println!("{}", "=".repeat(80));

    println!("\nğŸ“Š æ€»ç»“:");
    println!("   - DeepSeek Reasoner: åº”è¯¥æœ‰ reasoning_content");
    println!("   - Moonshot Kimi Thinking: åº”è¯¥æœ‰ reasoning_content");
    println!("   - Zhipu GLM-Z1: åº”è¯¥æœ‰ reasoning_contentï¼ˆä» ###Thinking æå–ï¼‰");
    println!("   - Aliyun Qwen Plus: éœ€è¦ enable_thinking=true");
    println!("   - OpenAI o1: å¯èƒ½æœ‰ reasoning_contentï¼ˆå–å†³äº API ç‰ˆæœ¬ï¼‰");
    println!("   - éæ¨ç†æ¨¡å‹: reasoning_content åº”è¯¥ä¸º None");

    Ok(())
}

