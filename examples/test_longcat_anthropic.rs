/// æµ‹è¯• LongCat Anthropic æ ¼å¼ API
/// 
/// æµ‹è¯•éæµå¼å’Œæµå¼å“åº”

use llm_connector::{LlmClient, types::{ChatRequest, Message, Role}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let api_key = std::env::var("LONGCAT_API_KEY")
        .unwrap_or_else(|_| "ak_11o3bI6O03mx2yS8jb2hD61q7DJ4d".to_string());
    
    println!("ğŸ§ª æµ‹è¯• LongCat Anthropic æ ¼å¼ API");
    println!("{}", "=".repeat(80));
    
    // åˆ›å»º LongCat Anthropic å®¢æˆ·ç«¯
    // æ³¨æ„ï¼šLongCat ä½¿ç”¨ Bearer è®¤è¯è€Œä¸æ˜¯æ ‡å‡† Anthropic çš„ x-api-key
    let client = LlmClient::longcat_anthropic(&api_key)?;
    
    // æµ‹è¯• 1: éæµå¼å“åº”
    println!("\nğŸ“ æµ‹è¯• 1: éæµå¼å“åº”");
    println!("{}", "-".repeat(80));
    
    let request = ChatRequest {
        model: "LongCat-Flash-Chat".to_string(),
        messages: vec![Message::text(Role::User, "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")],
        max_tokens: Some(1000),
        temperature: Some(0.7),
        ..Default::default()
    };
    
    println!("\nğŸ“¤ å‘é€è¯·æ±‚:");
    println!("   Model: LongCat-Flash-Chat");
    println!("   Message: ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±");
    
    match client.chat(&request).await {
        Ok(response) => {
            println!("\nâœ… è¯·æ±‚æˆåŠŸï¼");
            println!("\nğŸ“¥ å“åº”:");
            println!("   Model: {}", response.model);
            println!("   Content: {}", response.content);
            
            if let Some(usage) = &response.usage {
                println!("\nğŸ“Š Usage:");
                println!("   prompt_tokens: {}", usage.prompt_tokens);
                println!("   completion_tokens: {}", usage.completion_tokens);
                println!("   total_tokens: {}", usage.total_tokens);
            }
            
            if !response.choices.is_empty() {
                println!("\nâœ… Choices æ•°ç»„ä¸ä¸ºç©º");
                println!("   choices[0].finish_reason: {:?}", response.choices[0].finish_reason);
            }
        }
        Err(e) => {
            println!("\nâŒ è¯·æ±‚å¤±è´¥: {}", e);
            println!("   é”™è¯¯è¯¦æƒ…: {:?}", e);
            return Err(e.into());
        }
    }
    
    // æµ‹è¯• 2: æµå¼å“åº”
    #[cfg(feature = "streaming")]
    {
        use futures_util::StreamExt;
        
        println!("\n\nğŸ“ æµ‹è¯• 2: æµå¼å“åº”");
        println!("{}", "-".repeat(80));
        
        let mut streaming_request = request.clone();
        streaming_request.messages = vec![Message::text(Role::User, "ç”¨ä¸€å¥è¯ä»‹ç»åŒ—äº¬")];
        streaming_request.stream = Some(true);
        
        println!("\nğŸ“¤ å‘é€æµå¼è¯·æ±‚:");
        println!("   Model: LongCat-Flash-Chat");
        println!("   Message: ç”¨ä¸€å¥è¯ä»‹ç»åŒ—äº¬");
        println!("   Stream: true");
        
        match client.chat_stream(&streaming_request).await {
            Ok(mut stream) => {
                println!("\nğŸ“¥ æ¥æ”¶æµå¼å“åº”:");
                println!("{}", "-".repeat(80));
                
                let mut chunk_count = 0;
                let mut content_chunks = 0;
                let mut full_content = String::new();
                
                while let Some(chunk) = stream.next().await {
                    match chunk {
                        Ok(response) => {
                            chunk_count += 1;
                            
                            if let Some(choice) = response.choices.first() {
                                if let Some(ref content) = choice.delta.content {
                                    if !content.is_empty() {
                                        content_chunks += 1;
                                        full_content.push_str(content);
                                        print!("{}", content);
                                        std::io::Write::flush(&mut std::io::stdout())?;
                                    }
                                }
                                
                                if let Some(ref reason) = choice.finish_reason {
                                    println!("\n\nğŸ finish_reason: {}", reason);
                                }
                            }
                            
                            if let Some(ref usage) = response.usage {
                                println!("\nğŸ“Š Usage:");
                                println!("   prompt_tokens: {}", usage.prompt_tokens);
                                println!("   completion_tokens: {}", usage.completion_tokens);
                                println!("   total_tokens: {}", usage.total_tokens);
                            }
                        }
                        Err(e) => {
                            println!("\nâŒ é”™è¯¯: {}", e);
                            break;
                        }
                    }
                }
                
                println!("\n{}", "-".repeat(80));
                println!("ğŸ“Š ç»Ÿè®¡:");
                println!("   æ€»æµå¼å—æ•°: {}", chunk_count);
                println!("   åŒ…å«å†…å®¹çš„å—æ•°: {}", content_chunks);
                println!("   å®Œæ•´å†…å®¹é•¿åº¦: {} å­—ç¬¦", full_content.len());
                
                if content_chunks > 0 {
                    println!("\nâœ… æµå¼å“åº”æ­£å¸¸ï¼");
                    println!("   å®Œæ•´å†…å®¹: {}", full_content);
                } else {
                    println!("\nâŒ æ²¡æœ‰æ”¶åˆ°å†…å®¹å—");
                }
            }
            Err(e) => {
                println!("\nâŒ æµå¼è¯·æ±‚å¤±è´¥: {}", e);
                println!("   é”™è¯¯è¯¦æƒ…: {:?}", e);
                return Err(e.into());
            }
        }
    }
    
    #[cfg(not(feature = "streaming"))]
    {
        println!("\n\nâš ï¸  æµå¼æµ‹è¯•è·³è¿‡ï¼ˆéœ€è¦ --features streamingï¼‰");
    }
    
    println!("\n{}", "=".repeat(80));
    println!("âœ… LongCat Anthropic æ ¼å¼æµ‹è¯•å®Œæˆï¼");
    println!("{}", "=".repeat(80));
    
    Ok(())
}

