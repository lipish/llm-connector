use llm_connector::{LlmClient, types::{ChatRequest, Message}};

#[cfg(feature = "streaming")]
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    #[cfg(not(feature = "streaming"))]
    {
        println!("âŒ æ­¤ç¤ºä¾‹éœ€è¦å¯ç”¨ streaming åŠŸèƒ½");
        println!("è¯·ä½¿ç”¨: cargo run --example test_pure_ollama_format --features streaming");
        return Ok(());
    }

    #[cfg(feature = "streaming")]
    {
        // ä½¿ç”¨æ™ºè°±AIä½œä¸ºç¤ºä¾‹
        let api_key = std::env::var("ZHIPU_API_KEY")
            .expect("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPU_API_KEY");

        let client = LlmClient::zhipu(&api_key);

        let request = ChatRequest {
            model: "glm-4-flash".to_string(),
            messages: vec![Message::user("ç®€å•å›å¤ï¼šæµ‹è¯•çº¯Ollamaæ ¼å¼")],
            max_tokens: Some(50),
            ..Default::default()
        };

        println!("ğŸ§ª æµ‹è¯•çº¯Ollamaæ ¼å¼è¾“å‡º");
        println!("ğŸ¯ éªŒè¯è¾“å‡ºæ˜¯å¦ä¸ºçº¯OllamaStreamChunkç±»å‹\n");

        // ä½¿ç”¨çº¯Ollamaæ ¼å¼çš„æµå¼è¾“å‡º
        let mut stream = client.chat_stream_ollama(&request).await?;
        let mut chunk_count = 0;
        
        while let Some(chunk) = stream.next().await {
            match chunk {
                Ok(ollama_chunk) => {
                    chunk_count += 1;
                    
                    // éªŒè¯è¿™æ˜¯çº¯OllamaStreamChunkç±»å‹
                    println!("ğŸ“¦ Chunk #{}: OllamaStreamChunk", chunk_count);
                    println!("  æ¨¡å‹: {}", ollama_chunk.model);
                    println!("  æ—¶é—´: {}", ollama_chunk.created_at);
                    println!("  è§’è‰²: {}", ollama_chunk.message.role);
                    println!("  å†…å®¹: '{}'", ollama_chunk.message.content);
                    println!("  å®Œæˆ: {}", ollama_chunk.done);
                    
                    // æ˜¾ç¤ºJSONåºåˆ—åŒ–ç»“æœ
                    let json_str = serde_json::to_string(&ollama_chunk)?;
                    println!("  JSON: {}", json_str);
                    println!();
                    
                    // æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆchunk
                    if ollama_chunk.done {
                        println!("âœ… æ£€æµ‹åˆ°æœ€ç»ˆchunk (done: true)");
                        
                        if let Some(usage) = ollama_chunk.prompt_eval_count {
                            println!("ğŸ“Š åŒ…å«ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯:");
                            println!("  è¾“å…¥tokens: {}", usage);
                            if let Some(output) = ollama_chunk.eval_count {
                                println!("  è¾“å‡ºtokens: {}", output);
                            }
                        }
                        break;
                    }
                }
                Err(e) => {
                    println!("âŒ é”™è¯¯ï¼š{}", e);
                    break;
                }
            }
        }

        println!("\nğŸ¯ éªŒè¯ç»“æœ:");
        println!("âœ… è¾“å‡ºæ ¼å¼: çº¯OllamaStreamChunkç±»å‹");
        println!("âœ… æ— åµŒå¥—æ ¼å¼: ä¸åŒ…å«OpenAIæ ¼å¼åŒ…è£…");
        println!("âœ… ç›´æ¥å¯ç”¨: å¯ç›´æ¥åºåˆ—åŒ–ä¸ºJSON");
        println!("âœ… å®Œæˆæ ‡è®°: æ­£ç¡®çš„done:trueæœ€ç»ˆchunk");
        println!("âœ… æ€»chunkæ•°: {}", chunk_count);
        
        println!("\nğŸ’¡ è¿™ç§æ ¼å¼å¯ä»¥ç›´æ¥ç”¨äº:");
        println!("  â€¢ Zed.devç¼–è¾‘å™¨");
        println!("  â€¢ Ollamaå…¼å®¹çš„å·¥å…·");
        println!("  â€¢ ä»»ä½•æœŸæœ›çº¯Ollamaæ ¼å¼çš„åº”ç”¨");
    }

    Ok(())
}
