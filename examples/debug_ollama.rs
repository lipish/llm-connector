use std::time::Instant;
use futures_util::StreamExt;
use llm_connector::{LlmClient, types::{ChatRequest, Message}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ” è¯Šæ–­ llm-connector Ollama é›†æˆ");

    // åˆ›å»º Ollama å®¢æˆ·ç«¯
    let client = LlmClient::ollama(Some("http://localhost:11434"));
    println!("âœ… Ollama å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ");

    // æµ‹è¯•è·å–æ¨¡å‹åˆ—è¡¨
    println!("\nğŸ“‹ æµ‹è¯•è·å–æ¨¡å‹åˆ—è¡¨...");
    let models_start = Instant::now();
    match client.fetch_models().await {
        Ok(models) => {
            println!("âœ… æ¨¡å‹åˆ—è¡¨è·å–æˆåŠŸ ({:?}): {:?}", models_start.elapsed(), models);
        }
        Err(e) => {
            println!("âŒ æ¨¡å‹åˆ—è¡¨è·å–å¤±è´¥: {}", e);
        }
    }

    // æµ‹è¯•éæµå¼èŠå¤©
    println!("\nğŸ’¬ æµ‹è¯•éæµå¼èŠå¤©...");
    let non_stream_start = Instant::now();
    let request = ChatRequest {
        model: "glm-4-flash".to_string(),
        messages: vec![Message::user("Say hello")],
        max_tokens: Some(50),
        ..Default::default()
    };

    println!("ğŸ“ å‘é€éæµå¼è¯·æ±‚...");
    match client.chat(&request).await {
        Ok(response) => {
            println!("âœ… éæµå¼å“åº”æˆåŠŸ ({:?})", non_stream_start.elapsed());
            println!("ğŸ“„ å“åº”: {}", response.choices[0].message.content);
        }
        Err(e) => {
            println!("âŒ éæµå¼å“åº”å¤±è´¥: {}", e);
        }
    }

    // æµ‹è¯•æµå¼èŠå¤©
    println!("\nğŸŒŠ æµ‹è¯•æµå¼èŠå¤©...");
    let stream_start = Instant::now();

    println!("ğŸ“ å‘é€æµå¼è¯·æ±‚...");
    match client.chat_stream(&request).await {
        Ok(mut stream) => {
            println!("âœ… æµå¼è¿æ¥å»ºç«‹æˆåŠŸ ({:?})", stream_start.elapsed());
            let mut chunk_count = 0;
            let mut content = String::new();

            while let Some(chunk_result) = stream.next().await {
                chunk_count += 1;
                match chunk_result {
                    Ok(chunk) => {
                        if let Some(chunk_content) = chunk.get_content() {
                            print!("{}", chunk_content);
                            content.push_str(chunk_content);
                        }

                        if let Some(finish_reason) = chunk.choices.first()
                            .and_then(|c| c.finish_reason.as_deref()) {
                            if finish_reason == "stop" {
                                println!("\nâœ… æµå¼å“åº”å®Œæˆ ({} å—, {:?})", chunk_count, stream_start.elapsed());
                                break;
                            }
                        }
                    }
                    Err(e) => {
                        println!("\nâŒ æµå¼å—é”™è¯¯: {}", e);
                        break;
                    }
                }

                // å®‰å…¨é™åˆ¶
                if chunk_count > 50 {
                    println!("\nâš ï¸ è¶…è¿‡æœ€å¤§å—æ•°é™åˆ¶");
                    break;
                }
            }

            println!("ğŸ“„ æµå¼å®Œæ•´å†…å®¹: {}", content);
        }
        Err(e) => {
            println!("âŒ æµå¼è¿æ¥å¤±è´¥: {}", e);
        }
    }

    println!("\nğŸ¯ è¯Šæ–­å®Œæˆ");
    Ok(())
}

#[cfg(not(feature = "streaming"))]
fn main() {
    println!("âŒ éœ€è¦å¯ç”¨ 'streaming' åŠŸèƒ½æ¥è¿è¡Œæ­¤ç¤ºä¾‹");
    println!("   è¯·ä½¿ç”¨: cargo run --example debug_ollama --features streaming");
}