//! OpenAIåŸºç¡€ç¤ºä¾‹
//!
//! å±•ç¤ºå¦‚ä½•ä½¿ç”¨OpenAIåè®®è¿›è¡ŒåŸºæœ¬çš„èŠå¤©å¯¹è¯
//!
//! è¿è¡Œæ–¹å¼: cargo run --example openai_basic

use llm_connector::{LlmClient, types::{ChatRequest, Message}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ¤– OpenAIåŸºç¡€èŠå¤©ç¤ºä¾‹\n");

    // ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
    let api_key = std::env::var("OPENAI_API_KEY")
        .unwrap_or_else(|_| {
            println!("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY");
            println!("   export OPENAI_API_KEY=your-api-key");
            std::process::exit(1);
        });

    // åˆ›å»ºOpenAIå®¢æˆ·ç«¯
    let client = LlmClient::openai(&api_key)?;

    // æ„å»ºèŠå¤©è¯·æ±‚
    let request = ChatRequest {
        model: "gpt-3.5-turbo".to_string(),
        messages: vec![
            Message::user("è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹Rustç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹ã€‚")
        ],
        max_tokens: Some(200),
        temperature: Some(0.7),
        ..Default::default()
    };

    println!("ğŸš€ å‘é€è¯·æ±‚åˆ°OpenAI...");
    println!("ğŸ“ æ¨¡å‹: {}", request.model);
    println!("ğŸ’¬ æ¶ˆæ¯: {}", request.messages[0].content_as_text()_as_text());
    println!();

    // å‘é€è¯·æ±‚
    match client.chat(&request).await {
        Ok(response) => {
            println!("âœ… æˆåŠŸæ”¶åˆ°å›å¤:");
            println!("{}", response.content);
            println!();
            println!("ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:");
            println!("  è¾“å…¥: {} tokens", response.prompt_tokens());
            println!("  è¾“å‡º: {} tokens", response.completion_tokens());
            println!("  æ€»è®¡: {} tokens", response.total_tokens());
        }
        Err(e) => {
            println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
            println!();
            println!("ğŸ’¡ è¯·æ£€æŸ¥:");
            println!("  1. OPENAI_API_KEY æ˜¯å¦æ­£ç¡®è®¾ç½®");
            println!("  2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸");
            println!("  3. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ");
        }
    }

    Ok(())
}
