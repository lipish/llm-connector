#[cfg(feature = "tencent")]
use llm_connector::{LlmClient, types::{ChatRequest, Message}};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    #[cfg(not(feature = "tencent"))]
    {
        println!("âŒ æ­¤ç¤ºä¾‹éœ€è¦å¯ç”¨ tencent åŠŸèƒ½");
        println!("è¯·ä½¿ç”¨: cargo run --example tencent_basic --features tencent");
        return Ok(());
    }

    #[cfg(feature = "tencent")]
    {
        // è…¾è®¯äº‘æ··å…ƒ API Key (OpenAI å…¼å®¹æ ¼å¼)
        let api_key = std::env::var("TENCENT_API_KEY")
            .expect("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ TENCENT_API_KEY (æ ¼å¼: sk-...)");

        let client = LlmClient::tencent(&api_key)?;

        let model = std::env::var("HUNYUAN_MODEL").unwrap_or_else(|_| "hunyuan-lite".to_string());
        let request = ChatRequest {
            model,
            messages: vec![Message::user("è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹è…¾è®¯æ··å…ƒå¤§æ¨¡å‹çš„ç‰¹ç‚¹ï¼Œä½¿ç”¨åŸç”ŸAPIè°ƒç”¨ã€‚")],
            max_tokens: Some(256),
            ..Default::default()
        };

        println!("ğŸš€ è…¾è®¯æ··å…ƒ OpenAI å…¼å®¹ API éæµå¼è¿æ¥æµ‹è¯• (model={})\n", request.model);

        match client.chat(&request).await {
            Ok(resp) => {
                println!("âœ… æˆåŠŸï¼Œè¾“å‡ºï¼š\n{}", resp.choices[0].message.content_as_text());
                println!("\nğŸ“Š Token ä½¿ç”¨æƒ…å†µ:");
                println!("  è¾“å…¥ tokens: {}", resp.prompt_tokens());
                println!("  è¾“å‡º tokens: {}", resp.completion_tokens());
                println!("  æ€»è®¡ tokens: {}", resp.total_tokens());
                println!("\nğŸ†” è¯·æ±‚ID: {}", resp.id);
            }
            Err(e) => {
                println!("âŒ å¤±è´¥ï¼š{}", e);
                println!("\nğŸ’¡ è¯·æ£€æŸ¥ï¼š");
                println!("  1. TENCENT_SECRET_ID å’Œ TENCENT_SECRET_KEY æ˜¯å¦æ­£ç¡®");
                println!("  2. è´¦æˆ·æ˜¯å¦æœ‰æ··å…ƒå¤§æ¨¡å‹çš„è®¿é—®æƒé™");
                println!("  3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸");
            }
        }
        Ok(())
    }
}
