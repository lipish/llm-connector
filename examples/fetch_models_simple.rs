//! Simple example showing how to fetch available models from the API
//!
//! Run with: cargo run --example fetch_models_simple

use llm_connector::LlmClient;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ” Fetching Available Models from API\n");

    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Example: OpenAI Protocol");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("ğŸŒ fetch_models() - Fetch models from API:");
    println!("   â„¹ï¸  This makes an API call to get real-time model list");
    println!("   âš ï¸  Requires valid API key\n");

    // Try to load from keys.yaml if available
    if let Ok(keys_content) = std::fs::read_to_string("keys.yaml") {
        use serde::{Deserialize};

        #[derive(Debug, Deserialize)]
        struct ProviderConfig {
            api_key: String,
            base_url: String,
        }

        #[derive(Debug, Deserialize)]
        struct KeysConfig {
            providers: std::collections::HashMap<String, ProviderConfig>,
        }

        if let Ok(config) = serde_yaml::from_str::<KeysConfig>(&keys_content) {
            if let Some(deepseek) = config.providers.get("deepseek") {
                println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
                println!("Testing with DeepSeek (OpenAI-compatible)");
                println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

                let client = LlmClient::openai(
                    &deepseek.api_key,
                    Some(&deepseek.base_url),
                );

                match client.fetch_models().await {
                    Ok(models) => {
                        println!("   âœ… Success! Found {} models", models.len());
                        println!("   Models: {:?}", models);
                        println!("   âœ… Real-time data from DeepSeek API");
                    }
                    Err(e) => {
                        println!("   âŒ Error: {}", e);
                    }
                }
            }
        }
    } else {
        println!("   â„¹ï¸  keys.yaml not found - skipping live test");
    }

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“ Summary");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("fetch_models():");
    println!("  âœ… Real-time data from API");
    println!("  âœ… Always up-to-date");
    println!("  âœ… Works with OpenAI-compatible providers");
    println!("  âŒ Requires API call (slower)");
    println!("  âŒ Requires valid API key");
    println!("  âŒ Not supported by all protocols (Anthropic, Aliyun, Ollama)\n");

    println!("ğŸ’¡ Recommendation:");
    println!("  - Use fetch_models() to get the latest model list from the API");
    println!("  - Cache fetch_models() results to avoid repeated API calls");
    println!("  - Supported by: OpenAI and OpenAI-compatible providers (DeepSeek, Zhipu, Moonshot, etc.)\n");

    Ok(())
}

