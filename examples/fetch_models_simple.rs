//! Simple example showing the difference between supported_models() and fetch_models()
//!
//! Run with: cargo run --example fetch_models_simple

use llm_connector::LlmClient;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ” Comparing supported_models() vs fetch_models()\n");

    // Example 1: OpenAI
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Example 1: OpenAI Protocol");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    let openai_client = LlmClient::openai("test-key");

    println!("ğŸ“¦ supported_models() - Static/Cached:");
    let static_models = openai_client.supported_models();
    println!("   Returns: {:?}", static_models);
    println!("   â„¹ï¸  This is fast but returns empty for OpenAI\n");

    println!("ğŸŒ fetch_models() - Online from API:");
    println!("   â„¹ï¸  This makes an API call to get real-time model list");
    println!("   âš ï¸  Requires valid API key\n");

    // Example 2: DeepSeek (if you have a key)
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Example 2: DeepSeek (OpenAI-compatible)");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // Try to load from keys.yaml if available
    if let Ok(keys_content) = std::fs::read_to_string("keys.yaml") {
        use serde::{Deserialize};
        
        #[derive(Debug, Deserialize)]
        struct ProviderConfig {
            api_key: String,
            base_url: String,
        }
        
        #[derive(Debug, Deserialize)]
        struct KeysConfig {
            providers: std::collections::HashMap<String, ProviderConfig>,
        }

        if let Ok(config) = serde_yaml::from_str::<KeysConfig>(&keys_content) {
            if let Some(deepseek) = config.providers.get("deepseek") {
                let client = LlmClient::openai_compatible(
                    &deepseek.api_key,
                    &deepseek.base_url,
                );

                println!("ğŸ“¦ supported_models():");
                let static_models = client.supported_models();
                println!("   Returns: {:?}", static_models);
                println!("   âœ… Empty - no hardcoded models\n");

                println!("ğŸŒ fetch_models():");
                match client.fetch_models().await {
                    Ok(models) => {
                        println!("   âœ… Success! Found {} models", models.len());
                        println!("   Models: {:?}", models);
                        println!("   âœ… Real-time data from DeepSeek API");
                    }
                    Err(e) => {
                        println!("   âŒ Error: {}", e);
                    }
                }
            }
        }
    } else {
        println!("   â„¹ï¸  keys.yaml not found - skipping live test");
    }

    println!("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("ğŸ“ Summary");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    println!("supported_models():");
    println!("  âœ… Fast - no API call");
    println!("  âœ… No authentication needed");
    println!("  âŒ Returns empty for OpenAI protocol");
    println!("  âŒ May be outdated for other protocols\n");

    println!("fetch_models():");
    println!("  âœ… Real-time data from API");
    println!("  âœ… Always up-to-date");
    println!("  âœ… Works with OpenAI-compatible providers");
    println!("  âŒ Requires API call (slower)");
    println!("  âŒ Requires valid API key");
    println!("  âŒ Not supported by all protocols\n");

    println!("ğŸ’¡ Recommendation:");
    println!("  - Use fetch_models() when you need the latest model list");
    println!("  - Use supported_models() for quick checks (if available)");
    println!("  - Cache fetch_models() results to avoid repeated API calls\n");

    Ok(())
}

