//! Ollama Basic Example
//!
//! Demonstrates how to use a local Ollama service for a basic chat conversation.
//!
//! Run: cargo run --example ollama_basic

use llm_connector::{
    LlmClient,
    types::{ChatRequest, Message},
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ¤– Ollama Local Model Basic Chat Example\n");

    // Create Ollama client (defaults to http://localhost:11434)
    let client = LlmClient::ollama().unwrap();

    // Fetch available models
    println!("ğŸ” Fetching available models...");
    match client.models().await {
        Ok(models) => {
            if models.is_empty() {
                println!("âŒ No available models found");
                println!("ğŸ’¡ Please download a model first, e.g.:");
                println!("   ollama pull llama2");
                println!("   ollama pull qwen:7b");
                return Ok(());
            }

            println!("âœ… Found {} available models:", models.len());
            for (i, model) in models.iter().enumerate() {
                println!("  {}. {}", i + 1, model);
            }
            println!();
        }
        Err(e) => {
            println!("âŒ Failed to fetch model list: {}", e);
            println!("ğŸ’¡ Please check:");
            println!("  1. Whether the Ollama service is running (ollama serve)");
            println!("  2. Whether the service URL is correct (default: http://localhost:11434)");
            return Ok(());
        }
    }

    // Use the first available model or a default model
    let model = std::env::var("OLLAMA_MODEL").unwrap_or_else(|_| "llama2".to_string());

    // Build chat request
    let request = ChatRequest {
        model: model.clone(),
        messages: vec![Message::user(
            "Please briefly introduce yourself and what you can help me with.",
        )],
        max_tokens: Some(200),
        temperature: Some(0.7),
        ..Default::default()
    };

    println!("ğŸš€ Sending request to Ollama...");
    println!("ğŸ“ Model: {}", request.model);
    println!("ğŸ’¬ Message: {}", request.messages[0].content_as_text());
    println!();

    // Send request
    match client.chat(&request).await {
        Ok(response) => {
            println!("âœ… Received response successfully:");
            println!("{}", response.content);
            println!();
            println!("ğŸ“Š Token usage:");
            println!("  Input: {} tokens", response.prompt_tokens());
            println!("  Output: {} tokens", response.completion_tokens());
            println!("  Total: {} tokens", response.total_tokens());
        }
        Err(e) => {
            println!("âŒ Request failed: {}", e);
            println!();
            println!("ğŸ’¡ Please check:");
            println!("  1. Whether the Ollama service is running");
            println!("  2. Whether the model '{}' has been downloaded", model);
            println!("  3. Whether your network connection is working");
            println!();
            println!("ğŸ”§ Common commands:");
            println!("  ollama serve          # Start Ollama service");
            println!("  ollama pull {}   # Download model", model);
            println!("  ollama list           # List downloaded models");
        }
    }

    Ok(())
}
