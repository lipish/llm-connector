//! ç»Ÿä¸€é…ç½®æ¥å£ç¤ºä¾‹
//!
//! å±•ç¤ºå¦‚ä½•ä½¿ç”¨ç»Ÿä¸€çš„é…ç½®æ–¹å¼åˆ›å»ºä¸åŒçš„ LLM å®¢æˆ·ç«¯

use llm_connector::{LlmClient, types::{ChatRequest, Message}};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// ç»Ÿä¸€çš„ LLM åç«¯é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "provider", rename_all = "lowercase")]
pub enum LlmBackendConfig {
    OpenAI {
        api_key: String,
        #[serde(skip_serializing_if = "Option::is_none")]
        base_url: Option<String>,
        #[serde(default = "default_timeout")]
        timeout_ms: u64,
    },
    Anthropic {
        api_key: String,
        #[serde(default = "default_timeout")]
        timeout_ms: u64,
    },
    Zhipu {
        api_key: String,
        #[serde(default = "default_timeout")]
        timeout_ms: u64,
    },
    Aliyun {
        api_key: String,
    },
    Ollama {
        #[serde(skip_serializing_if = "Option::is_none")]
        base_url: Option<String>,
    },
}

fn default_timeout() -> u64 {
    30000 // 30ç§’é»˜è®¤è¶…æ—¶
}

impl LlmBackendConfig {
    /// ä»é…ç½®åˆ›å»º LLM å®¢æˆ·ç«¯
    pub fn create_client(&self) -> LlmClient {
        match self {
            LlmBackendConfig::OpenAI { api_key, base_url, timeout_ms } => {
                LlmClient::openai_with_timeout(api_key, base_url.as_deref(), *timeout_ms)
            }
            LlmBackendConfig::Anthropic { api_key, timeout_ms } => {
                LlmClient::anthropic_with_timeout(api_key, *timeout_ms)
            }
            LlmBackendConfig::Zhipu { api_key, timeout_ms } => {
                LlmClient::zhipu_with_timeout(api_key, *timeout_ms)
            }
            LlmBackendConfig::Aliyun { api_key } => {
                LlmClient::aliyun(api_key)
            }
            LlmBackendConfig::Ollama { base_url } => {
                LlmClient::ollama(base_url.as_deref())
            }
        }
    }

    /// è·å–æä¾›å•†åç§°
    pub fn provider_name(&self) -> &'static str {
        match self {
            LlmBackendConfig::OpenAI { .. } => "openai",
            LlmBackendConfig::Anthropic { .. } => "anthropic",
            LlmBackendConfig::Zhipu { .. } => "zhipu",
            LlmBackendConfig::Aliyun { .. } => "aliyun",
            LlmBackendConfig::Ollama { .. } => "ollama",
        }
    }

    /// è·å–è¶…æ—¶é…ç½®
    pub fn timeout_ms(&self) -> u64 {
        match self {
            LlmBackendConfig::OpenAI { timeout_ms, .. } => *timeout_ms,
            LlmBackendConfig::Anthropic { timeout_ms, .. } => *timeout_ms,
            LlmBackendConfig::Zhipu { timeout_ms, .. } => *timeout_ms,
            LlmBackendConfig::Aliyun { .. } => 30000, // Aliyun ä½¿ç”¨é»˜è®¤è¶…æ—¶
            LlmBackendConfig::Ollama { .. } => 30000, // Ollama ä½¿ç”¨é»˜è®¤è¶…æ—¶
        }
    }
}

/// å¤šæä¾›å•†é…ç½®ç®¡ç†å™¨
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MultiProviderConfig {
    pub providers: HashMap<String, LlmBackendConfig>,
    pub default_provider: String,
}

impl MultiProviderConfig {
    /// è·å–é»˜è®¤å®¢æˆ·ç«¯
    pub fn default_client(&self) -> Option<LlmClient> {
        self.providers.get(&self.default_provider)
            .map(|config| config.create_client())
    }

    /// è·å–æŒ‡å®šæä¾›å•†çš„å®¢æˆ·ç«¯
    pub fn client(&self, provider: &str) -> Option<LlmClient> {
        self.providers.get(provider)
            .map(|config| config.create_client())
    }

    /// åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æä¾›å•†
    pub fn available_providers(&self) -> Vec<&String> {
        self.providers.keys().collect()
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ”§ ç»Ÿä¸€é…ç½®æ¥å£ç¤ºä¾‹\n");

    // ç¤ºä¾‹1: å•ä¸ªæä¾›å•†é…ç½®
    println!("ğŸ“‹ ç¤ºä¾‹1: å•ä¸ªæä¾›å•†é…ç½®");
    
    let openai_config = LlmBackendConfig::OpenAI {
        api_key: std::env::var("OPENAI_API_KEY").unwrap_or_else(|_| "sk-test".to_string()),
        base_url: None,
        timeout_ms: 45000, // 45ç§’è¶…æ—¶
    };

    let client = openai_config.create_client();
    println!("âœ… åˆ›å»º {} å®¢æˆ·ç«¯ï¼Œè¶…æ—¶: {}ms", 
             openai_config.provider_name(), 
             openai_config.timeout_ms());

    // ç¤ºä¾‹2: ä» YAML é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆæ¨¡æ‹Ÿï¼‰
    println!("\nğŸ“‹ ç¤ºä¾‹2: å¤šæä¾›å•†é…ç½®");
    
    let yaml_config = r#"
providers:
  primary_openai:
    provider: openai
    api_key: "sk-your-openai-key"
    timeout_ms: 60000
  backup_zhipu:
    provider: zhipu
    api_key: "sk-your-zhipu-key"
    timeout_ms: 30000
  local_ollama:
    provider: ollama
    base_url: "http://localhost:11434"
  anthropic_claude:
    provider: anthropic
    api_key: "sk-ant-your-key"
    timeout_ms: 45000
default_provider: "primary_openai"
"#;

    if let Ok(multi_config) = serde_yaml::from_str::<MultiProviderConfig>(yaml_config) {
        println!("âœ… é…ç½®åŠ è½½æˆåŠŸ");
        println!("   é»˜è®¤æä¾›å•†: {}", multi_config.default_provider);
        println!("   å¯ç”¨æä¾›å•†: {:?}", multi_config.available_providers());

        // ä½¿ç”¨é»˜è®¤å®¢æˆ·ç«¯
        if let Some(default_client) = multi_config.default_client() {
            println!("   é»˜è®¤å®¢æˆ·ç«¯åè®®: {}", default_client.protocol_name());
        }

        // ä½¿ç”¨ç‰¹å®šæä¾›å•†
        if let Some(zhipu_client) = multi_config.client("backup_zhipu") {
            println!("   Zhipu å®¢æˆ·ç«¯åè®®: {}", zhipu_client.protocol_name());
        }
    }

    // ç¤ºä¾‹3: åŠ¨æ€é…ç½®åˆ‡æ¢
    println!("\nğŸ“‹ ç¤ºä¾‹3: åŠ¨æ€é…ç½®åˆ‡æ¢");
    
    let configs = vec![
        ("OpenAI", LlmBackendConfig::OpenAI {
            api_key: "sk-test".to_string(),
            base_url: Some("https://api.openai.com/v1".to_string()),
            timeout_ms: 30000,
        }),
        ("DeepSeek", LlmBackendConfig::OpenAI {
            api_key: "sk-test".to_string(),
            base_url: Some("https://api.deepseek.com/v1".to_string()),
            timeout_ms: 45000,
        }),
        ("Zhipu", LlmBackendConfig::Zhipu {
            api_key: "sk-test".to_string(),
            timeout_ms: 25000,
        }),
    ];

    for (name, config) in configs {
        let client = config.create_client();
        println!("   {} -> åè®®: {}, è¶…æ—¶: {}ms", 
                 name, 
                 client.protocol_name(), 
                 config.timeout_ms());
    }

    // ç¤ºä¾‹4: å®é™…æµ‹è¯•ï¼ˆå¦‚æœæœ‰æœ‰æ•ˆçš„ API Keyï¼‰
    println!("\nğŸ“‹ ç¤ºä¾‹4: å®é™…æµ‹è¯•");
    
    let test_configs = vec![
        ("ZHIPU_API_KEY", LlmBackendConfig::Zhipu {
            api_key: std::env::var("ZHIPU_API_KEY").unwrap_or_default(),
            timeout_ms: 15000,
        }),
        ("OPENAI_API_KEY", LlmBackendConfig::OpenAI {
            api_key: std::env::var("OPENAI_API_KEY").unwrap_or_default(),
            base_url: None,
            timeout_ms: 20000,
        }),
    ];

    for (env_var, config) in test_configs {
        if std::env::var(env_var).is_ok() {
            let client = config.create_client();
            println!("   æµ‹è¯• {} å®¢æˆ·ç«¯...", config.provider_name());
            
            let request = ChatRequest {
                model: match config.provider_name() {
                    "zhipu" => "glm-4-flash".to_string(),
                    "openai" => "gpt-3.5-turbo".to_string(),
                    _ => "default".to_string(),
                },
                messages: vec![Message::user("Hello!")],
                max_tokens: Some(10),
                ..Default::default()
            };

            match client.chat(&request).await {
                Ok(response) => {
                    println!("   âœ… æˆåŠŸ: {}", response.choices[0].message.content.chars().take(50).collect::<String>());
                }
                Err(e) => {
                    println!("   âŒ å¤±è´¥: {}", e);
                }
            }
        } else {
            println!("   â­ï¸  è·³è¿‡ {} æµ‹è¯•ï¼ˆæœªè®¾ç½®ç¯å¢ƒå˜é‡ï¼‰", env_var);
        }
    }

    println!("\nğŸ¯ ç»Ÿä¸€é…ç½®ç¤ºä¾‹å®Œæˆï¼");
    println!("\nğŸ’¡ ä½¿ç”¨å»ºè®®:");
    println!("   1. ä½¿ç”¨ LlmBackendConfig æšä¸¾ç»Ÿä¸€ç®¡ç†ä¸åŒæä¾›å•†");
    println!("   2. é€šè¿‡ YAML/JSON é…ç½®æ–‡ä»¶ç®¡ç†å¤šä¸ªæä¾›å•†");
    println!("   3. ä¸ºä¸åŒæä¾›å•†è®¾ç½®åˆé€‚çš„è¶…æ—¶æ—¶é—´");
    println!("   4. å®ç°é…ç½®çƒ­é‡è½½ä»¥æ”¯æŒåŠ¨æ€åˆ‡æ¢");

    Ok(())
}
