use llm_connector::{LlmClient, types::{ChatRequest, Message}};

#[cfg(feature = "streaming")]
use futures_util::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    #[cfg(not(feature = "streaming"))]
    {
        println!("âŒ æ­¤ç¤ºä¾‹éœ€è¦å¯ç”¨ streaming åŠŸèƒ½");
        println!("è¯·ä½¿ç”¨: cargo run --example ollama_streaming_simple --features streaming");
        return Ok(());
    }

    #[cfg(feature = "streaming")]
    {
        // ä½¿ç”¨æ™ºè°±AIä½œä¸ºç¤ºä¾‹
        let api_key = std::env::var("ZHIPU_API_KEY")
            .expect("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPU_API_KEY");

        let client = LlmClient::zhipu(&api_key);

        let request = ChatRequest {
            model: "glm-4-flash".to_string(),
            messages: vec![Message::user("ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚")],
            max_tokens: Some(100),
            ..Default::default()
        };

        println!("ğŸš€ Ollamaæ ¼å¼æµå¼è¾“å‡ºç¤ºä¾‹");
        println!("ğŸ¯ è¿™ç§æ ¼å¼ä¸Zed.devå…¼å®¹\n");

        // ä½¿ç”¨çº¯Ollamaæ ¼å¼çš„æµå¼è¾“å‡º
        let mut stream = client.chat_stream_ollama(&request).await?;

        println!("ğŸ’¬ AIå›å¤ï¼ˆçº¯Ollamaæ ¼å¼ï¼‰ï¼š");
        println!("{}", "-".repeat(40));

        while let Some(chunk) = stream.next().await {
            match chunk {
                Ok(ollama_chunk) => {
                    // ollama_chunk ç°åœ¨æ˜¯çº¯OllamaStreamChunkç±»å‹
                    if !ollama_chunk.message.content.is_empty() {
                        print!("{}", ollama_chunk.message.content);
                        std::io::Write::flush(&mut std::io::stdout()).unwrap();
                    }

                    // æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆchunk
                    if ollama_chunk.done {
                        println!("\n");
                        println!("{}", "-".repeat(40));
                        println!("âœ… æµå¼è¾“å‡ºå®Œæˆ");

                        // æ˜¾ç¤ºæœ€ç»ˆchunkçš„è¯¦ç»†ä¿¡æ¯
                        if ollama_chunk.prompt_eval_count.is_some() {
                            println!("\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡ï¼š");
                            if let Some(prompt_tokens) = ollama_chunk.prompt_eval_count {
                                println!("  è¾“å…¥tokens: {}", prompt_tokens);
                            }
                            if let Some(completion_tokens) = ollama_chunk.eval_count {
                                println!("  è¾“å‡ºtokens: {}", completion_tokens);
                            }
                            if let Some(total_duration) = ollama_chunk.total_duration {
                                println!("  æ€»è€—æ—¶: {}ms", total_duration / 1_000_000);
                            }
                        }

                        println!("\nğŸ” æœ€ç»ˆchunkç»“æ„:");
                        println!("  æ¨¡å‹: {}", ollama_chunk.model);
                        println!("  åˆ›å»ºæ—¶é—´: {}", ollama_chunk.created_at);
                        println!("  å®Œæˆæ ‡è®°: {}", ollama_chunk.done);
                        break;
                    }
                }
                Err(e) => {
                    println!("\nâŒ æµå¼è¾“å‡ºé”™è¯¯ï¼š{}", e);
                    break;
                }
            }
        }

        println!("\nğŸ’¡ è¯´æ˜ï¼š");
        println!("â€¢ è¿™ç§Ollamaæ ¼å¼çš„è¾“å‡ºå¯ä»¥ç›´æ¥ç”¨äºZed.dev");
        println!("â€¢ æ¯ä¸ªchunkéƒ½æ˜¯å®Œæ•´çš„JSONå¯¹è±¡");
        println!("â€¢ æœ€åä¸€ä¸ªchunkåŒ…å« 'done: true' æ ‡è®°");
        println!("â€¢ åŒ…å«è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯");
    }

    Ok(())
}
