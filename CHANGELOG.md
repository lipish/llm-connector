# Changelog

All notable changes to this project will be documented in this file.

## [0.5.15] - 2026-02-14

### üìù Documentation

- **README Refactoring** - Simplified from 1316 lines to ~220 lines
  - Moved detailed content to dedicated docs/ files with links
  - Cleaner structure with quick reference table
- **New Documentation Files**
  - `docs/STREAMING.md` - Comprehensive streaming guide
  - `docs/TOOLS.md` - Function calling / tools guide
  - `docs/MULTIMODAL.md` - Multi-modal content guide
- **Updated docs/PROVIDERS.md**
  - Added Xiaomi MiMo provider section
  - Added Google Gemini detailed configuration
  - Added Ollama Model Management section
  - Added environment variables reference table

## [0.5.14] - 2026-02-14

### üöÄ New Features

- **Xiaomi MiMo Provider**
  - Added support for Xiaomi MiMo API (OpenAI-compatible)
  - New provider functions: `xiaomi()`, `xiaomi_with_config()`
  - New client methods: `LlmClient::xiaomi()`, `LlmClient::xiaomi_with_config()`
  - Base URL: `https://api.xiaomimimo.com/v1`
  - Supported model: `mimo-v2-flash`
  - Added example: `examples/xiaomi_basic.rs`

## [0.5.13] - 2026-01-03

### üîß Breaking Changes

- **OpenAI-compatible providers**: Removed automatic `/v1` path appending
  - `OpenAIProtocol` endpoints now only append `/chat/completions` and `/models`
  - `ConfigurableProtocol::openai_compatible` templates no longer include `/v1`
  - `openai_with_config` defaults to `https://api.openai.com/v1` only when no base_url provided
  - Base URLs are now respected verbatim for all OpenAI-compatible providers

### üìù Documentation

- Updated provider tests to reflect new endpoint URL behavior

## [0.5.11] - 2026-01-02

### üöÄ New Features

- **Tencent Hunyuan Native API v3 Streaming**
  - Implemented `chat_stream` for Tencent Native API v3.
  - Added strict streaming response parsing for Hunyuan's PascalCase SSE format.
  - Added example: `examples/tencent_native_streaming.rs`

### üìù Documentation

- Updated README with Tencent Native streaming usage.
- Updated provider docs to mention Tencent streaming support.

## [0.5.12] - 2026-01-03

### üöÄ New Features

- **Google Gemini Streaming**
  - Implemented `chat_stream` for Google Gemini via SSE (`streamGenerateContent`).
  - Added example: `examples/google_streaming.rs`

### üîß Improvements

- Updated Google authentication to use `x-goog-api-key` header (per official docs).

### üìù Documentation

- Updated README with Google Gemini streaming usage and runnable example.

## [0.5.10] - 2026-01-02

### üöÄ New Features

- **Google Gemini provider**
  - `LlmClient::google(api_key)`
  - `LlmClient::google_with_config(api_key, base_url, timeout_secs, proxy)`
  - Example: `examples/google_basic.rs`

### üìù Documentation

- Updated README with Google Gemini provider usage and notes.

## [0.5.9] - 2026-01-02

### üîß Maintenance

- Version bump and release preparation.

## [0.5.8] - 2026-01-02

### ‚ö†Ô∏è Breaking Changes

#### Tencent Hunyuan Native API v3
- **BREAKING**: Replaced OpenAI-compatible wrapper with native Tencent Cloud API v3 using `TC3-HMAC-SHA256` signature.
- **Affected**: `LlmClient::tencent()` and `tencent()` provider functions.
- **New Signature**: `tencent(secret_id, secret_key)` (previously `tencent(api_key)`).
- **Rationale**: Support native signature verification for better security and stability.

### ‚ú® Improvements

- **Security**: Hardcoded API keys removed from documentation and code.
- **Documentation**: Updated Tencent guide with native API usage.


## [0.5.7] - 2025-11-23

### üöÄ New Features

#### Aliyun DashScope Tools Support
- **Added**: Full tool calling support for Aliyun DashScope API
  - Non-streaming tool calls
  - Streaming tool calls
  - Compatible with OpenAI tool format (no conversion needed)
- **Updated**: `AliyunParameters` - Added `tools` and `tool_choice` fields
- **Updated**: `AliyunMessage` - Added `tool_calls` field
- **Updated**: Request/response conversion to handle tools
- **Updated**: Streaming response to handle tool calls

### üîß Improvements

#### Repository Cleanup
- **Removed**: Personal tool configurations from git tracking
  - `.augment/rules/rust.md` - Augment AI configuration
  - `.zed/settings.json` - Zed editor configuration
  - Files removed from git but preserved locally
  - Already in `.gitignore` but were tracked before

### üß™ Testing

#### New Test Examples
- **Added**: `examples/test_aliyun_tools.rs`
  - Demonstrates non-streaming tool calls
  - Demonstrates streaming tool calls
  - Weather tool example
  - Calculator tool example

### üìö Documentation

#### New Documentation
- **Added**: `docs/ALIYUN_TOOLS_FIX_ANALYSIS.md`
  - Detailed problem analysis
  - Solution design
  - Code change examples
- **Added**: `docs/ALIYUN_TOOLS_IMPLEMENTATION_SUMMARY.md`
  - Implementation summary
  - Testing results
  - Usage examples

### ‚úÖ Verification

- All 82 tests passing
- Build successful
- Fully backward compatible
- No breaking changes

### üîÑ Migration

No migration needed. All changes are backward compatible.

#### Using Aliyun Tools (New Feature)

```rust
use llm_connector::{LlmClient, types::*};

let client = LlmClient::aliyun("your-api-key")?;

let tools = vec![
    Tool {
        tool_type: "function".to_string(),
        function: Function {
            name: "get_weather".to_string(),
            description: Some("Get weather information".to_string()),
            parameters: serde_json::json!({
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                },
                "required": ["location"]
            }),
        },
    },
];

let request = ChatRequest {
    model: "qwen-plus".to_string(),
    messages: vec![Message::text(Role::User, "What's the weather in Beijing?")],
    tools: Some(tools),
    tool_choice: Some(ToolChoice::Auto),
    ..Default::default()
};

let response = client.chat(&request).await?;

if let Some(tool_calls) = &response.choices[0].message.tool_calls {
    for tool_call in tool_calls {
        println!("Tool: {}", tool_call.function.name);
        println!("Arguments: {}", tool_call.function.arguments);
    }
}
```

---

## [0.5.6] - 2025-11-23

### üî• Critical Fix

#### Proxy Timeout Issue
- **Fixed**: Disabled system proxy by default in HTTP client
  - **Root cause**: reqwest automatically uses system proxy settings
  - **Problem**: System proxy can be slow, unreachable, or misconfigured
  - **Impact**: Caused timeout errors even when direct connection works
  - **Solution**: Explicitly disable system proxy, require explicit configuration

#### Changes
- **HttpClient::new()**: Added `.no_proxy()` to disable system proxy
- **HttpClient::with_config()**: Added `.no_proxy()` when proxy parameter is None
- **Explicit proxy**: Only use proxy when explicitly configured via parameters

### üéØ Benefits

1. **No unexpected timeouts** - System proxy no longer interferes
2. **Better performance** - Direct connections are faster
3. **Predictable behavior** - Same behavior across all environments
4. **Explicit control** - Users must explicitly set proxy if needed

### üß™ Testing

#### New Test Examples
- **Added**: `examples/test_proxy_issue.rs`
  - Tests default behavior (no proxy)
  - Tests invalid proxy handling
  - Checks system proxy environment variables

#### Test Results
- ‚úÖ All 82 tests passing
- ‚úÖ Zhipu streaming: 2.05s, 52 chunks (no timeout)
- ‚úÖ Invalid proxy fails fast: 41ms (no long timeout)
- ‚úÖ Direct connection works reliably

### üìö Documentation

#### New Documentation
- **Added**: `docs/PROXY_TIMEOUT_FIX.md`
  - Detailed technical explanation
  - Root cause analysis
  - Usage examples and migration guide
- **Added**: `docs/PROXY_FIX_SUMMARY.md`
  - Executive summary
  - Impact analysis
  - Migration guide for different scenarios

### üîÑ Migration

#### Most Users (No Change Needed)
```rust
// Works better now (no timeout issues)
let client = LlmClient::zhipu_openai_compatible(api_key)?;
```

#### Users Who Need Proxy
```rust
// Explicitly set proxy
let client = LlmClient::zhipu_with_config(
    api_key,
    true,                           // OpenAI compatible
    None,                           // Default base URL
    None,                           // Default timeout
    Some("http://proxy:8080"),      // Explicit proxy
)?;
```

#### Users Relying on System Proxy (Rare)
```rust
// Read system proxy and set explicitly
let proxy = std::env::var("HTTPS_PROXY").ok();
let client = LlmClient::zhipu_with_config(
    api_key,
    true,
    None,
    None,
    proxy.as_deref(),
)?;
```

### ‚ö†Ô∏è Breaking Change (Rare)

**Only affects users relying on system proxy settings**:
- Before: System proxy was used automatically
- After: System proxy is ignored, must be set explicitly

**Workaround**: See migration guide above

### ‚úÖ Verification

- All tests passing (82/82)
- Streaming verified with Zhipu GLM API
- Performance improved (no proxy overhead)
- Fully backward compatible for most users

---

## [0.5.5] - 2025-11-23

### üöÄ Improvements

#### Streaming Timeout Configuration
- **Increased**: Default HTTP timeout from 30s to 60s
  - Better support for long streaming responses
  - More reasonable default for LLM APIs
  - Reduces risk of premature timeouts
- **Added**: Streaming-specific HTTP headers
  - `Accept: text/event-stream` - Standard SSE header
  - `Cache-Control: no-cache` - Prevents caching issues
  - `Connection: keep-alive` - Maintains connection
- **Improved**: Error messages for timeout errors
  - Now suggests increasing timeout for long-running streams
  - More actionable guidance for troubleshooting

#### Code Quality
- **Completed**: Full Chinese to English translation
  - All source code comments now in English
  - 100% internationalization complete
  - Zero Chinese characters remaining in codebase
  - 18 source files completely translated
  - 4 cleanup scripts created for future reference

### üß™ Testing

#### New Test Examples
- **Added**: `examples/test_zhipu_streaming_timeout.rs`
  - Tests basic streaming functionality
  - Verifies timeout behavior
  - Measures chunk count and timing
- **Added**: `examples/test_zhipu_long_streaming.rs`
  - Tests long streaming responses (17+ seconds)
  - Verifies no timeout issues with extended streams
  - Demonstrates 600+ chunk handling

### üìö Documentation

#### New Documentation
- **Added**: `docs/STREAMING_TIMEOUT_FIX.md`
  - Detailed explanation of timeout improvements
  - Usage recommendations and guidelines
  - Timeout configuration examples
- **Added**: `docs/STREAMING_INVESTIGATION_REPORT.md`
  - Investigation results and findings
  - Test evidence and verification
  - Recommendations for integration
- **Added**: `docs/COMPLETE_CHINESE_CLEANUP_FINAL.md`
  - Complete translation summary
  - Statistics and verification
  - Quality assurance report
- **Added**: `docs/CHINESE_CLEANUP_STATUS.md`
  - Translation progress tracking
  - Remaining work documentation

### ‚úÖ Verification

- All 82 tests passing
- Streaming verified with Zhipu GLM API
- Short responses: 2.27s, 52 chunks ‚úÖ
- Long responses: 17.4s, 633 chunks ‚úÖ
- No timeout issues observed
- Fully backward compatible

### üîÑ Migration

No breaking changes. All existing code continues to work.

Optional: Use custom timeout for very long responses:
```rust
// 120 seconds timeout for long responses
let client = LlmClient::zhipu_with_timeout(api_key, 120)?;
```

---

## [0.5.4] - 2025-01-22

### üîß Bug Fixes

#### Streaming Tool Calls Fix
- **Fixed**: Incremental accumulation and deduplication logic for streaming tool_calls
  - Modified `ToolCall` and `FunctionCall` data structures to support incremental parsing
  - Implemented `merge_delta()` method for merging incremental data
  - Added accumulation state management in `sse_to_streaming_response`
  - Only send complete tool_calls to prevent duplicate execution
- **Improved**: Support for OpenAI streaming API's incremental tool_calls format
  - Added `index` field to identify tool_calls
  - Implemented `is_complete()` method to check completeness
  - Accumulate each tool_call's incremental data by `index`
- **Guaranteed**: Each tool_call is sent only once, preventing duplicate execution
- **Compatible**: Fully backward compatible, no impact on existing code
- **Tests**: Added comprehensive test suite to verify accumulation logic
  - `test_streaming_tool_calls_accumulation` - Verify accumulation logic
  - `test_streaming_tool_calls_parsing` - Verify incremental parsing
- **Documentation**:
  - Added `docs/STREAMING_TOOL_CALLS.md` - Technical documentation
  - Added `docs/STREAMING_TOOL_CALLS_FIX.md` - Fix summary
  - Updated `README.md` with Function Calling section

### üìö Documentation

#### Documentation Cleanup
- **Cleanup**: Moved outdated temporary documents to archive
  - `DOCS_CLEANUP_SUMMARY.md` ‚Üí `docs/archive/reports/`
  - `RELEASE_v0.5.3_SUMMARY.md` ‚Üí `docs/archive/releases/`
  - `SENSITIVE_INFO_OBFUSCATION.md` ‚Üí `docs/archive/reports/`
- **Updated**: `docs/README.md` with new documentation index
- **Simplified**: README.md changelog section, pointing to CHANGELOG.md
- **Removed**: Outdated references to non-existent examples (`test_keys_yaml`, `debug_deepseek`, `fetch_models_simple`)
- **Updated**: Examples section to reflect actual available examples
- **Improved**: Troubleshooting section with practical solutions
- **Reorganized**: README.md structure for better user experience
  - Moved "Supported Providers" overview right after Quick Start
  - Moved "Function Calling / Tools" and "Streaming" sections earlier
  - Moved architecture details ("Unified Output Format") to later sections
  - Removed duplicate sections
  - New flow: Introduction ‚Üí Features ‚Üí Quick Start ‚Üí Providers ‚Üí Key Features ‚Üí Architecture ‚Üí Advanced Topics

#### Language Standardization
- **Converted**: All Chinese text to English for international accessibility
  - README.md Function Calling section
  - CHANGELOG.md v0.5.4 section
  - Test files and example files
  - All comments and user-facing messages
  - Removed Chinese characters from provider names (Tencent Hunyuan, Volcengine, Moonshot)
  - `docs/README.md` - Converted entire documentation index to English
  - `docs/REASONING_MODELS_SUPPORT.md` - Completely rewritten in English
- **Simplified**: Removed excessive emoji usage from README.md
  - Kept minimal, professional formatting
  - Replaced emoji bullets with standard markdown bullets
  - Removed decorative emojis from section headers
- **Archived**: Moved original Chinese version of REASONING_MODELS_SUPPORT.md to `docs/archive/reports/`
- **Documentation**:
  - Added `docs/CHINESE_TO_ENGLISH_CONVERSION.md` for conversion summary
  - Added `docs/EMOJI_AND_CHINESE_CLEANUP.md` for cleanup summary

## [0.5.3] - 2025-01-15

### üéâ New Features

#### Universal Reasoning Models Support üß†
- **Added**: Universal support for reasoning models across all providers
- **Supported Models**:
  - ‚úÖ Volcengine Doubao-Seed-Code (`reasoning_content`)
  - ‚úÖ DeepSeek R1 (`reasoning_content` / `reasoning`)
  - ‚úÖ OpenAI o1 series (`thought` / `reasoning_content`)
  - ‚úÖ Qwen reasoning models (`reasoning`)
  - ‚úÖ Anthropic Claude extended thinking (`thinking`)
- **Key Benefits**:
  - Zero configuration - automatic field detection
  - Unified interface - same code for all reasoning models
  - Backward compatible - standard models work as before
  - Priority-based extraction - standard `content` field takes precedence
- **Documentation**: Added `docs/REASONING_MODELS_SUPPORT.md`

### üìö Documentation

#### Documentation Structure Cleanup
- **Reorganized**: Cleaned up docs directory from 52 to 30 files (-42%)
- **New Structure**:
  - Core documents (6): Architecture, migration guides, reasoning models support
  - Provider guides (7): Dedicated guide for each provider in `docs/guides/`
  - Archive (17): Historical releases and reports in `docs/archive/`
- **New Provider Guides**:
  - `docs/guides/ALIYUN_GUIDE.md` - Aliyun DashScope usage guide
  - `docs/guides/ANTHROPIC_GUIDE.md` - Anthropic Claude usage guide
  - `docs/guides/ZHIPU_GUIDE.md` - Zhipu GLM usage guide
  - Updated existing guides for DeepSeek, Moonshot, Tencent, Volcengine
- **Improvements**:
  - Clear documentation index in `docs/README.md`
  - Removed duplicate and outdated content
  - Better organization and discoverability

#### Security
- **Obfuscated**: All sensitive information in documentation and examples
  - API keys replaced with placeholders
  - Endpoint IDs replaced with example values
  - Created `keys.yaml.example` for configuration reference
- **Documentation**: Added `docs/SENSITIVE_INFO_OBFUSCATION.md`

### üêõ Bug Fixes

#### Volcengine Streaming Support
- **Fixed**: Volcengine streaming now correctly extracts content from reasoning models (Doubao-Seed-Code)
- **Issue**: `StreamingResponse.get_content()` returned `None` for Doubao-Seed-Code model responses
- **Root Cause**: Reasoning models output content in `delta.reasoning_content` field instead of `delta.content`
- **Solution**: Enhanced SSE parser to check multiple content fields in priority order:
  1. `delta.content` (standard OpenAI format, non-empty)
  2. `delta.reasoning_content` (Volcengine Doubao-Seed-Code, DeepSeek R1)
  3. `delta.reasoning` (Qwen, DeepSeek)
  4. `delta.thought` (OpenAI o1)
  5. `delta.thinking` (Anthropic)
- **Impact**: Benefits all reasoning models across different providers
- **Files Changed**: `src/sse.rs`
- **Tests Added**:
  - Unit test: `test_streaming_response_content_population`
  - Integration test: `examples/volcengine_streaming.rs`
  - Automation script: `scripts/test_volcengine_streaming.sh`
- **Documentation**:
  - `docs/VOLCENGINE_STREAMING_FIX.md` - Detailed fix documentation
  - `docs/VOLCENGINE_STREAMING_SUMMARY.md` - Fix summary
  - `docs/VOLCENGINE_STREAMING_FINAL_REPORT.md` - Final report
  - `docs/REASONING_MODELS_SUPPORT.md` - Universal reasoning models guide
- **Test Results**:
  - ‚úÖ 221 tests passed
  - ‚úÖ Volcengine streaming: 101 chunks, 477 chars extracted
  - ‚úÖ All existing functionality preserved

## [0.5.1] - 2025-01-21

### üîß Improvements

#### Code Quality
- Fixed all compilation errors and warnings discovered by rust-analyzer
- Fixed unused variable warnings by using underscore prefix
- Cleaned up 69% of example files (39 ‚Üí 12)
- Cleaned up 56% of test files (18 ‚Üí 8)
- Removed 36 duplicate, debug, and outdated files

#### Documentation
- Added `docs/RUST_CODING_RULES.md` - Rust coding standards
- Added `docs/MIGRATION_GUIDE_v0.5.0.md` - Complete migration guide
- Added `docs/RELEASE_v0.5.0.md` - Release notes
- Updated `examples/README.md` with cleaner structure
- Updated all examples to use new API

#### Examples Cleanup
- Removed duplicate examples (test_aliyun_basic.rs, test_deepseek.rs, etc.)
- Removed debug files (debug_aliyun_response.rs, debug_longcat_stream.rs, etc.)
- Removed verification files (verify_aliyun_choices.rs, verify_reasoning_content.rs, etc.)
- Removed shell test scripts (9 files)
- Renamed test_aliyun_enable_thinking.rs ‚Üí aliyun_thinking.rs

#### Bug Fixes
- Fixed Message construction in all examples
- Fixed content access using content_as_text()
- Fixed streaming examples with proper feature gates
- Fixed tencent_basic.rs API usage
- Fixed all integration tests

### üìä Statistics

- **Tests**: 221 passed; 0 failed (100% pass rate)
- **Compilation**: 0 errors, 0 warnings
- **Code reduction**: 74% fewer lines in examples/tests

## [0.5.0] - 2025-01-21

### üéâ Major Features - Multi-modal Content Support

**‚ö†Ô∏è BREAKING CHANGE**: `Message.content` changed from `String` to `Vec<MessageBlock>`

This is a major architectural improvement that enables native multi-modal content support (text + images).

#### New Types

- **`MessageBlock`** - Enum for different content types
  - `Text { text: String }` - Text content
  - `Image { source: ImageSource }` - Image (Anthropic format)
  - `ImageUrl { image_url: ImageUrl }` - Image URL (OpenAI format)
- **`ImageSource`** - Image source (Base64 or URL)
- **`ImageUrl`** - Image URL with optional detail level

#### Migration Guide

```rust
// Old (0.4.x)
let message = Message {
    role: Role::User,
    content: "Hello".to_string(),
    ..Default::default()
};

// New (0.5.0) - Option 1: Use text() constructor (recommended)
let message = Message::text(Role::User, "Hello");

// New (0.5.0) - Option 2: Use new() with MessageBlock
let message = Message::new(
    Role::User,
    vec![MessageBlock::text("Hello")],
);

// New (0.5.0) - Multi-modal example
let message = Message::new(
    Role::User,
    vec![
        MessageBlock::text("What's in this image?"),
        MessageBlock::image_url("https://example.com/image.jpg"),
    ],
);
```

#### New Methods

**Message**:
- `Message::text(role, text)` - Create text-only message
- `Message::new(role, blocks)` - Create multi-modal message
- `Message::content_as_text()` - Extract all text content
- `Message::is_text_only()` - Check if message contains only text
- `Message::has_images()` - Check if message contains images

**MessageBlock**:
- `MessageBlock::text(text)` - Create text block
- `MessageBlock::image_base64(media_type, data)` - Create Base64 image
- `MessageBlock::image_url(url)` - Create image URL block
- `MessageBlock::image_url_with_detail(url, detail)` - Create image URL with detail
- `MessageBlock::as_text()` - Get text content if it's a text block
- `MessageBlock::is_text()` - Check if it's a text block
- `MessageBlock::is_image()` - Check if it's an image block

#### Updated Protocols

- ‚úÖ **OpenAI** - Supports both string and array formats
- ‚úÖ **Anthropic** - Always uses array format
- ‚úÖ **Aliyun** - Converts to text format
- ‚úÖ **Zhipu** - Converts to text format
- ‚úÖ **Ollama** - Converts to text format

#### Examples

- `examples/multimodal_basic.rs` - Comprehensive multi-modal examples

#### Tests

- Added 8 new unit tests for `MessageBlock`
- All 64 tests passing

#### Documentation

- `docs/MULTIMODAL_CONTENT_DESIGN.md` - Design comparison
- `docs/MULTIMODAL_NATIVE_DESIGN.md` - Native design approach
- `docs/MULTIMODAL_MIGRATION_PLAN.md` - Migration plan

---

## [Unreleased]

### Added
- **Moonshot (Moonshot AI) Provider**
  - OpenAI-compatible API
  - `LlmClient::moonshot(api_key)`
  - Models: moonshot-v1-8k, moonshot-v1-32k, moonshot-v1-128k
  - Long context support (up to 128k tokens)
  - Full streaming support

- **DeepSeek Provider**
  - OpenAI-compatible API
  - `LlmClient::deepseek(api_key)`
  - Models: deepseek-chat, deepseek-reasoner
  - Reasoning model support with thinking process
  - Automatic extraction of reasoning content
  - Full streaming support for both chat and reasoning models

## [0.4.20] - 2025-10-21

### üéØ Major Update: Unified Output Format & Configuration-Driven Architecture

#### ‚ú® Unified Output Format

**All providers now output the same unified `StreamingResponse` format**, regardless of their native API format.

```
Different Input Formats ‚Üí Protocol Conversion ‚Üí Unified StreamingResponse
```

**Benefits**:
- ‚úÖ Consistent API across all providers
- ‚úÖ Easy provider switching without changing business logic
- ‚úÖ Type-safe compile-time guarantees
- ‚úÖ Lower learning curve - learn once, use everywhere

**Example**:
```rust
// Same code works with ANY provider
let mut stream = client.chat_stream(&request).await?;
while let Some(chunk) = stream.next().await {
    let chunk = chunk?;  // Always StreamingResponse
    if let Some(content) = chunk.get_content() {
        print!("{}", content);
    }
}
```

#### üèóÔ∏è Configuration-Driven Architecture

**New Core Modules**:

1. **ProviderBuilder** (`src/core/builder.rs`)
   - Unified builder pattern for all providers
   - Chain-able API: `.timeout()` / `.proxy()` / `.header()`
   - Eliminates repetitive `xxx_with_config` boilerplate
   - Reduces code by ~50%

2. **ConfigurableProtocol** (`src/core/configurable.rs`)
   - Configuration-driven protocol adapter
   - `ProtocolConfig` - Static configuration (name, endpoints, auth)
   - `EndpointConfig` - Endpoint templates with `{base_url}` variable
   - `AuthConfig` - Flexible authentication (Bearer/ApiKeyHeader/None/Custom)
   - New providers only need configuration, not code

**Code Reduction**:
- Tencent: 169 lines ‚Üí 122 lines (-28%)
- Volcengine: 169 lines ‚Üí 145 lines (-14%)
- LongCat: 169 lines ‚Üí 145 lines (-14%)
- **Average: -19% code reduction**

#### üÜï New Providers

1. **Tencent Hunyuan (Hunyuan) **
   - OpenAI-compatible API
   - `LlmClient::tencent(api_key)`
   - Models: hunyuan-lite, hunyuan-standard, hunyuan-pro, hunyuan-turbo

2. **LongCat API**
   - Dual format support
   - `LlmClient::longcat_openai(api_key)` - OpenAI format
   - `LlmClient::longcat_anthropic(api_key)` - Anthropic format with Bearer auth

#### üîß Anthropic Streaming Fix

**Problem**: LongCat Anthropic streaming failed with "missing field `id`" error

**Solution**: Implemented custom `parse_stream_response` for Anthropic protocol
- Correctly handles Anthropic's multi-event streaming format:
  - `message_start` - Extract message ID
  - `content_block_delta` - Extract text increments
  - `message_delta` - Extract usage and stop_reason
- Converts to unified `StreamingResponse` format
- **Now works perfectly with LongCat Anthropic!**

**Test Results**:
```
‚úÖ LongCat Anthropic non-streaming: Working
‚úÖ LongCat Anthropic streaming: Working (fixed!)
   - Total chunks: 20
   - Content chunks: 19
   - finish_reason: end_turn
   - usage: prompt_tokens: 15, completion_tokens: 30
```

#### üßπ Code Cleanup

- Removed deprecated v1 architecture code (5641 lines)
- Removed `v1-legacy` feature flag
- Cleaner codebase with focused abstractions

#### üìö Documentation

**New Documents**:
- `docs/REFACTORING_SUMMARY.md` - Complete refactoring documentation
- `docs/POST_REFACTORING_TEST_REPORT.md` - Comprehensive test report (90% pass rate)
- `docs/ANTHROPIC_STREAMING_FIX.md` - Anthropic streaming fix details

**Updated**:
- README.md - Added unified output format explanation
- README.md - Added new providers (Tencent, LongCat)

#### ‚úÖ Testing

**Comprehensive Testing**:
- ‚úÖ All providers tested: 10/10 tests passed
- ‚úÖ Non-streaming: 100% pass rate (5/5)
- ‚úÖ Streaming: 100% pass rate (5/5)
- ‚úÖ 46 unit tests passing
- ‚úÖ Full backward compatibility verified

**Tested Providers**:
- Tencent (refactored) - ‚úÖ Non-streaming + Streaming
- LongCat OpenAI (unchanged) - ‚úÖ Non-streaming + Streaming
- LongCat Anthropic (refactored) - ‚úÖ Non-streaming + Streaming (fixed!)
- Zhipu (unchanged) - ‚úÖ Non-streaming + Streaming
- Aliyun (unchanged) - ‚úÖ Non-streaming + Streaming

#### üìà Performance & Metrics

- Code reduction: -19% in refactored providers
- New provider cost: -70% (170 lines ‚Üí 50 lines)
- Maintenance cost: -50% (centralized logic)
- Test pass rate: 100% (10/10)

#### üîÑ Migration Guide

**No breaking changes!** All existing APIs continue to work.

**Before (still works)**:
```rust
let client = LlmClient::openai_compatible(
    "sk-...",
    "https://api.hunyuan.cloud.tencent.com",
    "tencent"
)?;
```

**After (recommended)**:
```rust
let client = LlmClient::tencent("sk-...")?;
```

---

## [0.4.19] - 2025-10-18

### ‚ú® New Features

#### **Add Dedicated Volcengine Provider**

**Volcengine Overview**:
- Volcengine is ByteDance's cloud platform
- Provides LLM services (Ark)
- Uses an OpenAI-compatible API format, but the endpoint path differs

**New Capabilities**:

1. **Create a VolcengineProtocol adapter**
   - Wraps the OpenAI protocol but uses Volcengine endpoint paths
   - Endpoint: `/api/v3/chat/completions` (instead of `/v1/chat/completions`)
   - Fully compatible with OpenAI request/response format

2. **Add dedicated client methods**
   - `LlmClient::volcengine()` - create a Volcengine client
   - `LlmClient::volcengine_with_config()` - client with custom configuration

3. **Support reasoning-model features**
   - Supports the `reasoning_content` field (thinking process)
   - In streaming responses, thinking content arrives before the final answer
   - Similar to OpenAI o1-style reasoning models

**Example**:

```rust
// Create client
let client = LlmClient::volcengine("xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx")?;

// Create request (use endpoint ID)
let request = ChatRequest {
    model: "ep-20250118155555-xxxxx".to_string(),  // endpoint ID
    messages: vec![Message {
        role: Role::User,
        content: "Hello".to_string(),
        ..Default::default()
    }],
    max_tokens: Some(1000),
    ..Default::default()
};

// Non-streaming
let response = client.chat(&request).await?;

// Streaming
#[cfg(feature = "streaming")]
{
    let mut stream = client.chat_stream(&request).await?;
    while let Some(chunk) = stream.next().await {
        // Handle streaming response
    }
}
```

**Test Results**:

| Feature | Status | Details |
|------|------|------|
| Non-streaming response | ‚úÖ | Fully working |
| Streaming response | ‚úÖ | Fully working |
| reasoning_content | ‚úÖ | Thinking process supported |
| llm-connector compatibility | ‚úÖ | Fully compatible |

**New Files**:
- `src/providers/volcengine.rs` - Volcengine dedicated provider
- `examples/test_volcengine.rs` - Test example
- `tests/test_volcengine_raw.sh` - Raw API test
- `tests/test_volcengine_streaming_raw.sh` - Streaming API test
- `docs/VOLCENGINE_GUIDE.md` - Full usage guide

**Important Notes**:
- Volcengine uses endpoint IDs (`ep-xxxxxx`) instead of model names
- Endpoint IDs must be created and retrieved in the Volcengine console
- API keys are UUID format, not `sk-` format

---

## [0.4.18] - 2025-10-18

### ‚ú® New Features

#### **Add LongCat API Support**

**LongCat Overview**:
- LongCat is an AI service platform providing high-performance chat models
- Supports both OpenAI and Anthropic API formats
- Daily free quota: 500,000 Tokens

**What‚Äôs Added**:

1. **LongCat OpenAI Format Support** - ‚úÖ Fully Available
   - Use `LlmClient::openai_compatible()` method
   - Endpoint: `https://api.longcat.chat/openai`
   - Supports non-streaming and streaming responses
   - Fully compatible with llm-connector
   - Works seamlessly with llm-connector
 
 2. **LongCat Anthropic Format Support** - ‚úÖ Non-streaming Available
    - Create `LongCatAnthropicProtocol` adapter
    - Uses `Authorization: Bearer` auth (instead of standard Anthropic `x-api-key`)
    - Add `LlmClient::longcat_anthropic()` method
    - Add `LlmClient::longcat_anthropic_with_config()` method
    - Supports non-streaming responses
    - ‚ö†Ô∏è Streaming not supported yet (Anthropic event format needs a dedicated parser)

**Example Usage**:

```rust
// Method 1: OpenAI format (recommended, streaming and non-streaming)
let client = LlmClient::openai_compatible(
    "ak_...",
    "https://api.longcat.chat/openai",
    "longcat"
)?;

// Method 2: Anthropic format (non-streaming only)
let client = LlmClient::longcat_anthropic("ak_...")?;
```

**Test Results**:

| Test Item | OpenAI Format | Anthropic Format |
|--------|------------|---------------|
| Non-streaming response | ‚úÖ Success | ‚úÖ Success |
| Streaming response | ‚úÖ Success | ‚ö†Ô∏è Not supported yet |
| llm-connector compatibility | ‚úÖ Fully compatible | ‚úÖ Non-streaming compatible |

**New Files**:
- `src/providers/longcat.rs` - LongCat Anthropic adapter
- `examples/test_longcat_openai.rs` - OpenAI format test
- `examples/test_longcat_anthropic.rs` - Anthropic format test
- `tests/test_longcat_anthropic_raw.sh` - Anthropic raw API test
- `tests/test_longcat_anthropic_streaming_raw.sh` - Streaming format test
- `docs/LONGCAT_TESTING_REPORT.md` - Full testing report

**Recommended Usage**:
- Streaming: `LlmClient::openai_compatible("ak_...", "https://api.longcat.chat/openai", "longcat")`
- Non-streaming: `LlmClient::longcat_anthropic("ak_...")` or the OpenAI format

 ### üêõ Bug Fixes
 
#### **Fix Missing Methods in AliyunProviderImpl**
 
**Issue**: Tests call `provider.protocol()` and `provider.client()`, but these methods did not exist.
 
**Fix**:
- Add `protocol()` to return a reference to the protocol instance
- Add `client()` to return a reference to the HTTP client
- Fix `models()` error messages to match test expectations
- Fix calls to non-existent methods in the `as_ollama()` doctest

 ### üìù Documentation
 
- Add `docs/LONGCAT_TESTING_REPORT.md` - Full LongCat API testing report
- Update `src/client.rs` - Add LongCat usage examples

---

## [0.4.17] - 2025-10-18

### üêõ Bug Fixes

#### **Fix Aliyun Response Parsing and Streaming**

**Issue 1: Inconsistent ChatResponse structure**

**Symptoms**:
- `choices` was empty
- `content` was populated but not derived from `choices[0]`
- `usage` information was missing

**Fix**:
- Update Aliyun response structs (`src/providers/aliyun.rs`) to include `usage`, `request_id`, and `finish_reason`
- Rebuild `choices` properly and derive convenience `content` from `choices[0].message.content`

**Issue 2: Streaming responses returned no content**

**Root cause**:
- Missing `X-DashScope-SSE: enable` header
- Missing `incremental_output: true` parameter
- Default SSE parsing did not match Aliyun's streaming format

**Fix**:
- Add streaming parameters and required headers
- Implement custom stream parsing and convert to unified `StreamingResponse`

**Result**:
- Non-streaming and streaming both work as expected
- Backward compatible (existing `content` field still works)

### üß™ Testing

**New Tests**:
1. `examples/test_aliyun_streaming.rs` - Streaming response test
2. `examples/verify_aliyun_choices.rs` - choices array verification
3. `tests/test_aliyun_streaming_format.sh` - API raw response test

### üìù Documentation
 
 - Add `docs/ALIYUN_FIXES_SUMMARY.md` - Aliyun fixes summary
 - Add `docs/CHATRESPONSE_DESIGN_ANALYSIS.md` - ChatResponse design analysis
 - Add `docs/ALIYUN_RESPONSE_VERIFICATION.md` - Aliyun response verification report

---

## [0.4.16] - 2025-10-18

### üêõ Bug Fixes

#### **Fix Duplicate Content-Type Header Causing Provider Failures**

**Problem**:
- Some providers (e.g., Aliyun) failed because `Content-Type` was set twice:
  - Once by `Protocol::auth_headers()`
  - Once by `.json(body)` in the HTTP client

**Fix**:
- Remove manual `Content-Type` from auth headers / provider header builders where `.json()` already sets it
- Apply the same cleanup across multiple providers to avoid duplicate header values

**Affected Providers**:
- ‚úÖ **Aliyun (DashScope)** - Fix failure to use
- ‚úÖ **Zhipu (GLM)** - Fix potential issue
- ‚úÖ **Anthropic (Vertex AI, Bedrock)** - Fix potential issue
- ‚úÖ **Ollama** - Fix potential issue
- ‚úÖ **OpenAI (Azure, Compatible)** - Fix potential issue

**Testing Verification**:
- ‚úÖ Compile successfully
- ‚úÖ Add `examples/test_aliyun_basic.rs` to verify fix
- ‚úÖ All providers no longer set duplicate `Content-Type`

**Fix Statistics**:
- Fixed files: 5
- Fixed providers: 6
- Removed duplicate settings: 9
- Added comments: 9

**Impact**:
- ‚úÖ Fix Aliyun provider failure to use
- ‚úÖ Fix potential compatibility issues in other providers
- ‚úÖ Improve HTTP header setting consistency
- ‚úÖ Fully backward compatible, no user code changes needed

- ‚úÖ **Ollama** - Fix potential issue
- ‚úÖ **OpenAI (Azure, Compatible)** - Fix potential issue

**Testing Verification**:
- ‚úÖ Compile successfully
- ‚úÖ Add `examples/test_aliyun_basic.rs` to verify fix
- ‚úÖ All providers no longer set duplicate `Content-Type`

**Fix Statistics**:
- Fixed files: 5
- Fixed providers: 6
- Removed duplicate settings: 9
- Added comments: 9

**Impact**:
- ‚úÖ Fix critical Aliyun provider failure to use
- ‚úÖ Fix potential compatibility issues in other providers
- ‚úÖ Improve HTTP header setting consistency
- ‚úÖ Fully backward compatible, no user code changes needed

### üß™ Testing

#### **Add Zhipu Streaming tool_calls Verification Tests**

**New Tests**:
1. `tests/test_zhipu_streaming_direct.sh` - Test Zhipu API raw streaming response
2. `examples/test_zhipu_flash_streaming_tool_calls.rs` - Test llm-connector parsing
3. `examples/debug_zhipu_streaming_tool_calls.rs` - Detailed debug example

**Verification Results**:
- ‚úÖ Zhipu API returns tool_calls in streaming mode
- ‚úÖ llm-connector parses tool_calls correctly
- ‚úÖ Confirms llm-connector 0.4.15 works as expected

### üìù Documentation

- Add `docs/FIX_DUPLICATE_CONTENT_TYPE_HEADER.md` - Duplicate header fix documentation
- Add `docs/ZHIPU_STREAMING_TOOL_CALLS_VERIFICATION.md` - Zhipu streaming verification report

---

## [0.4.15] - 2025-10-18

### Bug Fixes

#### **Fix Example Compilation Errors and Warnings**

**Fix**:
1. **Remove calls to non-existent methods** (`examples/test_openai_tool_streaming.rs`)
   - Remove calls to `LlmClient::openrouter()` (non-existent)
   - Use `LlmClient::openai()` instead
 
2. **Fix type errors** (`examples/test_openai_tool_streaming.rs`)
   - Fix tool_calls reference type issues
   - Replace `&tool_calls_buffer[0]` with `tool_calls_buffer[0].clone()`
 
3. **Reduce unused import warnings** (7 example files)
   - Move streaming imports under `#[cfg(feature = "streaming")]`
   - Avoid unused import warnings when streaming is disabled
   - Affected files:
     - `test_zhipu_tool_messages_detailed.rs`
     - `test_deepseek_tools.rs`
     - `test_openai_tool_streaming.rs`
     - `test_zhipu_tool_streaming_issue.rs`
     - `test_glm_models_tool_streaming.rs`
     - `zhipu_tools_streaming.rs`
     - `test_all_providers_tool_streaming.rs`

4. **Reduce unused field warnings** (`examples/test_all_providers_tool_streaming.rs`)
   - Add `#[allow(dead_code)]` to `TestResult`
 
5. **Fix clippy warnings**
   - Fix doc comment empty-line warnings
   - Replace `len() > 0` with `!is_empty()`

### Documentation
 
- Add `docs/EXAMPLES_AND_TESTS_REVIEW.md` - Examples and tests review report
- Add `docs/RELEASE_v0.4.14.md` - v0.4.14 release summary

**Verification**:
- All examples compile
- All tests pass
- No build errors
- Significantly fewer warnings

**Impact**:
- Fix example compilation issues
- Improve code quality
- Fully backward compatible

---

## [0.4.14] - 2025-10-18

### Bug Fixes

#### **Fix OpenAI Tool Calling Support + Remove Zhipu GLM Streaming Forced Fallback**

**Issue 1: OpenAI protocol missing tool calling support**

**Symptoms**:
- Missing fields in request/message/response for tool calling
- Tool calling did not work for providers built on the OpenAI protocol

**Fix**:
1. **Add tool fields to OpenAIRequest** (`src/protocols/openai.rs`)
   ```rust
   pub struct OpenAIRequest {
       // ... other fields
       pub tools: Option<Vec<serde_json::Value>>,      // ‚úÖ added
       pub tool_choice: Option<serde_json::Value>,     // ‚úÖ added
   }
   ```

2. **Add tool fields to OpenAIMessage** (`src/protocols/openai.rs`)
   ```rust
   pub struct OpenAIMessage {
       pub role: String,
       pub content: String,
       pub tool_calls: Option<Vec<serde_json::Value>>,  // ‚úÖ added
       pub tool_call_id: Option<String>,                // ‚úÖ added
       pub name: Option<String>,                        // ‚úÖ added
   }
   ```

3. **Add tool fields to OpenAIResponseMessage** (`src/protocols/openai.rs`)
   ```rust
   pub struct OpenAIResponseMessage {
       pub content: Option<String>,                     // ‚úÖ changed to Option
       pub tool_calls: Option<Vec<serde_json::Value>>,  // ‚úÖ added
   }
   ```

4. **Map tool calling fields in build_request** (`src/protocols/openai.rs:48-106`)
   - Map `tools`
   - Map `tool_choice`
   - Map message fields `tool_calls`, `tool_call_id`, `name`

5. **Parse tool calls in parse_response** (`src/protocols/openai.rs:116-149`)
   - Extract `tool_calls` from the response
   - Convert to the unified `ToolCall` type

**Issue 2: Zhipu GLM streaming was forced to fall back**

**Symptoms**:
- Hard-coded logic in `src/core/traits.rs` forced non-streaming when `Role::Tool` messages were present
- ‚ùå GLM-4.5 should return many streaming chunks, but tool results caused a forced fallback to a single chunk
- ‚ùå This was a temporary workaround and is no longer needed

**Fix**:
- Remove the hard-coded workaround (`src/core/traits.rs:155-173`)
- Zhipu GLM streaming now works when tool results are included

**Verification**:
- ‚úÖ OpenAI protocol fully supports tool calling (tools, tool_choice, tool_calls)
- ‚úÖ DeepSeek tool calling works
- ‚úÖ All OpenAI-compatible services can use tool calling
- ‚úÖ Zhipu GLM streaming works with Role::Tool
- ‚úÖ All core library tests pass (27 tests)

**New Example**:
- `examples/verify_tool_fix.rs` - Verify tool calling fix

**Impact**:
- Fix tool calling for all OpenAI-protocol based services
- Remove Zhipu GLM streaming limitation
- Fully backward compatible

---

## [0.4.13] - 2025-10-18

### üêõ Bug Fixes

#### **Fix Zhipu GLM Multi-round Tool Calling Support**

**Summary**:
- Add missing `tool_call_id` and `name` fields for tool messages
- Ensure multi-round function calling flows work correctly

**Details**:
1. **ZhipuMessage struct updates** (`src/providers/zhipu.rs:272-282`)
   ```rust
   pub struct ZhipuMessage {
       pub role: String,
       pub content: String,
       pub tool_calls: Option<Vec<serde_json::Value>>,
       pub tool_call_id: Option<String>,  // ‚úÖ added
       pub name: Option<String>,          // ‚úÖ added
   }
   ```

2. **build_request mapping updates** (`src/providers/zhipu.rs:77-96`)
   - Map `tool_call_id` correctly
   - Map `name` correctly

**Verification**:
- ‚úÖ Single-round: User prompt ‚Üí tool_calls returned
- ‚úÖ Multi-round: Tool result ‚Üí text response returned
- ‚úÖ Three-round: Follow-up triggers new tool_calls
- ‚úÖ Tool message serialization is correct (`role="tool"`, `tool_call_id`, `name`)

**New Examples**:
- `examples/zhipu_multiround_tools.rs` - Multi-round tool calling demo
- `examples/zhipu_tools_edge_cases.rs` - Edge case tests
- `examples/verify_tool_message_serialization.rs` - Serialization verification

**Impact**:
- Fix multi-round tool calling for Zhipu GLM
- Align with OpenAI Function Calling conventions
- Backward compatible

---

## [0.4.12] - 2025-10-18

### üêõ Bug Fixes

#### **Fix Zhipu GLM Streaming Responses and Tool Calling Support**

**Summary**:
- Implement a Zhipu-specific streaming parser for single-newline SSE
- Ensure `content` is populated correctly for streaming chunks
- Add missing request fields (`stream`, `tools`, `tool_choice`) and response parsing for tool_calls

**Details**:
1. **Streaming parser** (`src/providers/zhipu.rs:126-201`)
   - Implement Zhipu-specific `parse_stream_response()`
   - Support single-newline SSE
   - Populate `content` from `delta.content`
   
2. **Request fields** (`src/providers/zhipu.rs:216-234`)
   - Add `stream: Option<bool>`
   - Add `tools: Option<Vec<Tool>>`
   - Add `tool_choice: Option<ToolChoice>`
   
3. **Response parsing** (`src/providers/zhipu.rs:240-264`)
   - Use `#[serde(default)]` for `ZhipuMessage.content` (may be empty for tool calls)
   - Support `ZhipuMessage.tool_calls`
   - Ensure `ZhipuResponse` includes id/created/usage
   - Support `finish_reason` in `ZhipuChoice`

**Verification**:
- Streaming: chunks and output received correctly
- Tool calling works in both non-streaming and streaming modes

**Impact**:
- Only affects the Zhipu GLM provider
- Backward compatible
- Aligns behavior with the OpenAI protocol

**New Examples**:
- `examples/zhipu_tools.rs` - Tool calling (non-streaming)
- `examples/zhipu_tools_streaming.rs` - Tool calling (streaming)

---

## [0.4.11] - 2025-10-17

### üêõ Bug Fixes

**Initial fix for Zhipu streaming response parsing**
- Implement dedicated `ZhipuProtocol::parse_stream_response()`
- Support single-newline SSE format
- Handle `data:` prefix with or without spaces

---

## [Unreleased]

### üêõ **BUGFIX: Fix Zhipu Streaming Response Parsing**

**Problem**:
Zhipu API uses single-newline SSE events (`data: {...}\n`) rather than the standard double-newline format. This caused the default SSE parser to produce zero chunks.

**Fix**:
- Add a dedicated `ZhipuProtocol::parse_stream_response()` parser
- Support single-newline SSE
- Handle `data:` prefix with or without spaces
- Skip `[DONE]` and empty payloads
- Provide detailed error messages including raw JSON

**Test Improvements**:
- Update `examples/zhipu_streaming.rs` with chunk counters and parser type hints
  - Use `glm-4-flash` for faster responses
  - Add a warning when zero chunks are produced

#### **Impact**
- ‚úÖ **Fix**: Zhipu streaming API works correctly
- ‚úÖ **Compatibility**: Does not affect other providers' streaming
- ‚úÖ **Debuggability**: Show raw JSON on parse failures

---

### ‚ú® **FEAT: API Naming Standardization**

#### **Changed**
- **Unified Constructor Naming**
  - `ollama_with_url()` ‚Üí `ollama_with_base_url()` (kept old name as deprecated)
  - Removed redundant `zhipu_default()` (use `zhipu()` directly)
  - All providers now follow consistent `{provider}_with_base_url()` pattern

#### **Added**
- **Type-Safe Provider Conversions**
  - `LlmClient::as_ollama()` ‚Üí `Option<&OllamaProvider>`
  - `LlmClient::as_openai()` ‚Üí `Option<&OpenAIProvider>`
  - `LlmClient::as_aliyun()` ‚Üí `Option<&AliyunProvider>`
  - `LlmClient::as_anthropic()` ‚Üí `Option<&AnthropicProvider>`
  - `LlmClient::as_zhipu()` ‚Üí `Option<&ZhipuProvider>`
  
- **API Key Validation Functions**
  - `validate_openai_key()`
  - `validate_aliyun_key()`
  - `validate_anthropic_key()` (already existed)
  - `validate_zhipu_key()` (already existed)

- **Advanced Configuration Methods**
  - All `{provider}_with_config()` methods now exposed in `LlmClient`
  - All `{provider}_with_timeout()` methods now exposed in `LlmClient`
  - Cloud-specific methods: `anthropic_vertex()`, `anthropic_bedrock()`, `aliyun_international()`, etc.

#### **Documentation**
- **NEW**: `docs/NAMING_CONVENTIONS.md` - Comprehensive naming standards guide
- **NEW**: `.augment/rules/naming.md` - Qoder auto-check rules
- Updated all examples to use new naming conventions

#### **Deprecated**
- `LlmClient::ollama_with_url()` ‚Üí Use `ollama_with_base_url()`
- `providers::zhipu_default()` ‚Üí Use `zhipu()` directly
- `LlmClient::ollama()` (the method, not constructor) ‚Üí Use `as_ollama()`

#### **Impact**
- ‚úÖ **Consistency**: All providers follow same naming pattern
- ‚úÖ **Type Safety**: No more manual `downcast_ref` needed
- ‚úÖ **Completeness**: All provider variants exposed in `LlmClient`
- ‚úÖ **Documentation**: Clear naming rules for contributors

---

## [0.4.9] - 2025-10-16

### üìö **DOCS: Fix API Documentation and Examples**

#### **Fixed**
- **README API Examples** - Updated streaming API examples to reflect current V2 architecture
  - Replaced deprecated `chat_stream_universal()`, `chat_stream_sse()`, `chat_stream_ndjson()` with current `chat_stream()`
  - Updated streaming examples to use `StreamingResponse` and `get_content()` method
  - Added clear distinction between V2 (current) and V1 (legacy) APIs in changelog
  - Fixed 29 documentation tests that used incorrect import paths

#### **Added**
- **New Example**: `streaming_v2_demo.rs` - Demonstrates current V2 streaming API
- **API Clarification**: Clear documentation of current streaming interface
- **Migration Guide**: Explains differences between V1 and V2 streaming APIs

#### **Impact**
- ‚úÖ **Documentation**: All examples now reflect current API
- ‚úÖ **Tests**: All 93 tests pass (including 50 documentation tests)
- ‚úÖ **Clarity**: Clear separation between current and legacy APIs
- ‚úÖ **Examples**: Working examples for current streaming interface

## [0.4.8] - 2025-10-16

### üîß **REFACTOR: Simplify Configuration Module Structure**

#### **Simplified**
- **Configuration Module** - Simplified `src/config/` directory to single `src/config.rs` file
  - Eliminated confusion between `src/config/provider.rs` and `src/providers/` directory
  - Consolidated all configuration types into single, clear module
  - Maintained all existing functionality and API compatibility
  - All 28 unit tests pass

#### **Structure Changes**
- **Before**: `src/config/mod.rs` + `src/config/provider.rs` (confusing)
- **After**: `src/config.rs` (clear and simple)
- **Benefits**: No naming confusion, easier to find configuration code, simpler maintenance

#### **Impact**
- ‚úÖ **Clarity**: Eliminated naming confusion with providers
- ‚úÖ **Simplicity**: Single file for all configuration needs
- ‚úÖ **Maintainability**: Easier to locate and modify configuration code
- ‚úÖ **Compatibility**: No breaking changes to public API

## [0.4.7] - 2025-10-16

### üèóÔ∏è **ARCHITECTURE: Correct Protocol vs Provider Separation**

#### **Refactored**
- **Protocol/Provider Architecture** - Implemented correct separation of public vs private protocols
  - **Public Protocols** (`src/protocols/`): Only industry-standard protocols (OpenAI, Anthropic)
  - **Private Protocols** (`src/providers/`): Provider-specific protocols inline with implementations
  - Moved `AliyunProtocol` and `ZhipuProtocol` from `protocols/` to `providers/` as private protocols
  - Updated exports: Standard protocols from `protocols`, private protocols from `providers`
  - All 78 unit and integration tests pass

#### **Design Principles**
- **Public Protocols**: Industry-recognized standards that multiple providers might implement
- **Private Protocols**: Provider-specific APIs that are tightly coupled to their implementations
- **Clean Separation**: Protocols define API formats, providers implement service logic
- **Maintainability**: Private protocols are co-located with their implementations

#### **Impact**
- ‚úÖ **Architecture**: Clean separation of public vs private protocols
- ‚úÖ **Maintainability**: Private protocols are easier to maintain alongside providers
- ‚úÖ **Extensibility**: Clear guidelines for adding new protocols vs providers
- ‚úÖ **Tests**: All functionality tests pass (78/78)

## [0.4.6] - 2025-10-16

### üîß **HOTFIX: Streaming Integration Test Errors**

#### **Fixed**
- **Streaming integration test compilation errors** - Fixed all compilation errors in streaming tests
  - Fixed `tests/streaming_integration_tests.rs`: Added missing `Role` import
  - Updated all `Message::user()` calls to use proper `Message` construction with `Role::User`
  - Fixed all client creation calls: `.unwrap()` ‚Üí `?` for V2 architecture
  - Fixed error handling test to properly detect authentication errors
  - All streaming integration tests now pass (4/4 passed, 4 ignored for API keys)

#### **Impact**
- ‚úÖ **Streaming Tests**: All streaming integration tests compile and pass
- ‚úÖ **Test Coverage**: Complete test coverage for streaming functionality
- ‚úÖ **V2 Architecture**: All tests use correct V2 architecture APIs

## [0.4.5] - 2025-10-16

### üîß **HOTFIX: Test Compilation Errors**

#### **Fixed**
- **Test compilation errors** - Fixed compilation errors in test files
  - Fixed `tests/client_tests.rs`: Updated `protocol_name()` ‚Üí `provider_name()` method calls
  - Fixed main documentation tests in `src/lib.rs` and `src/client.rs`
  - Updated import statements to use correct V2 architecture paths
  - All unit tests and integration tests now pass successfully

#### **Impact**
- ‚úÖ **Tests**: All unit and integration tests compile and pass (78/78)
- ‚úÖ **Documentation**: Main documentation examples work correctly
- ‚úÖ **CI/CD**: Test suite runs successfully for automated builds

## [0.4.4] - 2025-10-16

### üîß **HOTFIX: Examples Compilation Errors**

#### **Fixed**
- **Examples compilation errors** - Fixed all compilation errors and warnings in example files
  - Updated `examples/zhipu_basic.rs`: Fixed API calls and imports for V2 architecture
  - Updated `examples/zhipu_streaming.rs`: Fixed Message construction and client creation
  - Updated `examples/streaming_basic.rs`: Fixed imports and Result handling
  - Updated `examples/ollama_model_management.rs`: Fixed Ollama provider interface usage
  - Updated `examples/v1_vs_v2_comparison.rs`: Removed deprecated feature flags
  - All examples now use V2 architecture APIs correctly

#### **Impact**
- ‚úÖ **Examples**: All examples compile and run successfully
- ‚úÖ **Documentation**: Examples serve as accurate V2 architecture documentation
- ‚úÖ **User Experience**: Users can run examples without compilation errors

## [0.4.3] - 2025-10-16

### üîß **HOTFIX: Module Privacy Error**

#### **Fixed**
- **Critical module privacy error** - Fixed private module access in streaming functionality
  - Fixed import path: `crate::types::streaming::ChatStream` ‚Üí `crate::types::ChatStream`
  - Fixed import path: `crate::types::streaming::StreamingResponse` ‚Üí `crate::types::StreamingResponse`
  - The `streaming` module is conditionally exported and should be accessed through `types` module
  - Affected file: `src/sse.rs`

#### **Impact**
- ‚úÖ **Compilation**: Now compiles successfully without privacy errors
- ‚úÖ **Streaming**: All streaming features work correctly
- ‚úÖ **Functionality**: No breaking changes to public API

## [0.4.2] - 2025-10-16

### üîß **HOTFIX: Type Mismatch Error**

#### **Fixed**
- **Critical type mismatch error** - Fixed streaming response type conversion
  - Added `sse_to_streaming_response()` function to convert `String` stream to `StreamingResponse` stream
  - Fixed type mismatch: expected `StreamingResponse` but found `String` in streaming methods
  - Affected files: `src/sse.rs`, `src/core/traits.rs`, `src/protocols/zhipu.rs`, `src/providers/ollama.rs`
  - All streaming functionality now works correctly with proper type conversion

#### **Impact**
- ‚úÖ **Compilation**: Now compiles successfully without type errors
- ‚úÖ **Streaming**: All streaming features work with correct types
- ‚úÖ **Functionality**: No breaking changes to public API

## [0.4.1] - 2025-10-16

### üîß **HOTFIX: Compilation Error**

#### **Fixed**
- **Critical compilation error** - Fixed unresolved import `crate::sse::SseStream`
  - Replaced incorrect `SseStream::new(response)` calls with `crate::sse::sse_events(response)`
  - Affected files: `src/core/traits.rs`, `src/protocols/zhipu.rs`, `src/providers/ollama.rs`
  - All streaming functionality now works correctly

#### **Impact**
- ‚úÖ **Compilation**: Now compiles successfully without errors
- ‚úÖ **Streaming**: All streaming features work as expected
- ‚úÖ **Functionality**: No breaking changes to public API

## [0.4.0] - 2025-10-16

### üöÄ **MAJOR RELEASE: V2 Architecture**

This is a major release that introduces the new V2 architecture as the default, providing significant performance improvements and a cleaner API design.

#### ‚ö° **Performance Improvements**
- **7,000x+ faster client creation** - From ~53ms to ~7¬µs
- **Minimal memory footprint** - Only 16 bytes per client instance
- **Zero-cost cloning** - Arc-based sharing for efficient cloning

#### üèóÔ∏è **New Architecture**
- **Clear Protocol/Provider separation** - Protocols define API specs, Providers implement services
- **Unified trait system** - `Protocol` and `Provider` traits for maximum extensibility
- **Type-safe HTTP client** - Compile-time guarantees for correctness
- **Generic provider implementation** - `GenericProvider<Protocol>` for most use cases

#### üîÑ **API Changes (Breaking)**

##### **Client Creation**
```rust
// V1 (Legacy)
let client = LlmClient::openai("sk-...", None);
let client = LlmClient::ollama(None);

// V2 (New Default)
let client = LlmClient::openai("sk-...")?;  // Returns Result
let client = LlmClient::ollama()?;          // Returns Result
```

##### **Method Renames**
```rust
// V1 ‚Üí V2
client.fetch_models()  ‚Üí client.models()
client.protocol_name() ‚Üí client.provider_name()
```

##### **Parameter Changes**
- **OpenAI**: Removed optional second parameter, use dedicated methods
  - `openai(key, Some(url))` ‚Üí `openai_with_base_url(key, url)?`
- **Ollama**: Removed optional parameter
  - `ollama(Some(url))` ‚Üí `ollama_with_url(url)?`

#### üÜï **New Features**

##### **Additional Client Creation Methods**
```rust
// Azure OpenAI support
LlmClient::azure_openai("key", "endpoint", "version")?

// OpenAI-compatible services
LlmClient::openai_compatible("key", "url", "name")?

// Zhipu GLM OpenAI-compatible mode
LlmClient::zhipu_openai_compatible("key")?

// Enhanced configuration options
LlmClient::openai_with_config("key", url, timeout, proxy)?
```

##### **Enhanced Ollama Support**
```rust
// Direct access to Ollama-specific features
if let Some(ollama) = client.as_ollama() {
    ollama.pull_model("llama2").await?;
    let models = ollama.models().await?;
}
```

#### üì¶ **Protocol Support**
- **OpenAI Protocol** - Complete OpenAI API specification
- **Anthropic Protocol** - Full Claude API support with Vertex AI and Bedrock
- **Aliyun Protocol** - DashScope API with regional support
- **Zhipu Protocol** - Native and OpenAI-compatible formats
- **Ollama Provider** - Custom implementation with model management

#### üîÑ **Migration Guide**

##### **Option 1: Backward Compatibility**
```toml
# Cargo.toml
[features]
v1-legacy = []
```

```rust
// Use V1 API
#[cfg(feature = "v1-legacy")]
use llm_connector::v1::LlmClient;

// Use V2 API (default)
#[cfg(not(feature = "v1-legacy"))]
use llm_connector::LlmClient;
```

##### **Option 2: Direct Migration**
1. Add `?` to handle `Result` return types
2. Update method names: `fetch_models()` ‚Üí `models()`, `protocol_name()` ‚Üí `provider_name()`
3. Replace parameter patterns with dedicated methods
4. Update imports if using internal traits

#### üèõÔ∏è **Architecture Benefits**
- **Extensibility** - Easy to add new protocols and providers
- **Type Safety** - Compile-time guarantees for all operations
- **Performance** - Optimized for speed and memory efficiency
- **Clarity** - Clear separation of concerns between protocols and providers
- **Maintainability** - Reduced code duplication and cleaner abstractions

## [0.3.6] - 2025-10-14

### ‚ú® Added

#### Ollama Streaming Support
- Implemented line-delimited JSON streaming for Ollama protocol
  - Added non-SSE parser for JSON lines stream
  - Integrated into core streaming pipeline with protocol switch
  - Normalized to `StreamingResponse` with `get_content()` for output
- Added `examples/ollama_streaming.rs` demonstrating `chat_stream()` usage

### üìù Updated
- README and examples already standardized to use `get_content()` for streaming output

## [0.2.3] - 2025-01-06

### ‚ú® Added

#### Improved Error Messages
- **Cleaned up authentication error messages** for OpenAI-compatible providers
  - Removes OpenAI-specific URLs (like "platform.openai.com") from error messages
  - Adds helpful context: "Please verify your API key is correct and has the necessary permissions"
  - Makes errors more generic and applicable to all OpenAI-compatible providers (DeepSeek, Zhipu, Moonshot, etc.)

#### New Debug Tools
- **Added `debug_deepseek.rs` example** for troubleshooting authentication issues
  - Validates API key format
  - Tests model fetching and chat requests
  - Provides specific troubleshooting guidance based on error type
  - Can accept API key from command line or environment variable

#### Documentation
- **Added `TROUBLESHOOTING.md`** - Comprehensive troubleshooting guide
  - Authentication errors and solutions
  - Connection errors and debugging steps
  - Rate limit handling
  - Model not found errors
  - Provider-specific instructions for DeepSeek, OpenAI, Zhipu, Moonshot
  - Example code for common scenarios

### üîß Changed

#### Simplified API - Removed `supported_models()`
- **Removed `supported_models()` method** from all traits and implementations
  - Removed from `Provider` trait
  - Removed from `ProviderAdapter` trait
  - Removed from `LlmClient`
  - Removed from all protocol implementations (OpenAI, Anthropic, Aliyun, Ollama)
- **Removed `supports_model()` method** from `Provider` trait (was dependent on `supported_models()`)
- **Removed hardcoded model lists** from protocol structs
  - Removed `supported_models` field from `AnthropicProtocol`
  - Removed `supported_models` field from `AliyunProtocol`
  - Removed `supported_models` field from `OllamaProtocol`

#### Rationale
- `supported_models()` returned empty `[]` for most protocols (OpenAI, Anthropic, Aliyun)
- Only Ollama had hardcoded models, which were outdated
- Users should use `fetch_models()` for real-time model discovery
- Simplifies the API by removing confusion between two similar methods

#### Migration Guide

**Before:**
```rust
let client = LlmClient::openai("sk-...");
let models = client.supported_models(); // Returns []
```

**After:**
```rust
let client = LlmClient::openai("sk-...");
let models = client.fetch_models().await?; // Returns actual models from API
```

For protocols that don't support `fetch_models()` (Anthropic, Aliyun, Ollama), you can use any model name directly in your requests.

### üìù Updated

- Updated tests to remove `supported_models()` usage
- Updated examples to demonstrate only `fetch_models()`
- Updated README.md to remove references to `supported_models()`
- Simplified documentation and examples

## [0.2.2] - 2025-01-06

Same as 0.2.1 - version bump for crates.io publication.

## [0.2.1] - 2025-01-06

### ‚ú® Added

#### Online Model Discovery
- **New `fetch_models()` method** for retrieving available models from API
  - Added to `Provider` trait, `LlmClient`, and `GenericProvider`
  - Makes GET request to `/v1/models` endpoint for OpenAI-compatible providers
  - Returns `Vec<String>` of available model IDs
  - Returns `UnsupportedOperation` error for protocols without model listing support

#### HTTP Transport Enhancement
- Added `get()` method to `HttpTransport` for GET requests
- Supports custom headers and authentication

#### Error Handling
- Added `UnsupportedOperation` error variant for unsupported operations
- Returns HTTP 501 status code for unsupported operations

#### Examples
- `examples/test_fetch_models.rs` - Comprehensive test with all providers
- `examples/fetch_models_simple.rs` - Simple comparison example
- `examples/test_with_keys.rs` - Test with keys.yaml configuration

#### Documentation
- `FETCH_MODELS_FEATURE.md` - Complete feature documentation
- `TEST_RESULTS.md` - Test results and verification
- Updated README.md with model discovery section
- Added comparison table for `supported_models()` vs `fetch_models()`

### üîß Changed

#### OpenAI Protocol
- **Removed hardcoded model lists** from `OpenAIProtocol`
- `supported_models()` now returns empty `[]` instead of hardcoded models
- Users can now use **any model name** without restrictions
- Implemented `models_endpoint_url()` to support `/v1/models` endpoint

#### Documentation Cleanup
- Removed references to third-party providers (DeepSeek, Zhipu, Moonshot, etc.) from OpenAI protocol docs
- Updated examples to focus on OpenAI instead of third-party providers
- Simplified documentation to emphasize protocol-first approach

#### Provider Type Aliases
- Removed provider-specific type aliases:
  - `DeepSeekProvider`
  - `ZhipuProvider`
  - `MoonshotProvider`
  - `VolcEngineProvider`
  - `TencentProvider`
  - `MiniMaxProvider`
  - `StepFunProvider`

### üêõ Fixed

#### Configuration
- Fixed `keys.yaml` model names:
  - Removed invalid `qwen3-turbo` model
  - Updated to valid Aliyun models: `qwen-turbo`, `qwen-plus`, `qwen-max`
  - Updated Qwen2 models to Qwen2.5 versions

#### Dependencies
- Added `serde_yaml` to `[dev-dependencies]` for examples
- Fixed `serde_yaml` resolution in test examples

#### Code Quality
- Removed unused imports (`HttpTransport`, `LlmConnectorError` from openai.rs)
- Fixed struct field issues (removed incorrect `transport` field)

### üìä Test Results

#### Successfully Tested Providers (Online Model Fetching)

| Provider | Status | Models Found | Example Models |
|----------|--------|--------------|----------------|
| DeepSeek | ‚úÖ | 2 | `deepseek-chat`, `deepseek-reasoner` |
| Zhipu (GLM) | ‚úÖ | 3 | `glm-4.5`, `glm-4.5-air`, `glm-4.6` |
| Moonshot | ‚úÖ | 12 | `moonshot-v1-32k`, `kimi-latest`, `kimi-thinking-preview` |
| LongCat | ‚ùå | - | `/models` endpoint not available |
| VolcEngine | ‚ùå | - | `/models` endpoint not available |
| Aliyun | ‚ÑπÔ∏è | - | Protocol doesn't support model listing |
| Anthropic | ‚ÑπÔ∏è | - | Protocol doesn't support model listing |

### üìù Migration Guide

#### For Users Relying on Hardcoded Models

**Before (v0.2.0):**
```rust
let client = LlmClient::openai("sk-...");
let models = client.supported_models();
// Returns: ["gpt-4", "gpt-3.5-turbo", "gpt-4-turbo"]
```

**After (v0.2.1):**
```rust
let client = LlmClient::openai("sk-...");

// Option 1: Use any model name directly (recommended)
let request = ChatRequest {
    model: "gpt-4o".to_string(), // Any model name works
    // ...
};

// Option 2: Fetch models online
let models = client.fetch_models().await?;
// Returns: actual models from OpenAI API
```

#### For OpenAI-Compatible Providers

**Before:**
```rust
// Had to check hardcoded list
let models = client.supported_models();
```

**After:**
```rust
// Fetch real-time models from provider
let client = LlmClient::openai_compatible(
    "sk-...",
    "https://api.deepseek.com/v1"
);
let models = client.fetch_models().await?;
// Returns: ["deepseek-chat", "deepseek-reasoner"]
```

### üéØ Benefits

1. **No Model Restrictions**: Use any model name without being limited by hardcoded lists
2. **Always Up-to-Date**: Get the latest models directly from the API
3. **Backward Compatible**: Existing code continues to work
4. **Flexible**: Providers can opt-in to model listing support
5. **Clear Errors**: Explicit error messages when operations aren't supported

### üîó Related Issues

- Fixed errors in `src/protocols/openai.rs`
- Removed hardcoded `supported_models`
- Implemented online model fetching (Option 3)
- Updated documentation to reflect changes

### üìö Documentation

- README.md: Added "Key Features" section
- README.md: Added "Model Discovery" section with comparison table
- README.md: Added "Recent Changes" section
- README.md: Updated error handling examples
- README.md: Updated examples section

### üß™ Testing

All tests passing:
```bash
cargo check --lib                    # ‚úÖ Success
cargo run --example test_openai_only # ‚úÖ All tests passed
cargo run --example test_with_keys   # ‚úÖ 6/6 providers tested
cargo run --example test_fetch_models # ‚úÖ Online fetching works
```

---

## [0.2.0] - Previous Release

Initial release with 4 protocol support and basic functionality.

---

## Future Enhancements

Potential improvements for future releases:

1. **Model Caching**: Cache fetched models to reduce API calls
2. **Model Metadata**: Return full model objects with capabilities, not just IDs
3. **Model Filtering**: Add parameters to filter models by capability
4. **Extended Protocol Support**: Implement model listing for other protocols if available
5. **Pagination Support**: Handle paginated model responses
## [0.3.3] - 2025-10-14

### ‚ú® Added
- README: Added ‚ÄúReasoning Synonyms‚Äù section with normalized keys and usage examples (`reasoning_any()`), covering non-streaming and streaming.

### üîß Changed
- Examples: Removed outdated examples using deprecated `openai_compatible` (`examples/test_fetch_models.rs`, `examples/test_with_keys.rs`).
- Examples: Updated DeepSeek and fetch models example to use `LlmClient::openai(api_key, Some(base_url))`.
- Docs: Fixed doctests across `lib.rs`, `protocols/core.rs`, `protocols/openai.rs`, `protocols/aliyun.rs`, `protocols/anthropic.rs` to match current API.
- Docs: Replaced obsolete imports like `protocols::aliyun::qwen` and `protocols::anthropic::claude` with `LlmClient::aliyun(...)` and `LlmClient::anthropic(...)`.
- Docs: Standardized message initialization to `Message::user(...)` or `Role` enums where appropriate.

### ‚úÖ Validation
- `cargo build --examples` passes.
- `cargo test` (library and integration with `streaming` feature) passes.
- `cargo test --doc` passes (13 passed, 0 failed, 4 ignored).
## 0.3.4 - 2025-10-14

Updates
- Add compatibility alias `types::ChatMessage = Message` to ease migration.
- Add `ChatResponse::get_usage_safe()` returning `(prompt, completion, total)`.
- Add `ChatResponse::get_content()` returning the first choice content as `Option<&str>`.
- README install snippet updated to `0.3.4`.

Notes
- `ChatRequest::new(model)` remains as minimal constructor.
- Use `ChatRequest::new_with_messages(model, messages)` to pass initial message list.
- `Message::user/assistant/system` are preferred constructors; reasoning fields are auto-populated.

## 0.3.5 - 2025-10-14

Updates
- Add `StreamingResponse::get_content()` for convenience and API symmetry with `ChatResponse::get_content()`.

Notes
- No breaking changes; existing code continues to work. You can still access `choices[0].delta.content` directly.

