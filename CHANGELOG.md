# Changelog

All notable changes to this project will be documented in this file.

## [0.5.1] - 2025-01-21

### ğŸ”§ Improvements

#### Code Quality
- Fixed all compilation errors and warnings discovered by rust-analyzer
- Fixed unused variable warnings by using underscore prefix
- Cleaned up 69% of example files (39 â†’ 12)
- Cleaned up 56% of test files (18 â†’ 8)
- Removed 36 duplicate, debug, and outdated files

#### Documentation
- Added `docs/RUST_CODING_RULES.md` - Rust coding standards
- Added `docs/MIGRATION_GUIDE_v0.5.0.md` - Complete migration guide
- Added `docs/RELEASE_v0.5.0.md` - Release notes
- Updated `examples/README.md` with cleaner structure
- Updated all examples to use new API

#### Examples Cleanup
- Removed duplicate examples (test_aliyun_basic.rs, test_deepseek.rs, etc.)
- Removed debug files (debug_aliyun_response.rs, debug_longcat_stream.rs, etc.)
- Removed verification files (verify_aliyun_choices.rs, verify_reasoning_content.rs, etc.)
- Removed shell test scripts (9 files)
- Renamed test_aliyun_enable_thinking.rs â†’ aliyun_thinking.rs

#### Bug Fixes
- Fixed Message construction in all examples
- Fixed content access using content_as_text()
- Fixed streaming examples with proper feature gates
- Fixed tencent_basic.rs API usage
- Fixed all integration tests

### ğŸ“Š Statistics

- **Tests**: 221 passed; 0 failed (100% pass rate)
- **Compilation**: 0 errors, 0 warnings
- **Code reduction**: 74% fewer lines in examples/tests

## [0.5.0] - 2025-01-21

### ğŸ‰ Major Features - Multi-modal Content Support

**âš ï¸ BREAKING CHANGE**: `Message.content` changed from `String` to `Vec<MessageBlock>`

This is a major architectural improvement that enables native multi-modal content support (text + images).

#### New Types

- **`MessageBlock`** - Enum for different content types
  - `Text { text: String }` - Text content
  - `Image { source: ImageSource }` - Image (Anthropic format)
  - `ImageUrl { image_url: ImageUrl }` - Image URL (OpenAI format)
- **`ImageSource`** - Image source (Base64 or URL)
- **`ImageUrl`** - Image URL with optional detail level

#### Migration Guide

```rust
// Old (0.4.x)
let message = Message {
    role: Role::User,
    content: "Hello".to_string(),
    ..Default::default()
};

// New (0.5.0) - Option 1: Use text() constructor (recommended)
let message = Message::text(Role::User, "Hello");

// New (0.5.0) - Option 2: Use new() with MessageBlock
let message = Message::new(
    Role::User,
    vec![MessageBlock::text("Hello")],
);

// New (0.5.0) - Multi-modal example
let message = Message::new(
    Role::User,
    vec![
        MessageBlock::text("What's in this image?"),
        MessageBlock::image_url("https://example.com/image.jpg"),
    ],
);
```

#### New Methods

**Message**:
- `Message::text(role, text)` - Create text-only message
- `Message::new(role, blocks)` - Create multi-modal message
- `Message::content_as_text()` - Extract all text content
- `Message::is_text_only()` - Check if message contains only text
- `Message::has_images()` - Check if message contains images

**MessageBlock**:
- `MessageBlock::text(text)` - Create text block
- `MessageBlock::image_base64(media_type, data)` - Create Base64 image
- `MessageBlock::image_url(url)` - Create image URL block
- `MessageBlock::image_url_with_detail(url, detail)` - Create image URL with detail
- `MessageBlock::as_text()` - Get text content if it's a text block
- `MessageBlock::is_text()` - Check if it's a text block
- `MessageBlock::is_image()` - Check if it's an image block

#### Updated Protocols

- âœ… **OpenAI** - Supports both string and array formats
- âœ… **Anthropic** - Always uses array format
- âœ… **Aliyun** - Converts to text format
- âœ… **Zhipu** - Converts to text format
- âœ… **Ollama** - Converts to text format

#### Examples

- `examples/multimodal_basic.rs` - Comprehensive multi-modal examples

#### Tests

- Added 8 new unit tests for `MessageBlock`
- All 64 tests passing

#### Documentation

- `docs/MULTIMODAL_CONTENT_DESIGN.md` - Design comparison
- `docs/MULTIMODAL_NATIVE_DESIGN.md` - Native design approach
- `docs/MULTIMODAL_MIGRATION_PLAN.md` - Migration plan

---

## [Unreleased]

### Added
- **Moonshot (æœˆä¹‹æš—é¢) Provider**
  - OpenAI-compatible API
  - `LlmClient::moonshot(api_key)`
  - Models: moonshot-v1-8k, moonshot-v1-32k, moonshot-v1-128k
  - Long context support (up to 128k tokens)
  - Full streaming support

- **DeepSeek Provider**
  - OpenAI-compatible API
  - `LlmClient::deepseek(api_key)`
  - Models: deepseek-chat, deepseek-reasoner
  - Reasoning model support with thinking process
  - Automatic extraction of reasoning content
  - Full streaming support for both chat and reasoning models

## [0.4.20] - 2025-10-21

### ğŸ¯ Major Update: Unified Output Format & Configuration-Driven Architecture

#### âœ¨ Unified Output Format

**All providers now output the same unified `StreamingResponse` format**, regardless of their native API format.

```
Different Input Formats â†’ Protocol Conversion â†’ Unified StreamingResponse
```

**Benefits**:
- âœ… Consistent API across all providers
- âœ… Easy provider switching without changing business logic
- âœ… Type-safe compile-time guarantees
- âœ… Lower learning curve - learn once, use everywhere

**Example**:
```rust
// Same code works with ANY provider
let mut stream = client.chat_stream(&request).await?;
while let Some(chunk) = stream.next().await {
    let chunk = chunk?;  // Always StreamingResponse
    if let Some(content) = chunk.get_content() {
        print!("{}", content);
    }
}
```

#### ğŸ—ï¸ Configuration-Driven Architecture

**New Core Modules**:

1. **ProviderBuilder** (`src/core/builder.rs`)
   - Unified builder pattern for all providers
   - Chain-able API: `.timeout()` / `.proxy()` / `.header()`
   - Eliminates repetitive `xxx_with_config` boilerplate
   - Reduces code by ~50%

2. **ConfigurableProtocol** (`src/core/configurable.rs`)
   - Configuration-driven protocol adapter
   - `ProtocolConfig` - Static configuration (name, endpoints, auth)
   - `EndpointConfig` - Endpoint templates with `{base_url}` variable
   - `AuthConfig` - Flexible authentication (Bearer/ApiKeyHeader/None/Custom)
   - New providers only need configuration, not code

**Code Reduction**:
- Tencent: 169 lines â†’ 122 lines (-28%)
- Volcengine: 169 lines â†’ 145 lines (-14%)
- LongCat: 169 lines â†’ 145 lines (-14%)
- **Average: -19% code reduction**

#### ğŸ†• New Providers

1. **Tencent Hunyuan (è…¾è®¯æ··å…ƒ)**
   - OpenAI-compatible API
   - `LlmClient::tencent(api_key)`
   - Models: hunyuan-lite, hunyuan-standard, hunyuan-pro, hunyuan-turbo

2. **LongCat API**
   - Dual format support
   - `LlmClient::longcat_openai(api_key)` - OpenAI format
   - `LlmClient::longcat_anthropic(api_key)` - Anthropic format with Bearer auth

#### ğŸ”§ Anthropic Streaming Fix

**Problem**: LongCat Anthropic streaming failed with "missing field `id`" error

**Solution**: Implemented custom `parse_stream_response` for Anthropic protocol
- Correctly handles Anthropic's multi-event streaming format:
  - `message_start` - Extract message ID
  - `content_block_delta` - Extract text increments
  - `message_delta` - Extract usage and stop_reason
- Converts to unified `StreamingResponse` format
- **Now works perfectly with LongCat Anthropic!**

**Test Results**:
```
âœ… LongCat Anthropic non-streaming: Working
âœ… LongCat Anthropic streaming: Working (fixed!)
   - Total chunks: 20
   - Content chunks: 19
   - finish_reason: end_turn
   - usage: prompt_tokens: 15, completion_tokens: 30
```

#### ğŸ§¹ Code Cleanup

- Removed deprecated v1 architecture code (5641 lines)
- Removed `v1-legacy` feature flag
- Cleaner codebase with focused abstractions

#### ğŸ“š Documentation

**New Documents**:
- `docs/REFACTORING_SUMMARY.md` - Complete refactoring documentation
- `docs/POST_REFACTORING_TEST_REPORT.md` - Comprehensive test report (90% pass rate)
- `docs/ANTHROPIC_STREAMING_FIX.md` - Anthropic streaming fix details

**Updated**:
- README.md - Added unified output format explanation
- README.md - Added new providers (Tencent, LongCat)

#### âœ… Testing

**Comprehensive Testing**:
- âœ… All providers tested: 10/10 tests passed
- âœ… Non-streaming: 100% pass rate (5/5)
- âœ… Streaming: 100% pass rate (5/5)
- âœ… 46 unit tests passing
- âœ… Full backward compatibility verified

**Tested Providers**:
- Tencent (refactored) - âœ… Non-streaming + Streaming
- LongCat OpenAI (unchanged) - âœ… Non-streaming + Streaming
- LongCat Anthropic (refactored) - âœ… Non-streaming + Streaming (fixed!)
- Zhipu (unchanged) - âœ… Non-streaming + Streaming
- Aliyun (unchanged) - âœ… Non-streaming + Streaming

#### ğŸ“ˆ Performance & Metrics

- Code reduction: -19% in refactored providers
- New provider cost: -70% (170 lines â†’ 50 lines)
- Maintenance cost: -50% (centralized logic)
- Test pass rate: 100% (10/10)

#### ğŸ”„ Migration Guide

**No breaking changes!** All existing APIs continue to work.

**Before (still works)**:
```rust
let client = LlmClient::openai_compatible(
    "sk-...",
    "https://api.hunyuan.cloud.tencent.com",
    "tencent"
)?;
```

**After (recommended)**:
```rust
let client = LlmClient::tencent("sk-...")?;
```

---

## [0.4.19] - 2025-10-18

### âœ¨ New Features

#### **æ·»åŠ ç«å±±å¼•æ“ï¼ˆVolcengineï¼‰ä¸“ç”¨ Provider**

**ç«å±±å¼•æ“ç®€ä»‹**:
- ç«å±±å¼•æ“æ˜¯å­—èŠ‚è·³åŠ¨æ——ä¸‹çš„äº‘æœåŠ¡å¹³å°
- æä¾›å¤§æ¨¡å‹æœåŠ¡ï¼ˆç«å±±æ–¹èˆŸï¼‰
- ä½¿ç”¨ OpenAI å…¼å®¹çš„ API æ ¼å¼ï¼Œä½†ç«¯ç‚¹è·¯å¾„ä¸åŒ

**æ–°å¢åŠŸèƒ½**:

1. **åˆ›å»º VolcengineProtocol é€‚é…å™¨**
   - åŒ…è£… OpenAI protocolï¼Œä½†ä½¿ç”¨ç«å±±å¼•æ“çš„ç«¯ç‚¹è·¯å¾„
   - ç«¯ç‚¹: `/api/v3/chat/completions` (è€Œä¸æ˜¯ `/v1/chat/completions`)
   - å®Œå…¨å…¼å®¹ OpenAI è¯·æ±‚/å“åº”æ ¼å¼

2. **æ·»åŠ ä¸“ç”¨ API æ–¹æ³•**
   - `LlmClient::volcengine()` - åˆ›å»ºç«å±±å¼•æ“å®¢æˆ·ç«¯
   - `LlmClient::volcengine_with_config()` - å¸¦è‡ªå®šä¹‰é…ç½®çš„å®¢æˆ·ç«¯

3. **æ”¯æŒæ¨ç†æ¨¡å‹ç‰¹æ€§**
   - æ”¯æŒ `reasoning_content` å­—æ®µï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
   - æµå¼å“åº”ä¸­å…ˆè¿”å›æ€è€ƒè¿‡ç¨‹ï¼Œå†è¿”å›å®é™…å›ç­”
   - ç±»ä¼¼ OpenAI o1 çš„æ¨ç†æ¨¡å‹

**ä½¿ç”¨ç¤ºä¾‹**:

```rust
// åˆ›å»ºå®¢æˆ·ç«¯
let client = LlmClient::volcengine("26f962bd-450e-4876-bc32-a732e6da9cd2")?;

// åˆ›å»ºè¯·æ±‚ï¼ˆä½¿ç”¨ç«¯ç‚¹ IDï¼‰
let request = ChatRequest {
    model: "ep-20251006132256-vrq2p".to_string(),  // ç«¯ç‚¹ ID
    messages: vec![Message {
        role: Role::User,
        content: "ä½ å¥½".to_string(),
        ..Default::default()
    }],
    max_tokens: Some(1000),
    ..Default::default()
};

// éæµå¼
let response = client.chat(&request).await?;

// æµå¼
#[cfg(feature = "streaming")]
{
    let mut stream = client.chat_stream(&request).await?;
    while let Some(chunk) = stream.next().await {
        // å¤„ç†æµå¼å“åº”
    }
}
```

**æµ‹è¯•ç»“æœ**:

| åŠŸèƒ½ | çŠ¶æ€ | è¯¦æƒ… |
|------|------|------|
| éæµå¼å“åº” | âœ… | å®Œå…¨å¯ç”¨ |
| æµå¼å“åº” | âœ… | å®Œå…¨å¯ç”¨ |
| reasoning_content | âœ… | æ”¯æŒæ¨ç†è¿‡ç¨‹ |
| llm-connector å…¼å®¹æ€§ | âœ… | å®Œå…¨å…¼å®¹ |

**æ–°å¢æ–‡ä»¶**:
- `src/providers/volcengine.rs` - ç«å±±å¼•æ“ä¸“ç”¨ Provider
- `examples/test_volcengine.rs` - æµ‹è¯•ç¤ºä¾‹
- `tests/test_volcengine_raw.sh` - åŸå§‹ API æµ‹è¯•
- `tests/test_volcengine_streaming_raw.sh` - æµå¼å“åº”æµ‹è¯•
- `docs/VOLCENGINE_GUIDE.md` - å®Œæ•´ä½¿ç”¨æŒ‡å—

**é‡è¦è¯´æ˜**:
- ç«å±±å¼•æ“ä½¿ç”¨ç«¯ç‚¹ IDï¼ˆ`ep-xxxxxx`ï¼‰è€Œä¸æ˜¯æ¨¡å‹åç§°
- ç«¯ç‚¹ ID éœ€è¦åœ¨ç«å±±å¼•æ“æ§åˆ¶å°åˆ›å»ºå’Œè·å–
- API Key æ ¼å¼ä¸º UUID è€Œä¸æ˜¯ `sk-` æ ¼å¼

---

## [0.4.18] - 2025-10-18

### âœ¨ New Features

#### **æ·»åŠ  LongCat API æ”¯æŒ**

**LongCat ç®€ä»‹**:
- LongCat æ˜¯ä¸€ä¸ª AI æœåŠ¡å¹³å°ï¼Œæä¾›é«˜æ€§èƒ½çš„å¯¹è¯æ¨¡å‹
- æ”¯æŒ OpenAI å’Œ Anthropic ä¸¤ç§ API æ ¼å¼
- æ¯æ—¥å…è´¹é¢åº¦: 500,000 Tokens

**æ–°å¢åŠŸèƒ½**:

1. **LongCat OpenAI æ ¼å¼æ”¯æŒ** - âœ… å®Œå…¨å¯ç”¨
   - ä½¿ç”¨ `LlmClient::openai_compatible()` æ–¹æ³•
   - ç«¯ç‚¹: `https://api.longcat.chat/openai`
   - æ”¯æŒéæµå¼å’Œæµå¼å“åº”
   - å®Œå…¨å…¼å®¹ llm-connector

2. **LongCat Anthropic æ ¼å¼æ”¯æŒ** - âœ… éæµå¼å¯ç”¨
   - åˆ›å»º `LongCatAnthropicProtocol` é€‚é…å™¨
   - ä½¿ç”¨ `Authorization: Bearer` è®¤è¯ï¼ˆè€Œä¸æ˜¯æ ‡å‡† Anthropic çš„ `x-api-key`ï¼‰
   - æ·»åŠ  `LlmClient::longcat_anthropic()` æ–¹æ³•
   - æ·»åŠ  `LlmClient::longcat_anthropic_with_config()` æ–¹æ³•
   - æ”¯æŒéæµå¼å“åº”
   - âš ï¸ æµå¼å“åº”æš‚ä¸æ”¯æŒï¼ˆAnthropic äº‹ä»¶æ ¼å¼éœ€è¦ä¸“é—¨è§£æå™¨ï¼‰

**ä½¿ç”¨ç¤ºä¾‹**:

```rust
// æ–¹å¼ 1: OpenAI æ ¼å¼ï¼ˆæ¨èï¼Œæµå¼å’Œéæµå¼éƒ½å¯ç”¨ï¼‰
let client = LlmClient::openai_compatible(
    "ak_...",
    "https://api.longcat.chat/openai",
    "longcat"
)?;

// æ–¹å¼ 2: Anthropic æ ¼å¼ï¼ˆä»…éæµå¼ï¼‰
let client = LlmClient::longcat_anthropic("ak_...")?;
```

**æµ‹è¯•ç»“æœ**:

| æµ‹è¯•é¡¹ | OpenAI æ ¼å¼ | Anthropic æ ¼å¼ |
|--------|------------|---------------|
| éæµå¼å“åº” | âœ… æˆåŠŸ | âœ… æˆåŠŸ |
| æµå¼å“åº” | âœ… æˆåŠŸ | âš ï¸ æš‚ä¸æ”¯æŒ |
| llm-connector å…¼å®¹æ€§ | âœ… å®Œå…¨å…¼å®¹ | âœ… éæµå¼å…¼å®¹ |

**æ–°å¢æ–‡ä»¶**:
- `src/providers/longcat.rs` - LongCat Anthropic é€‚é…å™¨
- `examples/test_longcat_openai.rs` - OpenAI æ ¼å¼æµ‹è¯•
- `examples/test_longcat_anthropic.rs` - Anthropic æ ¼å¼æµ‹è¯•
- `tests/test_longcat_anthropic_raw.sh` - Anthropic åŸå§‹ API æµ‹è¯•
- `tests/test_longcat_anthropic_streaming_raw.sh` - æµå¼å“åº”æ ¼å¼æµ‹è¯•
- `docs/LONGCAT_TESTING_REPORT.md` - å®Œæ•´æµ‹è¯•æŠ¥å‘Š

**æ¨èä½¿ç”¨æ–¹å¼**:
- æµå¼: `LlmClient::openai_compatible("ak_...", "https://api.longcat.chat/openai", "longcat")`
- éæµå¼: `LlmClient::longcat_anthropic("ak_...")` æˆ– OpenAI æ ¼å¼

### ğŸ› Bug Fixes

#### **ä¿®å¤ AliyunProviderImpl ç¼ºå¤±æ–¹æ³•**

**é—®é¢˜**: æµ‹è¯•ä»£ç è°ƒç”¨ `provider.protocol()` å’Œ `provider.client()` æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•ä¸å­˜åœ¨

**ä¿®å¤**:
- æ·»åŠ  `protocol()` æ–¹æ³•è¿”å›åè®®å®ä¾‹å¼•ç”¨
- æ·»åŠ  `client()` æ–¹æ³•è¿”å› HTTP å®¢æˆ·ç«¯å¼•ç”¨
- ä¿®å¤ `models()` é”™è¯¯ä¿¡æ¯ä»¥åŒ¹é…æµ‹è¯•æœŸæœ›
- ä¿®å¤ `as_ollama()` doctest ä¸­ä¸å­˜åœ¨çš„æ–¹æ³•è°ƒç”¨

### ğŸ“ Documentation

- æ·»åŠ  `docs/LONGCAT_TESTING_REPORT.md` - LongCat API å®Œæ•´æµ‹è¯•æŠ¥å‘Š
- æ›´æ–° `src/client.rs` - æ·»åŠ  LongCat ä½¿ç”¨ç¤ºä¾‹

---

## [0.4.17] - 2025-10-18

### ğŸ› Bug Fixes

#### **ä¿®å¤ Aliyun å“åº”è§£æå’Œæµå¼å“åº”é—®é¢˜**

**é—®é¢˜ 1: ChatResponse ç»“æ„ä¸ä¸€è‡´**

**é—®é¢˜æè¿°**:
- âŒ Aliyun çš„ `choices` æ•°ç»„ä¸ºç©º
- âŒ `content` å­—æ®µæœ‰æ•°æ®ï¼Œä½†ä¸æ˜¯ä» `choices[0]` æå–çš„
- âŒ ç¼ºå°‘ `usage` ä¿¡æ¯
- âŒ ä¸ OpenAI å®ç°ä¸ä¸€è‡´ï¼Œè¿åè®¾è®¡æ„å›¾

**æ ¹æœ¬åŸå› **:
- ä½¿ç”¨ `..Default::default()` å¯¼è‡´ `choices` ä¸ºç©ºæ•°ç»„
- ç›´æ¥è®¾ç½® `content` å­—æ®µï¼Œè€Œä¸æ˜¯ä» `choices[0].message.content` æå–
- æ²¡æœ‰æå– `usage` å’Œ `finish_reason` ä¿¡æ¯

**ä¿®å¤å†…å®¹**:

1. **æ›´æ–°å“åº”æ•°æ®ç»“æ„** (`src/providers/aliyun.rs`)
   - æ·»åŠ  `AliyunUsage` ç»“æ„ä½“
   - æ·»åŠ  `usage` å’Œ `request_id` å­—æ®µåˆ° `AliyunResponse`
   - æ·»åŠ  `finish_reason` å­—æ®µåˆ° `AliyunChoice`

2. **ä¿®å¤ parse_response æ–¹æ³•**
   - æ„å»ºå®Œæ•´çš„ `choices` æ•°ç»„ï¼ŒåŒ…å« `Choice` å¯¹è±¡
   - ä» `choices[0].message.content` æå– `content` ä½œä¸ºä¾¿åˆ©å­—æ®µ
   - æå– `usage` ä¿¡æ¯ï¼ˆ`input_tokens`, `output_tokens`, `total_tokens`ï¼‰
   - æå– `request_id` åˆ° `response.id`
   - æå– `finish_reason`

**é—®é¢˜ 2: æµå¼å“åº”æ— æ³•å·¥ä½œ**

**é—®é¢˜æè¿°**:
- âŒ æµå¼è¯·æ±‚æ²¡æœ‰æ”¶åˆ°ä»»ä½•å†…å®¹ chunks
- âŒ åªæ”¶åˆ°æœ€åä¸€ä¸ªç©ºçš„ final chunk
- âŒ æµå¼åŠŸèƒ½å®Œå…¨æ— æ³•ä½¿ç”¨

**æ ¹æœ¬åŸå› **:
- ç¼ºå°‘ `X-DashScope-SSE: enable` å¤´éƒ¨
- ç¼ºå°‘ `incremental_output: true` å‚æ•°
- ä½¿ç”¨é»˜è®¤çš„ SSE è§£æï¼Œæ— æ³•æ­£ç¡®å¤„ç† Aliyun çš„ç‰¹æ®Šæ ¼å¼

**ä¿®å¤å†…å®¹**:

1. **æ·»åŠ æµå¼å‚æ•°**
   - æ·»åŠ  `incremental_output` å­—æ®µåˆ° `AliyunParameters`
   - åœ¨ `build_request` ä¸­æ ¹æ® `stream` å‚æ•°è®¾ç½® `incremental_output`

2. **åˆ›å»ºè‡ªå®šä¹‰ Provider å®ç°**
   - åˆ›å»º `AliyunProviderImpl` ç»“æ„ä½“
   - å®ç° `Provider` traitï¼ŒåŒ…å« `chat`, `chat_stream`, `models` æ–¹æ³•
   - åœ¨ `chat_stream` ä¸­æ·»åŠ  `X-DashScope-SSE: enable` å¤´éƒ¨

3. **å®ç°è‡ªå®šä¹‰æµå¼è§£æ**
   - å®ç° `parse_stream_response` æ–¹æ³•
   - è§£æ Aliyun SSE æ ¼å¼ï¼ˆ`id:`, `event:`, `data:` è¡Œï¼‰
   - å¤„ç† `finish_reason: "null"` (å­—ç¬¦ä¸²) vs `"stop"`
   - è½¬æ¢ä¸º `StreamingResponse` æ ¼å¼

**éªŒè¯ç»“æœ**:

éæµå¼å“åº”:
- âœ… `choices` æ•°ç»„é•¿åº¦: 1
- âœ… `choices[0].message.content == content`
- âœ… åŒ…å« `usage` ä¿¡æ¯
- âœ… åŒ…å« `finish_reason`
- âœ… ç¬¦åˆ OpenAI æ ‡å‡†æ ¼å¼

æµå¼å“åº”:
- âœ… æ€»æµå¼å—æ•°: 10
- âœ… åŒ…å«å†…å®¹çš„å—æ•°: 9
- âœ… å®Œæ•´å†…å®¹æ­£å¸¸æ¥æ”¶
- âœ… æµå¼å“åº”æ­£å¸¸å·¥ä½œ

**å½±å“èŒƒå›´**:
- âœ… å®Œå…¨å‘åå…¼å®¹ï¼ˆ`content` å­—æ®µç»§ç»­å·¥ä½œï¼‰
- âœ… å¢å¼ºåŠŸèƒ½ï¼ˆç°åœ¨å¯ä»¥è®¿é—® `choices` æ•°ç»„å’Œ `usage` ä¿¡æ¯ï¼‰
- âœ… ä¿®å¤æµå¼å“åº”ï¼ˆä»å®Œå…¨ä¸å·¥ä½œåˆ°æ­£å¸¸å·¥ä½œï¼‰

### ğŸ§ª Testing

**æ–°å¢æµ‹è¯•**:
1. `examples/test_aliyun_streaming.rs` - æµå¼å“åº”æµ‹è¯•
2. `examples/verify_aliyun_choices.rs` - choices æ•°ç»„éªŒè¯
3. `tests/test_aliyun_streaming_format.sh` - API åŸå§‹å“åº”æµ‹è¯•

### ğŸ“ Documentation

- æ·»åŠ  `docs/ALIYUN_FIXES_SUMMARY.md` - Aliyun ä¿®å¤æ€»ç»“
- æ·»åŠ  `docs/CHATRESPONSE_DESIGN_ANALYSIS.md` - ChatResponse è®¾è®¡åˆ†æ
- æ·»åŠ  `docs/ALIYUN_RESPONSE_VERIFICATION.md` - Aliyun å“åº”éªŒè¯æŠ¥å‘Š

---

## [0.4.16] - 2025-10-18

### ğŸ› Bug Fixes

#### **ä¿®å¤é‡å¤ Content-Type å¤´éƒ¨å¯¼è‡´ Aliyun ç­‰ Provider æ— æ³•ä½¿ç”¨**

**é—®é¢˜æè¿°**:
- âŒ Aliyun Provider å®Œå…¨æ— æ³•ä½¿ç”¨
- âŒ é”™è¯¯ä¿¡æ¯: `Content-Type/Accept application/json,application/json is not supported`
- âŒ åŸå› : `auth_headers()` å’Œ `HttpClient::post().json()` éƒ½è®¾ç½®äº† `Content-Type`
- âŒ å¯¼è‡´ HTTP å¤´éƒ¨é‡å¤: `Content-Type: application/json, application/json`

**æ ¹æœ¬åŸå› **:
1. `Protocol::auth_headers()` è¿”å› `Content-Type: application/json`
2. `HttpClient::post()` ä½¿ç”¨ `.json(body)` ä¹Ÿä¼šè‡ªåŠ¨è®¾ç½® `Content-Type: application/json`
3. ä¸¤ä¸ªå¤´éƒ¨å€¼è¢«åˆå¹¶ï¼Œå¯¼è‡´é‡å¤
4. é˜¿é‡Œäº‘ APIï¼ˆä»¥åŠå…¶ä»–ä¸¥æ ¼çš„ APIï¼‰ä¸æ¥å—é‡å¤çš„å¤´éƒ¨å€¼

**ä¿®å¤å†…å®¹**:

1. **Aliyun Provider** (`src/providers/aliyun.rs`)
   - ä» `auth_headers()` ä¸­ç§»é™¤ `Content-Type` è®¾ç½®
   - æ·»åŠ æ³¨é‡Šè¯´æ˜ `.json()` å·²è‡ªåŠ¨è®¾ç½®

2. **Zhipu Provider** (`src/providers/zhipu.rs`)
   - ä» `auth_headers()` ä¸­ç§»é™¤ `Content-Type` è®¾ç½®
   - é¿å…æ½œåœ¨çš„é‡å¤å¤´éƒ¨é—®é¢˜

3. **Anthropic Provider** (`src/providers/anthropic.rs`)
   - Vertex AI: ç§»é™¤ `.with_header("Content-Type", ...)`
   - Bedrock: ç§»é™¤ `.with_header("Content-Type", ...)`

4. **Ollama Provider** (`src/providers/ollama.rs`)
   - `new()`: ç§»é™¤ `.with_header("Content-Type", ...)`
   - `with_config()`: ç§»é™¤ `.with_header("Content-Type", ...)`

5. **OpenAI Provider** (`src/providers/openai.rs`)
   - Azure OpenAI: ç§»é™¤ `.with_header("Content-Type", ...)`
   - OpenAI Compatible: ç§»é™¤ `.with_header("Content-Type", ...)`

**å½±å“çš„ Provider**:
- âœ… **Aliyun (DashScope)** - ä¿®å¤æ— æ³•ä½¿ç”¨çš„é—®é¢˜
- âœ… **Zhipu (GLM)** - ä¿®å¤æ½œåœ¨é—®é¢˜
- âœ… **Anthropic (Vertex AI, Bedrock)** - ä¿®å¤æ½œåœ¨é—®é¢˜
- âœ… **Ollama** - ä¿®å¤æ½œåœ¨é—®é¢˜
- âœ… **OpenAI (Azure, Compatible)** - ä¿®å¤æ½œåœ¨é—®é¢˜

**æµ‹è¯•éªŒè¯**:
- âœ… ç¼–è¯‘æˆåŠŸ
- âœ… æ·»åŠ  `examples/test_aliyun_basic.rs` éªŒè¯ä¿®å¤
- âœ… æ‰€æœ‰ Provider ä¸å†é‡å¤è®¾ç½® Content-Type

**ä¿®å¤ç»Ÿè®¡**:
- ä¿®å¤çš„æ–‡ä»¶: 5 ä¸ª
- ä¿®å¤çš„ Provider: 6 ä¸ª
- åˆ é™¤çš„é‡å¤è®¾ç½®: 9 å¤„
- æ·»åŠ çš„æ³¨é‡Š: 9 å¤„

**å½±å“èŒƒå›´**:
- âœ… ä¿®å¤ Aliyun Provider å®Œå…¨æ— æ³•ä½¿ç”¨çš„ä¸¥é‡é—®é¢˜
- âœ… ä¿®å¤å…¶ä»– Provider çš„æ½œåœ¨å…¼å®¹æ€§é—®é¢˜
- âœ… æå‡ HTTP å¤´éƒ¨è®¾ç½®çš„è§„èŒƒæ€§
- âœ… å®Œå…¨å‘åå…¼å®¹ï¼Œæ— éœ€ç”¨æˆ·ä¿®æ”¹ä»£ç 

### ğŸ§ª Testing

#### **æ·»åŠ æ™ºè°±æµå¼ tool_calls éªŒè¯æµ‹è¯•**

**æ–°å¢æµ‹è¯•**:
1. `tests/test_zhipu_streaming_direct.sh` - ç›´æ¥æµ‹è¯•æ™ºè°± API åŸå§‹å“åº”
2. `examples/test_zhipu_flash_streaming_tool_calls.rs` - æµ‹è¯• llm-connector è§£æ
3. `examples/debug_zhipu_streaming_tool_calls.rs` - è¯¦ç»†è°ƒè¯•ç¤ºä¾‹

**éªŒè¯ç»“æœ**:
- âœ… æ™ºè°± API åœ¨æµå¼æ¨¡å¼ä¸‹è¿”å› tool_calls
- âœ… llm-connector å¯ä»¥æ­£ç¡®è§£æ tool_calls
- âœ… è¯æ˜ llm-connector 0.4.15 æ²¡æœ‰ bugï¼ŒåŠŸèƒ½æ­£å¸¸

### ğŸ“ Documentation

- æ·»åŠ  `docs/FIX_DUPLICATE_CONTENT_TYPE_HEADER.md` - é‡å¤å¤´éƒ¨é—®é¢˜ä¿®å¤æ–‡æ¡£
- æ·»åŠ  `docs/ZHIPU_STREAMING_TOOL_CALLS_VERIFICATION.md` - æ™ºè°±æµå¼éªŒè¯æŠ¥å‘Š

---

## [0.4.15] - 2025-10-18

### ğŸ› Bug Fixes

#### **ä¿®å¤ç¤ºä¾‹ä»£ç ç¼–è¯‘é”™è¯¯å’Œè­¦å‘Š**

**ä¿®å¤å†…å®¹**:
1. **ç§»é™¤ä¸å­˜åœ¨çš„æ–¹æ³•è°ƒç”¨** (`examples/test_openai_tool_streaming.rs`)
   - ç§»é™¤äº†å¯¹ä¸å­˜åœ¨çš„ `LlmClient::openrouter()` æ–¹æ³•çš„è°ƒç”¨
   - æ”¹ä¸ºä½¿ç”¨ `LlmClient::openai()`

2. **ä¿®å¤ç±»å‹é”™è¯¯** (`examples/test_openai_tool_streaming.rs`)
   - ä¿®å¤ tool_calls å¼•ç”¨ç±»å‹é—®é¢˜
   - å°† `&tool_calls_buffer[0]` æ”¹ä¸º `tool_calls_buffer[0].clone()`

3. **æ¶ˆé™¤æœªä½¿ç”¨å¯¼å…¥è­¦å‘Š** (7 ä¸ªç¤ºä¾‹æ–‡ä»¶)
   - å°† streaming ç›¸å…³çš„å¯¼å…¥ç§»åˆ° `#[cfg(feature = "streaming")]` å†…
   - é¿å…åœ¨é streaming æ¨¡å¼ä¸‹äº§ç”Ÿæœªä½¿ç”¨å¯¼å…¥è­¦å‘Š
   - å½±å“æ–‡ä»¶ï¼š
     - `test_zhipu_tool_messages_detailed.rs`
     - `test_deepseek_tools.rs`
     - `test_openai_tool_streaming.rs`
     - `test_zhipu_tool_streaming_issue.rs`
     - `test_glm_models_tool_streaming.rs`
     - `zhipu_tools_streaming.rs`
     - `test_all_providers_tool_streaming.rs`

4. **æ¶ˆé™¤æœªä½¿ç”¨å­—æ®µè­¦å‘Š** (`examples/test_all_providers_tool_streaming.rs`)
   - æ·»åŠ  `#[allow(dead_code)]` åˆ° `TestResult` ç»“æ„ä½“

5. **ä¿®å¤ clippy è­¦å‘Š**
   - ä¿®å¤ doc comments ç©ºè¡Œè­¦å‘Š
   - ä¿®å¤é•¿åº¦æ¯”è¾ƒè­¦å‘Šï¼ˆ`len() > 0` â†’ `!is_empty()`ï¼‰

### ğŸ“ Documentation

- æ·»åŠ  `docs/EXAMPLES_AND_TESTS_REVIEW.md` - Examples å’Œ Tests å®¡æŸ¥æŠ¥å‘Š
- æ·»åŠ  `docs/RELEASE_v0.4.14.md` - v0.4.14 å‘å¸ƒæ€»ç»“

**æµ‹è¯•éªŒè¯**:
- âœ… æ‰€æœ‰ç¤ºä¾‹éƒ½èƒ½æ­£å¸¸ç¼–è¯‘
- âœ… æ‰€æœ‰æµ‹è¯•éƒ½èƒ½é€šè¿‡
- âœ… æ— ç¼–è¯‘é”™è¯¯
- âœ… å¤§å¹…å‡å°‘ç¼–è¯‘è­¦å‘Š

**å½±å“èŒƒå›´**:
- ä¿®å¤ç¤ºä¾‹ä»£ç çš„ç¼–è¯‘é—®é¢˜
- æå‡ä»£ç è´¨é‡
- å®Œå…¨å‘åå…¼å®¹

---

## [0.4.14] - 2025-10-18

### ğŸ› Bug Fixes

#### **ä¿®å¤ OpenAI åè®®å·¥å…·è°ƒç”¨æ”¯æŒ + ç§»é™¤æ™ºè°± GLM æµå¼å¼ºåˆ¶åˆ‡æ¢**

**é—®é¢˜ 1: OpenAI åè®®ç¼ºå°‘å·¥å…·è°ƒç”¨æ”¯æŒ**

**é—®é¢˜æè¿°**:
- âŒ `OpenAIRequest` ç¼ºå°‘ `tools` å’Œ `tool_choice` å­—æ®µï¼Œæ— æ³•ä¼ é€’å·¥å…·å®šä¹‰
- âŒ `OpenAIMessage` ç¼ºå°‘ `tool_calls`, `tool_call_id`, `name` å­—æ®µ
- âŒ `OpenAIResponseMessage` ç¼ºå°‘ `tool_calls` å­—æ®µï¼Œæ— æ³•è§£æå·¥å…·è°ƒç”¨å“åº”
- âŒ å¯¼è‡´æ‰€æœ‰ä½¿ç”¨ OpenAI åè®®çš„æœåŠ¡ï¼ˆDeepSeek, Moonshot ç­‰ï¼‰å®Œå…¨æ— æ³•ä½¿ç”¨å·¥å…·è°ƒç”¨

**ä¿®å¤å†…å®¹**:
1. **OpenAIRequest æ·»åŠ å·¥å…·å­—æ®µ** (`src/protocols/openai.rs`)
   ```rust
   pub struct OpenAIRequest {
       // ... å…¶ä»–å­—æ®µ
       pub tools: Option<Vec<serde_json::Value>>,      // âœ… æ–°å¢
       pub tool_choice: Option<serde_json::Value>,     // âœ… æ–°å¢
   }
   ```

2. **OpenAIMessage æ·»åŠ å·¥å…·å­—æ®µ** (`src/protocols/openai.rs`)
   ```rust
   pub struct OpenAIMessage {
       pub role: String,
       pub content: String,
       pub tool_calls: Option<Vec<serde_json::Value>>,  // âœ… æ–°å¢
       pub tool_call_id: Option<String>,                // âœ… æ–°å¢
       pub name: Option<String>,                        // âœ… æ–°å¢
   }
   ```

3. **OpenAIResponseMessage æ·»åŠ å·¥å…·å­—æ®µ** (`src/protocols/openai.rs`)
   ```rust
   pub struct OpenAIResponseMessage {
       pub content: Option<String>,                     // âœ… æ”¹ä¸º Option
       pub tool_calls: Option<Vec<serde_json::Value>>,  // âœ… æ–°å¢
   }
   ```

4. **build_request å®Œæ•´æ˜ å°„å·¥å…·è°ƒç”¨** (`src/protocols/openai.rs:48-106`)
   - æ­£ç¡®æ˜ å°„ `tools` å­—æ®µ
   - æ­£ç¡®æ˜ å°„ `tool_choice` å­—æ®µ
   - æ­£ç¡®æ˜ å°„æ¶ˆæ¯ä¸­çš„ `tool_calls`, `tool_call_id`, `name` å­—æ®µ

5. **parse_response æ­£ç¡®è§£æå·¥å…·è°ƒç”¨** (`src/protocols/openai.rs:116-149`)
   - ä»å“åº”ä¸­æå– `tool_calls`
   - è½¬æ¢ä¸ºç»Ÿä¸€çš„ `ToolCall` ç±»å‹

**é—®é¢˜ 2: æ™ºè°± GLM æµå¼å“åº”è¢«å¼ºåˆ¶åˆ‡æ¢**

**é—®é¢˜æè¿°**:
- âŒ `src/core/traits.rs` ä¸­å­˜åœ¨ç¡¬ç¼–ç é€»è¾‘ï¼Œæ£€æµ‹åˆ° `Role::Tool` æ¶ˆæ¯æ—¶å¼ºåˆ¶åˆ‡æ¢ä¸ºéæµå¼
- âŒ GLM-4.5 æ­£å¸¸å¯è¿”å› 91 ä¸ªæµå¼å—ï¼Œä½†åŒ…å«å·¥å…·ç»“æœæ—¶è¢«å¼ºåˆ¶åˆ‡æ¢ä¸º 1 ä¸ªå—
- âŒ è¿™æ˜¯ä¸€ä¸ªä¸´æ—¶ä¿®å¤ï¼ˆworkaroundï¼‰ï¼Œç°åœ¨å·²ä¸å†éœ€è¦

**ä¿®å¤å†…å®¹**:
- **ç§»é™¤ç¡¬ç¼–ç ä¿®å¤é€»è¾‘** (`src/core/traits.rs:155-173`)
  - åˆ é™¤äº†æ£€æµ‹ `Role::Tool` å’Œ `zhipu` çš„ç‰¹æ®Šå¤„ç†
  - æ™ºè°± GLM ç°åœ¨å¯ä»¥åœ¨åŒ…å«å·¥å…·è°ƒç”¨ç»“æœæ—¶æ­£å¸¸ä½¿ç”¨æµå¼å“åº”

**æµ‹è¯•éªŒè¯**:
- âœ… OpenAI åè®®å®Œæ•´æ”¯æŒå·¥å…·è°ƒç”¨ï¼ˆtools, tool_choice, tool_callsï¼‰
- âœ… DeepSeek ç°åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨å·¥å…·è°ƒç”¨
- âœ… æ‰€æœ‰ OpenAI å…¼å®¹æœåŠ¡ï¼ˆMoonshot, Together AI ç­‰ï¼‰éƒ½å¯ä»¥ä½¿ç”¨å·¥å…·è°ƒç”¨
- âœ… æ™ºè°± GLM åœ¨åŒ…å« Role::Tool æ—¶å¯ä»¥ä½¿ç”¨æµå¼å“åº”
- âœ… æ‰€æœ‰æ ¸å¿ƒåº“æµ‹è¯•é€šè¿‡ï¼ˆ27 ä¸ªæµ‹è¯•ï¼‰

**æ–°å¢ç¤ºä¾‹**:
- `examples/verify_tool_fix.rs` - éªŒè¯å·¥å…·è°ƒç”¨ä¿®å¤æ•ˆæœ

**å½±å“èŒƒå›´**:
- ä¿®å¤æ‰€æœ‰ä½¿ç”¨ OpenAI åè®®çš„æœåŠ¡çš„å·¥å…·è°ƒç”¨åŠŸèƒ½
- ç§»é™¤æ™ºè°± GLM çš„æµå¼å“åº”é™åˆ¶
- å®Œå…¨å‘åå…¼å®¹

---

## [0.4.13] - 2025-10-18

### ğŸ› Bug Fixes

#### **ä¿®å¤æ™ºè°± GLM å¤šè½®å·¥å…·è°ƒç”¨æ”¯æŒ**

**é—®é¢˜æè¿°**:
- âŒ `ZhipuMessage` ç¼ºå°‘ `tool_call_id` å­—æ®µï¼Œæ— æ³•åœ¨ Tool æ¶ˆæ¯ä¸­å…³è”å·¥å…·è°ƒç”¨
- âŒ `ZhipuMessage` ç¼ºå°‘ `name` å­—æ®µï¼Œæ— æ³•ä¼ é€’å·¥å…·åç§°
- âŒ å¯¼è‡´å¤šè½® Function Calling å¯¹è¯å¤±è´¥ï¼ˆç¬¬äºŒè½®æ— æ³•æ­£ç¡®ä¼ é€’å·¥å…·æ‰§è¡Œç»“æœï¼‰

**ä¿®å¤å†…å®¹**:
1. **ZhipuMessage ç»“æ„å®Œå–„** (`src/providers/zhipu.rs:272-282`)
   ```rust
   pub struct ZhipuMessage {
       pub role: String,
       pub content: String,
       pub tool_calls: Option<Vec<serde_json::Value>>,
       pub tool_call_id: Option<String>,  // âœ… æ–°å¢
       pub name: Option<String>,          // âœ… æ–°å¢
   }
   ```

2. **build_request æ˜ å°„è¡¥å……** (`src/providers/zhipu.rs:77-96`)
   - æ­£ç¡®æ˜ å°„ `tool_call_id` å­—æ®µ
   - æ­£ç¡®æ˜ å°„ `name` å­—æ®µ

**æµ‹è¯•éªŒè¯**:
- âœ… å•è½®å·¥å…·è°ƒç”¨ï¼šUser æé—® â†’ LLM è¿”å› tool_calls
- âœ… å¤šè½®å·¥å…·è°ƒç”¨ï¼šTool ç»“æœ â†’ LLM è¿”å›æ–‡æœ¬å“åº”
- âœ… ä¸‰è½®å¯¹è¯ï¼šUser è¿½é—® â†’ LLM æ­£ç¡®è§¦å‘æ–°çš„ tool_calls
- âœ… Tool æ¶ˆæ¯åºåˆ—åŒ–ï¼š`role="tool"`, `tool_call_id`, `name` å…¨éƒ¨æ­£ç¡®

**æ–°å¢ç¤ºä¾‹**:
- `examples/zhipu_multiround_tools.rs` - å¤šè½®å·¥å…·è°ƒç”¨æ¼”ç¤º
- `examples/zhipu_tools_edge_cases.rs` - è¾¹ç¼˜æƒ…å†µæµ‹è¯•
- `examples/verify_tool_message_serialization.rs` - åºåˆ—åŒ–éªŒè¯

**å½±å“èŒƒå›´**:
- ä¿®å¤æ™ºè°± GLM çš„å¤šè½®å·¥å…·è°ƒç”¨åŠŸèƒ½
- å®Œå…¨ç¬¦åˆ OpenAI Function Calling è§„èŒƒ
- å®Œå…¨å‘åå…¼å®¹

---

## [0.4.12] - 2025-10-18

### ğŸ› Bug Fixes

#### **ä¿®å¤æ™ºè°± GLM æµå¼å“åº”å’Œå·¥å…·è°ƒç”¨æ”¯æŒ**

**æµå¼å“åº”é—®é¢˜**:
- âŒ é—®é¢˜ï¼šæ™ºè°± API ä½¿ç”¨å•æ¢è¡Œåˆ†éš” SSEï¼ˆ`data: {...}\n`ï¼‰ï¼Œå¯¼è‡´é»˜è®¤è§£æå™¨å¤±è´¥
- âŒ é—®é¢˜ï¼š`StreamingResponse.content` å­—æ®µæœªå¡«å……ï¼Œ`get_content()` è¿”å›ç©ºå­—ç¬¦ä¸²
- âŒ é—®é¢˜ï¼š`ZhipuRequest` ç¼ºå°‘ `stream` å‚æ•°ï¼ŒAPI ä¸çŸ¥é“è¦è¿”å›æµå¼å“åº”

**å·¥å…·è°ƒç”¨é—®é¢˜**:
- âŒ é—®é¢˜ï¼š`ZhipuRequest` ç¼ºå°‘ `tools` å’Œ `tool_choice` å­—æ®µ
- âŒ é—®é¢˜ï¼š`ZhipuMessage` ä¸æ”¯æŒ `tool_calls` å“åº”
- âŒ é—®é¢˜ï¼šæµå¼å’Œéæµå¼è¯·æ±‚éƒ½æ— æ³•ä¼ é€’å·¥å…·å‚æ•°

**ä¿®å¤å†…å®¹**:
1. **æµå¼è§£æå™¨** (`src/providers/zhipu.rs:126-201`)
   - å®ç°æ™ºè°±ä¸“ç”¨ `parse_stream_response()`
   - æ”¯æŒå•æ¢è¡Œåˆ†éš”çš„ SSE æ ¼å¼
   - è‡ªåŠ¨å¡«å…… `content` å­—æ®µï¼ˆä» `delta.content` å¤åˆ¶ï¼‰
   
2. **è¯·æ±‚å‚æ•°** (`src/providers/zhipu.rs:216-234`)
   - æ·»åŠ  `stream: Option<bool>` å­—æ®µ
   - æ·»åŠ  `tools: Option<Vec<Tool>>` å­—æ®µ
   - æ·»åŠ  `tool_choice: Option<ToolChoice>` å­—æ®µ
   
3. **å“åº”è§£æ** (`src/providers/zhipu.rs:240-264`)
   - `ZhipuMessage.content` ä½¿ç”¨ `#[serde(default)]`ï¼ˆå·¥å…·è°ƒç”¨æ—¶å¯ä¸ºç©ºï¼‰
   - `ZhipuMessage.tool_calls` æ”¯æŒå·¥å…·è°ƒç”¨å“åº”
   - `ZhipuResponse` åŒ…å«å®Œæ•´å…ƒæ•°æ®ï¼ˆid, created, usageï¼‰
   - `ZhipuChoice` æ”¯æŒ `finish_reason`ï¼ˆè¯†åˆ« `tool_calls` ç»“æŸï¼‰

**æµ‹è¯•éªŒè¯**:
- âœ… æµå¼å“åº”ï¼š124 ä¸ªæ•°æ®å—ï¼Œ642 å­—ç¬¦è¾“å‡º
- âœ… éæµå¼å·¥å…·è°ƒç”¨ï¼š`finish_reason: "tool_calls"`ï¼Œæ­£ç¡®è§£æå‚æ•°
- âœ… æµå¼å·¥å…·è°ƒç”¨ï¼š`finish_reason: "tool_calls"`ï¼Œæ­£ç¡®è§£æå‚æ•°

**å½±å“èŒƒå›´**:
- ä»…å½±å“æ™ºè°± GLM provider
- å®Œå…¨å‘åå…¼å®¹
- ä¿®å¤åä¸ OpenAI åè®®å¯¹é½

**æ–°å¢ç¤ºä¾‹**:
- `examples/zhipu_tools.rs` - å·¥å…·è°ƒç”¨ï¼ˆéæµå¼ï¼‰
- `examples/zhipu_tools_streaming.rs` - å·¥å…·è°ƒç”¨ï¼ˆæµå¼ï¼‰

---

## [0.4.11] - 2025-10-17

### ğŸ› Bug Fixes

**ä¿®å¤æ™ºè°±æµå¼å“åº”è§£æé—®é¢˜ï¼ˆåˆæ­¥ä¿®å¤ï¼‰**
- å®ç° `ZhipuProtocol::parse_stream_response()` ä¸“ç”¨æµå¼è§£æå™¨
- æ”¯æŒå•æ¢è¡Œåˆ†éš”çš„ SSE æ ¼å¼
- æ­£ç¡®å¤„ç† `data:` å‰ç¼€ï¼ˆå¸¦æˆ–ä¸å¸¦ç©ºæ ¼ï¼‰

---

## [Unreleased]

### ğŸ› **BUGFIX: ä¿®å¤æ™ºè°±æµå¼å“åº”è§£æé—®é¢˜**

#### **é—®é¢˜æè¿°**
æ™ºè°± API ä½¿ç”¨å•æ¢è¡Œåˆ†éš” SSE äº‹ä»¶ï¼ˆ`data: {...}\n`ï¼‰ï¼Œè€Œä¸æ˜¯æ ‡å‡†çš„åŒæ¢è¡Œï¼ˆ`data: {...}\n\n`ï¼‰ï¼Œå¯¼è‡´é»˜è®¤ SSE è§£æå™¨æ— æ³•æ­£ç¡®è§£ææµå¼å“åº”ï¼Œäº§ç”Ÿ 0 ä¸ªæ•°æ®å—ã€‚

#### **ä¿®å¤å†…å®¹**
- **æ–°å¢**: `ZhipuProtocol::parse_stream_response()` ä¸“ç”¨æµå¼è§£æå™¨
  - æ”¯æŒå•æ¢è¡Œåˆ†éš”çš„ SSE æ ¼å¼
  - æ­£ç¡®å¤„ç† `data:` å‰ç¼€ï¼ˆå¸¦æˆ–ä¸å¸¦ç©ºæ ¼ï¼‰
  - è·³è¿‡ `[DONE]` æ ‡è®°å’Œç©º payload
  - æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼ˆåŒ…å«åŸå§‹ JSONï¼‰

#### **æµ‹è¯•æ”¹è¿›**
- æ›´æ–° `examples/zhipu_streaming.rs`
  - æ·»åŠ æ•°æ®å—è®¡æ•°å™¨
  - æ˜¾ç¤ºè§£æå™¨ç±»å‹æç¤º
  - ä½¿ç”¨ `glm-4-flash` æ¨¡å‹ï¼ˆæ›´å¿«å“åº”ï¼‰
  - æ·»åŠ é›¶æ•°æ®å—è­¦å‘Š

#### **å½±å“**
- âœ… **ä¿®å¤**: æ™ºè°±æµå¼ API ç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œ
- âœ… **å…¼å®¹æ€§**: ä¸å½±å“å…¶ä»– Provider çš„æµå¼åŠŸèƒ½
- âœ… **è°ƒè¯•æ€§**: è§£æå¤±è´¥æ—¶æ˜¾ç¤ºåŸå§‹ JSON

---

### âœ¨ **FEAT: API Naming Standardization**

#### **Changed**
- **Unified Constructor Naming**
  - `ollama_with_url()` â†’ `ollama_with_base_url()` (kept old name as deprecated)
  - Removed redundant `zhipu_default()` (use `zhipu()` directly)
  - All providers now follow consistent `{provider}_with_base_url()` pattern

#### **Added**
- **Type-Safe Provider Conversions**
  - `LlmClient::as_ollama()` â†’ `Option<&OllamaProvider>`
  - `LlmClient::as_openai()` â†’ `Option<&OpenAIProvider>`
  - `LlmClient::as_aliyun()` â†’ `Option<&AliyunProvider>`
  - `LlmClient::as_anthropic()` â†’ `Option<&AnthropicProvider>`
  - `LlmClient::as_zhipu()` â†’ `Option<&ZhipuProvider>`
  
- **API Key Validation Functions**
  - `validate_openai_key()`
  - `validate_aliyun_key()`
  - `validate_anthropic_key()` (already existed)
  - `validate_zhipu_key()` (already existed)

- **Advanced Configuration Methods**
  - All `{provider}_with_config()` methods now exposed in `LlmClient`
  - All `{provider}_with_timeout()` methods now exposed in `LlmClient`
  - Cloud-specific methods: `anthropic_vertex()`, `anthropic_bedrock()`, `aliyun_international()`, etc.

#### **Documentation**
- **NEW**: `docs/NAMING_CONVENTIONS.md` - Comprehensive naming standards guide
- **NEW**: `.augment/rules/naming.md` - Qoder auto-check rules
- Updated all examples to use new naming conventions

#### **Deprecated**
- `LlmClient::ollama_with_url()` â†’ Use `ollama_with_base_url()`
- `providers::zhipu_default()` â†’ Use `zhipu()` directly
- `LlmClient::ollama()` (the method, not constructor) â†’ Use `as_ollama()`

#### **Impact**
- âœ… **Consistency**: All providers follow same naming pattern
- âœ… **Type Safety**: No more manual `downcast_ref` needed
- âœ… **Completeness**: All provider variants exposed in `LlmClient`
- âœ… **Documentation**: Clear naming rules for contributors

---

## [0.4.9] - 2025-10-16

### ğŸ“š **DOCS: Fix API Documentation and Examples**

#### **Fixed**
- **README API Examples** - Updated streaming API examples to reflect current V2 architecture
  - Replaced deprecated `chat_stream_universal()`, `chat_stream_sse()`, `chat_stream_ndjson()` with current `chat_stream()`
  - Updated streaming examples to use `StreamingResponse` and `get_content()` method
  - Added clear distinction between V2 (current) and V1 (legacy) APIs in changelog
  - Fixed 29 documentation tests that used incorrect import paths

#### **Added**
- **New Example**: `streaming_v2_demo.rs` - Demonstrates current V2 streaming API
- **API Clarification**: Clear documentation of current streaming interface
- **Migration Guide**: Explains differences between V1 and V2 streaming APIs

#### **Impact**
- âœ… **Documentation**: All examples now reflect current API
- âœ… **Tests**: All 93 tests pass (including 50 documentation tests)
- âœ… **Clarity**: Clear separation between current and legacy APIs
- âœ… **Examples**: Working examples for current streaming interface

## [0.4.8] - 2025-10-16

### ğŸ”§ **REFACTOR: Simplify Configuration Module Structure**

#### **Simplified**
- **Configuration Module** - Simplified `src/config/` directory to single `src/config.rs` file
  - Eliminated confusion between `src/config/provider.rs` and `src/providers/` directory
  - Consolidated all configuration types into single, clear module
  - Maintained all existing functionality and API compatibility
  - All 28 unit tests pass

#### **Structure Changes**
- **Before**: `src/config/mod.rs` + `src/config/provider.rs` (confusing)
- **After**: `src/config.rs` (clear and simple)
- **Benefits**: No naming confusion, easier to find configuration code, simpler maintenance

#### **Impact**
- âœ… **Clarity**: Eliminated naming confusion with providers
- âœ… **Simplicity**: Single file for all configuration needs
- âœ… **Maintainability**: Easier to locate and modify configuration code
- âœ… **Compatibility**: No breaking changes to public API

## [0.4.7] - 2025-10-16

### ğŸ—ï¸ **ARCHITECTURE: Correct Protocol vs Provider Separation**

#### **Refactored**
- **Protocol/Provider Architecture** - Implemented correct separation of public vs private protocols
  - **Public Protocols** (`src/protocols/`): Only industry-standard protocols (OpenAI, Anthropic)
  - **Private Protocols** (`src/providers/`): Provider-specific protocols inline with implementations
  - Moved `AliyunProtocol` and `ZhipuProtocol` from `protocols/` to `providers/` as private protocols
  - Updated exports: Standard protocols from `protocols`, private protocols from `providers`
  - All 78 unit and integration tests pass

#### **Design Principles**
- **Public Protocols**: Industry-recognized standards that multiple providers might implement
- **Private Protocols**: Provider-specific APIs that are tightly coupled to their implementations
- **Clean Separation**: Protocols define API formats, providers implement service logic
- **Maintainability**: Private protocols are co-located with their implementations

#### **Impact**
- âœ… **Architecture**: Clean separation of public vs private protocols
- âœ… **Maintainability**: Private protocols are easier to maintain alongside providers
- âœ… **Extensibility**: Clear guidelines for adding new protocols vs providers
- âœ… **Tests**: All functionality tests pass (78/78)

## [0.4.6] - 2025-10-16

### ğŸ”§ **HOTFIX: Streaming Integration Test Errors**

#### **Fixed**
- **Streaming integration test compilation errors** - Fixed all compilation errors in streaming tests
  - Fixed `tests/streaming_integration_tests.rs`: Added missing `Role` import
  - Updated all `Message::user()` calls to use proper `Message` construction with `Role::User`
  - Fixed all client creation calls: `.unwrap()` â†’ `?` for V2 architecture
  - Fixed error handling test to properly detect authentication errors
  - All streaming integration tests now pass (4/4 passed, 4 ignored for API keys)

#### **Impact**
- âœ… **Streaming Tests**: All streaming integration tests compile and pass
- âœ… **Test Coverage**: Complete test coverage for streaming functionality
- âœ… **V2 Architecture**: All tests use correct V2 architecture APIs

## [0.4.5] - 2025-10-16

### ğŸ”§ **HOTFIX: Test Compilation Errors**

#### **Fixed**
- **Test compilation errors** - Fixed compilation errors in test files
  - Fixed `tests/client_tests.rs`: Updated `protocol_name()` â†’ `provider_name()` method calls
  - Fixed main documentation tests in `src/lib.rs` and `src/client.rs`
  - Updated import statements to use correct V2 architecture paths
  - All unit tests and integration tests now pass successfully

#### **Impact**
- âœ… **Tests**: All unit and integration tests compile and pass (78/78)
- âœ… **Documentation**: Main documentation examples work correctly
- âœ… **CI/CD**: Test suite runs successfully for automated builds

## [0.4.4] - 2025-10-16

### ğŸ”§ **HOTFIX: Examples Compilation Errors**

#### **Fixed**
- **Examples compilation errors** - Fixed all compilation errors and warnings in example files
  - Updated `examples/zhipu_basic.rs`: Fixed API calls and imports for V2 architecture
  - Updated `examples/zhipu_streaming.rs`: Fixed Message construction and client creation
  - Updated `examples/streaming_basic.rs`: Fixed imports and Result handling
  - Updated `examples/ollama_model_management.rs`: Fixed Ollama provider interface usage
  - Updated `examples/v1_vs_v2_comparison.rs`: Removed deprecated feature flags
  - All examples now use V2 architecture APIs correctly

#### **Impact**
- âœ… **Examples**: All examples compile and run successfully
- âœ… **Documentation**: Examples serve as accurate V2 architecture documentation
- âœ… **User Experience**: Users can run examples without compilation errors

## [0.4.3] - 2025-10-16

### ğŸ”§ **HOTFIX: Module Privacy Error**

#### **Fixed**
- **Critical module privacy error** - Fixed private module access in streaming functionality
  - Fixed import path: `crate::types::streaming::ChatStream` â†’ `crate::types::ChatStream`
  - Fixed import path: `crate::types::streaming::StreamingResponse` â†’ `crate::types::StreamingResponse`
  - The `streaming` module is conditionally exported and should be accessed through `types` module
  - Affected file: `src/sse.rs`

#### **Impact**
- âœ… **Compilation**: Now compiles successfully without privacy errors
- âœ… **Streaming**: All streaming features work correctly
- âœ… **Functionality**: No breaking changes to public API

## [0.4.2] - 2025-10-16

### ğŸ”§ **HOTFIX: Type Mismatch Error**

#### **Fixed**
- **Critical type mismatch error** - Fixed streaming response type conversion
  - Added `sse_to_streaming_response()` function to convert `String` stream to `StreamingResponse` stream
  - Fixed type mismatch: expected `StreamingResponse` but found `String` in streaming methods
  - Affected files: `src/sse.rs`, `src/core/traits.rs`, `src/protocols/zhipu.rs`, `src/providers/ollama.rs`
  - All streaming functionality now works correctly with proper type conversion

#### **Impact**
- âœ… **Compilation**: Now compiles successfully without type errors
- âœ… **Streaming**: All streaming features work with correct types
- âœ… **Functionality**: No breaking changes to public API

## [0.4.1] - 2025-10-16

### ğŸ”§ **HOTFIX: Compilation Error**

#### **Fixed**
- **Critical compilation error** - Fixed unresolved import `crate::sse::SseStream`
  - Replaced incorrect `SseStream::new(response)` calls with `crate::sse::sse_events(response)`
  - Affected files: `src/core/traits.rs`, `src/protocols/zhipu.rs`, `src/providers/ollama.rs`
  - All streaming functionality now works correctly

#### **Impact**
- âœ… **Compilation**: Now compiles successfully without errors
- âœ… **Streaming**: All streaming features work as expected
- âœ… **Functionality**: No breaking changes to public API

## [0.4.0] - 2025-10-16

### ğŸš€ **MAJOR RELEASE: V2 Architecture**

This is a major release that introduces the new V2 architecture as the default, providing significant performance improvements and a cleaner API design.

#### âš¡ **Performance Improvements**
- **7,000x+ faster client creation** - From ~53ms to ~7Âµs
- **Minimal memory footprint** - Only 16 bytes per client instance
- **Zero-cost cloning** - Arc-based sharing for efficient cloning

#### ğŸ—ï¸ **New Architecture**
- **Clear Protocol/Provider separation** - Protocols define API specs, Providers implement services
- **Unified trait system** - `Protocol` and `Provider` traits for maximum extensibility
- **Type-safe HTTP client** - Compile-time guarantees for correctness
- **Generic provider implementation** - `GenericProvider<Protocol>` for most use cases

#### ğŸ”„ **API Changes (Breaking)**

##### **Client Creation**
```rust
// V1 (Legacy)
let client = LlmClient::openai("sk-...", None);
let client = LlmClient::ollama(None);

// V2 (New Default)
let client = LlmClient::openai("sk-...")?;  // Returns Result
let client = LlmClient::ollama()?;          // Returns Result
```

##### **Method Renames**
```rust
// V1 â†’ V2
client.fetch_models()  â†’ client.models()
client.protocol_name() â†’ client.provider_name()
```

##### **Parameter Changes**
- **OpenAI**: Removed optional second parameter, use dedicated methods
  - `openai(key, Some(url))` â†’ `openai_with_base_url(key, url)?`
- **Ollama**: Removed optional parameter
  - `ollama(Some(url))` â†’ `ollama_with_url(url)?`

#### ğŸ†• **New Features**

##### **Additional Client Creation Methods**
```rust
// Azure OpenAI support
LlmClient::azure_openai("key", "endpoint", "version")?

// OpenAI-compatible services
LlmClient::openai_compatible("key", "url", "name")?

// Zhipu GLM OpenAI-compatible mode
LlmClient::zhipu_openai_compatible("key")?

// Enhanced configuration options
LlmClient::openai_with_config("key", url, timeout, proxy)?
```

##### **Enhanced Ollama Support**
```rust
// Direct access to Ollama-specific features
if let Some(ollama) = client.as_ollama() {
    ollama.pull_model("llama2").await?;
    let models = ollama.models().await?;
}
```

#### ğŸ“¦ **Protocol Support**
- **OpenAI Protocol** - Complete OpenAI API specification
- **Anthropic Protocol** - Full Claude API support with Vertex AI and Bedrock
- **Aliyun Protocol** - DashScope API with regional support
- **Zhipu Protocol** - Native and OpenAI-compatible formats
- **Ollama Provider** - Custom implementation with model management

#### ğŸ”„ **Migration Guide**

##### **Option 1: Backward Compatibility**
```toml
# Cargo.toml
[features]
v1-legacy = []
```

```rust
// Use V1 API
#[cfg(feature = "v1-legacy")]
use llm_connector::v1::LlmClient;

// Use V2 API (default)
#[cfg(not(feature = "v1-legacy"))]
use llm_connector::LlmClient;
```

##### **Option 2: Direct Migration**
1. Add `?` to handle `Result` return types
2. Update method names: `fetch_models()` â†’ `models()`, `protocol_name()` â†’ `provider_name()`
3. Replace parameter patterns with dedicated methods
4. Update imports if using internal traits

#### ğŸ›ï¸ **Architecture Benefits**
- **Extensibility** - Easy to add new protocols and providers
- **Type Safety** - Compile-time guarantees for all operations
- **Performance** - Optimized for speed and memory efficiency
- **Clarity** - Clear separation of concerns between protocols and providers
- **Maintainability** - Reduced code duplication and cleaner abstractions

## [0.3.6] - 2025-10-14

### âœ¨ Added

#### Ollama Streaming Support
- Implemented line-delimited JSON streaming for Ollama protocol
  - Added non-SSE parser for JSON lines stream
  - Integrated into core streaming pipeline with protocol switch
  - Normalized to `StreamingResponse` with `get_content()` for output
- Added `examples/ollama_streaming.rs` demonstrating `chat_stream()` usage

### ğŸ“ Updated
- README and examples already standardized to use `get_content()` for streaming output

## [0.2.3] - 2025-01-06

### âœ¨ Added

#### Improved Error Messages
- **Cleaned up authentication error messages** for OpenAI-compatible providers
  - Removes OpenAI-specific URLs (like "platform.openai.com") from error messages
  - Adds helpful context: "Please verify your API key is correct and has the necessary permissions"
  - Makes errors more generic and applicable to all OpenAI-compatible providers (DeepSeek, Zhipu, Moonshot, etc.)

#### New Debug Tools
- **Added `debug_deepseek.rs` example** for troubleshooting authentication issues
  - Validates API key format
  - Tests model fetching and chat requests
  - Provides specific troubleshooting guidance based on error type
  - Can accept API key from command line or environment variable

#### Documentation
- **Added `TROUBLESHOOTING.md`** - Comprehensive troubleshooting guide
  - Authentication errors and solutions
  - Connection errors and debugging steps
  - Rate limit handling
  - Model not found errors
  - Provider-specific instructions for DeepSeek, OpenAI, Zhipu, Moonshot
  - Example code for common scenarios

### ğŸ”§ Changed

#### Simplified API - Removed `supported_models()`
- **Removed `supported_models()` method** from all traits and implementations
  - Removed from `Provider` trait
  - Removed from `ProviderAdapter` trait
  - Removed from `LlmClient`
  - Removed from all protocol implementations (OpenAI, Anthropic, Aliyun, Ollama)
- **Removed `supports_model()` method** from `Provider` trait (was dependent on `supported_models()`)
- **Removed hardcoded model lists** from protocol structs
  - Removed `supported_models` field from `AnthropicProtocol`
  - Removed `supported_models` field from `AliyunProtocol`
  - Removed `supported_models` field from `OllamaProtocol`

#### Rationale
- `supported_models()` returned empty `[]` for most protocols (OpenAI, Anthropic, Aliyun)
- Only Ollama had hardcoded models, which were outdated
- Users should use `fetch_models()` for real-time model discovery
- Simplifies the API by removing confusion between two similar methods

#### Migration Guide

**Before:**
```rust
let client = LlmClient::openai("sk-...");
let models = client.supported_models(); // Returns []
```

**After:**
```rust
let client = LlmClient::openai("sk-...");
let models = client.fetch_models().await?; // Returns actual models from API
```

For protocols that don't support `fetch_models()` (Anthropic, Aliyun, Ollama), you can use any model name directly in your requests.

### ğŸ“ Updated

- Updated tests to remove `supported_models()` usage
- Updated examples to demonstrate only `fetch_models()`
- Updated README.md to remove references to `supported_models()`
- Simplified documentation and examples

## [0.2.2] - 2025-01-06

Same as 0.2.1 - version bump for crates.io publication.

## [0.2.1] - 2025-01-06

### âœ¨ Added

#### Online Model Discovery
- **New `fetch_models()` method** for retrieving available models from API
  - Added to `Provider` trait, `LlmClient`, and `GenericProvider`
  - Makes GET request to `/v1/models` endpoint for OpenAI-compatible providers
  - Returns `Vec<String>` of available model IDs
  - Returns `UnsupportedOperation` error for protocols without model listing support

#### HTTP Transport Enhancement
- Added `get()` method to `HttpTransport` for GET requests
- Supports custom headers and authentication

#### Error Handling
- Added `UnsupportedOperation` error variant for unsupported operations
- Returns HTTP 501 status code for unsupported operations

#### Examples
- `examples/test_fetch_models.rs` - Comprehensive test with all providers
- `examples/fetch_models_simple.rs` - Simple comparison example
- `examples/test_with_keys.rs` - Test with keys.yaml configuration

#### Documentation
- `FETCH_MODELS_FEATURE.md` - Complete feature documentation
- `TEST_RESULTS.md` - Test results and verification
- Updated README.md with model discovery section
- Added comparison table for `supported_models()` vs `fetch_models()`

### ğŸ”§ Changed

#### OpenAI Protocol
- **Removed hardcoded model lists** from `OpenAIProtocol`
- `supported_models()` now returns empty `[]` instead of hardcoded models
- Users can now use **any model name** without restrictions
- Implemented `models_endpoint_url()` to support `/v1/models` endpoint

#### Documentation Cleanup
- Removed references to third-party providers (DeepSeek, Zhipu, Moonshot, etc.) from OpenAI protocol docs
- Updated examples to focus on OpenAI instead of third-party providers
- Simplified documentation to emphasize protocol-first approach

#### Provider Type Aliases
- Removed provider-specific type aliases:
  - `DeepSeekProvider`
  - `ZhipuProvider`
  - `MoonshotProvider`
  - `VolcEngineProvider`
  - `TencentProvider`
  - `MiniMaxProvider`
  - `StepFunProvider`

### ğŸ› Fixed

#### Configuration
- Fixed `keys.yaml` model names:
  - Removed invalid `qwen3-turbo` model
  - Updated to valid Aliyun models: `qwen-turbo`, `qwen-plus`, `qwen-max`
  - Updated Qwen2 models to Qwen2.5 versions

#### Dependencies
- Added `serde_yaml` to `[dev-dependencies]` for examples
- Fixed `serde_yaml` resolution in test examples

#### Code Quality
- Removed unused imports (`HttpTransport`, `LlmConnectorError` from openai.rs)
- Fixed struct field issues (removed incorrect `transport` field)

### ğŸ“Š Test Results

#### Successfully Tested Providers (Online Model Fetching)

| Provider | Status | Models Found | Example Models |
|----------|--------|--------------|----------------|
| DeepSeek | âœ… | 2 | `deepseek-chat`, `deepseek-reasoner` |
| Zhipu (GLM) | âœ… | 3 | `glm-4.5`, `glm-4.5-air`, `glm-4.6` |
| Moonshot | âœ… | 12 | `moonshot-v1-32k`, `kimi-latest`, `kimi-thinking-preview` |
| LongCat | âŒ | - | `/models` endpoint not available |
| VolcEngine | âŒ | - | `/models` endpoint not available |
| Aliyun | â„¹ï¸ | - | Protocol doesn't support model listing |
| Anthropic | â„¹ï¸ | - | Protocol doesn't support model listing |

### ğŸ“ Migration Guide

#### For Users Relying on Hardcoded Models

**Before (v0.2.0):**
```rust
let client = LlmClient::openai("sk-...");
let models = client.supported_models();
// Returns: ["gpt-4", "gpt-3.5-turbo", "gpt-4-turbo"]
```

**After (v0.2.1):**
```rust
let client = LlmClient::openai("sk-...");

// Option 1: Use any model name directly (recommended)
let request = ChatRequest {
    model: "gpt-4o".to_string(), // Any model name works
    // ...
};

// Option 2: Fetch models online
let models = client.fetch_models().await?;
// Returns: actual models from OpenAI API
```

#### For OpenAI-Compatible Providers

**Before:**
```rust
// Had to check hardcoded list
let models = client.supported_models();
```

**After:**
```rust
// Fetch real-time models from provider
let client = LlmClient::openai_compatible(
    "sk-...",
    "https://api.deepseek.com/v1"
);
let models = client.fetch_models().await?;
// Returns: ["deepseek-chat", "deepseek-reasoner"]
```

### ğŸ¯ Benefits

1. **No Model Restrictions**: Use any model name without being limited by hardcoded lists
2. **Always Up-to-Date**: Get the latest models directly from the API
3. **Backward Compatible**: Existing code continues to work
4. **Flexible**: Providers can opt-in to model listing support
5. **Clear Errors**: Explicit error messages when operations aren't supported

### ğŸ”— Related Issues

- Fixed errors in `src/protocols/openai.rs`
- Removed hardcoded `supported_models`
- Implemented online model fetching (Option 3)
- Updated documentation to reflect changes

### ğŸ“š Documentation

- README.md: Added "Key Features" section
- README.md: Added "Model Discovery" section with comparison table
- README.md: Added "Recent Changes" section
- README.md: Updated error handling examples
- README.md: Updated examples section

### ğŸ§ª Testing

All tests passing:
```bash
cargo check --lib                    # âœ… Success
cargo run --example test_openai_only # âœ… All tests passed
cargo run --example test_with_keys   # âœ… 6/6 providers tested
cargo run --example test_fetch_models # âœ… Online fetching works
```

---

## [0.2.0] - Previous Release

Initial release with 4 protocol support and basic functionality.

---

## Future Enhancements

Potential improvements for future releases:

1. **Model Caching**: Cache fetched models to reduce API calls
2. **Model Metadata**: Return full model objects with capabilities, not just IDs
3. **Model Filtering**: Add parameters to filter models by capability
4. **Extended Protocol Support**: Implement model listing for other protocols if available
5. **Pagination Support**: Handle paginated model responses
## [0.3.3] - 2025-10-14

### âœ¨ Added
- README: Added â€œReasoning Synonymsâ€ section with normalized keys and usage examples (`reasoning_any()`), covering non-streaming and streaming.

### ğŸ”§ Changed
- Examples: Removed outdated examples using deprecated `openai_compatible` (`examples/test_fetch_models.rs`, `examples/test_with_keys.rs`).
- Examples: Updated DeepSeek and fetch models example to use `LlmClient::openai(api_key, Some(base_url))`.
- Docs: Fixed doctests across `lib.rs`, `protocols/core.rs`, `protocols/openai.rs`, `protocols/aliyun.rs`, `protocols/anthropic.rs` to match current API.
- Docs: Replaced obsolete imports like `protocols::aliyun::qwen` and `protocols::anthropic::claude` with `LlmClient::aliyun(...)` and `LlmClient::anthropic(...)`.
- Docs: Standardized message initialization to `Message::user(...)` or `Role` enums where appropriate.

### âœ… Validation
- `cargo build --examples` passes.
- `cargo test` (library and integration with `streaming` feature) passes.
- `cargo test --doc` passes (13 passed, 0 failed, 4 ignored).
## 0.3.4 - 2025-10-14

Updates
- Add compatibility alias `types::ChatMessage = Message` to ease migration.
- Add `ChatResponse::get_usage_safe()` returning `(prompt, completion, total)`.
- Add `ChatResponse::get_content()` returning the first choice content as `Option<&str>`.
- README install snippet updated to `0.3.4`.

Notes
- `ChatRequest::new(model)` remains as minimal constructor.
- Use `ChatRequest::new_with_messages(model, messages)` to pass initial message list.
- `Message::user/assistant/system` are preferred constructors; reasoning fields are auto-populated.

## 0.3.5 - 2025-10-14

Updates
- Add `StreamingResponse::get_content()` for convenience and API symmetry with `ChatResponse::get_content()`.

Notes
- No breaking changes; existing code continues to work. You can still access `choices[0].delta.content` directly.

